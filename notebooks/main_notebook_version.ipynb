{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "from config.execution_parameters import *\n",
    "\n",
    "# Project libraries\n",
    "from src.simulator_package.simulator_functions import *\n",
    "import src.hmm_package.generate_hmm\n",
    "from src.hmm_package.generate_hmm import *\n",
    "from src.plot_and_print_info.plots_and_print_info import *\n",
    "\n",
    "import random\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "importlib.reload(src.hmm_package.generate_hmm)\n",
    "from src.hmm_package.generate_hmm import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "advertising_campaign = SimulationClass(ANALYSIS_MODE)\n",
    "advertising_campaign.simulate()\n",
    "\n",
    "adstock = compute_adstock(observation=advertising_campaign.results[\"user_expositions\"])\n",
    "output = np.append( np.zeros([adstock.shape[0], 1]), advertising_campaign.results[\"user_outcome\"], axis=1 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([12626, 3, 31])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adstock.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "empty_users = 6000\n",
    "adstock_with_empty = tf.concat([adstock, tf.zeros([empty_users, adstock.shape[1], adstock.shape[2]], dtype=float)], axis = 0)\n",
    "output_with_empty = np.append(output, np.zeros([empty_users, adstock.shape[2]]), axis = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "(12188, 31)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "empty_users = 5000\n",
    "adstock_only_empty = tf.zeros([empty_users, adstock.shape[1], adstock.shape[2]], dtype=float)\n",
    "output_only_empty = np.zeros([empty_users, adstock.shape[2]])\n",
    "\n",
    "# Lets add a 3% of conversion due to external factors\n",
    "for temp in range(round(0.03*empty_users)):\n",
    "    output_only_empty[temp, random.randint(1,adstock.shape[2]):] = 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: [array([-0.2528566 ,  0.1959858 ,  0.23891842, -0.30465543,  0.42454708,\n",
      "       -0.27133608,  0.25765383,  0.43076503, -0.20775962,  0.4620564 ,\n",
      "       -0.03529656,  0.0498271 ], dtype=float32)]\n",
      "Epoch 1/10000\n",
      "51/51 [==============================] - 84s 2s/step - loss: 377.0730\n",
      "Beta: [array([-0.20107226,  0.24683352,  0.18611048, -0.25145397,  0.4761612 ,\n",
      "       -0.22030638,  0.2055543 ,  0.48345813, -0.16921237,  0.51218545,\n",
      "       -0.08697189,  0.1017909 ], dtype=float32)]\n",
      "Epoch 2/10000\n",
      "51/51 [==============================] - 82s 2s/step - loss: 365.7585\n",
      "Beta: [array([-0.14135803,  0.2940745 ,  0.13057911, -0.19395177,  0.53116673,\n",
      "       -0.1745625 ,  0.15232125,  0.53862613, -0.11434188,  0.5585369 ,\n",
      "       -0.14190851,  0.15820615], dtype=float32)]\n",
      "Epoch 3/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 354.8901\n",
      "Beta: [array([-0.0696855 ,  0.3376508 ,  0.06937398, -0.12876624,  0.5929885 ,\n",
      "       -0.13398919,  0.09556693,  0.5994662 , -0.046658  ,  0.60023135,\n",
      "       -0.20161305,  0.22076842], dtype=float32)]\n",
      "Epoch 4/10000\n",
      "51/51 [==============================] - 86s 2s/step - loss: 342.3131\n",
      "Beta: [array([ 0.00698213,  0.37511152,  0.00642026, -0.05884394,  0.6559595 ,\n",
      "       -0.10204525,  0.04035524,  0.66130674,  0.02733396,  0.6349886 ,\n",
      "       -0.262218  ,  0.2867604 ], dtype=float32)]\n",
      "Epoch 5/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 330.2010\n",
      "Beta: [array([ 0.087144  ,  0.40556204, -0.05693795,  0.01476637,  0.717317  ,\n",
      "       -0.07853271, -0.01063712,  0.7218442 ,  0.10300744,  0.6612924 ,\n",
      "       -0.31986538,  0.3521677 ], dtype=float32)]\n",
      "Epoch 6/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 318.9672\n",
      "Beta: [array([ 0.1631777 ,  0.42799002, -0.1149324 ,  0.08620979,  0.7695864 ,\n",
      "       -0.0637282 , -0.0508623 ,  0.77289   ,  0.16924515,  0.67890996,\n",
      "       -0.3661721 ,  0.4081064 ], dtype=float32)]\n",
      "Epoch 7/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 309.1862\n",
      "Beta: [array([ 0.23326461,  0.44411537, -0.16686574,  0.15427735,  0.8108324 ,\n",
      "       -0.05498667, -0.07832684,  0.8105043 ,  0.22281587,  0.6890055 ,\n",
      "       -0.39767084,  0.44872922], dtype=float32)]\n",
      "Epoch 8/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 302.5750\n",
      "Beta: [array([ 0.29698235,  0.4556983 , -0.21270198,  0.21823482,  0.8410727 ,\n",
      "       -0.05026848, -0.09320158,  0.83253706,  0.26200208,  0.69401336,\n",
      "       -0.41274592,  0.4708923 ], dtype=float32)]\n",
      "Epoch 9/10000\n",
      "51/51 [==============================] - 84s 2s/step - loss: 296.6805\n",
      "Beta: [array([ 0.35370642,  0.46381617, -0.25195047,  0.27643025,  0.86067384,\n",
      "       -0.04826543, -0.09560446,  0.8383661 ,  0.28630498,  0.69507277,\n",
      "       -0.41039568,  0.4721057 ], dtype=float32)]\n",
      "Epoch 10/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 291.8311\n",
      "Beta: [array([ 0.40477178,  0.46968064, -0.28682724,  0.33102253,  0.87132883,\n",
      "       -0.04799767, -0.08800192,  0.831596  ,  0.30145404,  0.69333375,\n",
      "       -0.39588866,  0.460961  ], dtype=float32)]\n",
      "Epoch 11/10000\n",
      "51/51 [==============================] - 86s 2s/step - loss: 287.8405\n",
      "Beta: [array([ 0.45249563,  0.47399655, -0.31855446,  0.38283372,  0.8784381 ,\n",
      "       -0.04885901, -0.07417939,  0.8185422 ,  0.31283903,  0.6899842 ,\n",
      "       -0.37308413,  0.44410866], dtype=float32)]\n",
      "Epoch 12/10000\n",
      "51/51 [==============================] - 93s 2s/step - loss: 284.1624\n",
      "Beta: [array([ 0.49791744,  0.47720572, -0.34802693,  0.43264484,  0.8818263 ,\n",
      "       -0.0505423 , -0.05486818,  0.8024086 ,  0.3193674 ,  0.68514943,\n",
      "       -0.34240207,  0.42358702], dtype=float32)]\n",
      "Epoch 13/10000\n",
      "51/51 [==============================] - 150s 3s/step - loss: 280.7472\n",
      "Beta: [array([ 0.5389218 ,  0.479238  , -0.37477982,  0.47980547,  0.8806079 ,\n",
      "       -0.05308619, -0.02976283,  0.78187895,  0.3226866 ,  0.6789363 ,\n",
      "       -0.30547047,  0.40335968], dtype=float32)]\n",
      "Epoch 14/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 277.1725\n",
      "Beta: [array([ 0.57848084,  0.48060492, -0.40012577,  0.525353  ,  0.8776908 ,\n",
      "       -0.05619029, -0.00156337,  0.76279753,  0.32371417,  0.67148507,\n",
      "       -0.26416922,  0.38388425], dtype=float32)]\n",
      "Epoch 15/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 274.0882\n",
      "Beta: [array([ 0.61576676,  0.48133582, -0.42398354,  0.5691454 ,  0.8719617 ,\n",
      "       -0.05986459,  0.03084256,  0.74222964,  0.3238374 ,  0.6631767 ,\n",
      "       -0.219471  ,  0.3671623 ], dtype=float32)]\n",
      "Epoch 16/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 271.2910\n",
      "Beta: [array([ 0.65096176,  0.48147407, -0.4470499 ,  0.6118984 ,  0.86474985,\n",
      "       -0.06403823,  0.06441065,  0.72673804,  0.3223698 ,  0.65385795,\n",
      "       -0.17274155,  0.3554175 ], dtype=float32)]\n",
      "Epoch 17/10000\n",
      "51/51 [==============================] - 82s 2s/step - loss: 268.6630\n",
      "Beta: [array([ 0.6839089 ,  0.48113248, -0.4679794 ,  0.65172106,  0.8546647 ,\n",
      "       -0.06870907,  0.10286245,  0.708074  ,  0.31760806,  0.6437276 ,\n",
      "       -0.12240799,  0.34437433], dtype=float32)]\n",
      "Epoch 18/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 265.9266\n",
      "Beta: [array([ 0.7148758 ,  0.48033914, -0.48837388,  0.6900674 ,  0.84227765,\n",
      "       -0.07387957,  0.14198732,  0.6936745 ,  0.3088427 ,  0.6325389 ,\n",
      "       -0.0690088 ,  0.33437744], dtype=float32)]\n",
      "Epoch 19/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 264.2721\n",
      "Beta: [array([ 0.7440422 ,  0.47916666, -0.50748074,  0.7263095 ,  0.82862407,\n",
      "       -0.07950886,  0.18317993,  0.6806363 ,  0.2988496 ,  0.62062305,\n",
      "       -0.01455468,  0.32704136], dtype=float32)]\n",
      "Epoch 20/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 262.0872\n",
      "Beta: [array([ 0.77235043,  0.47773057, -0.5256565 ,  0.76064014,  0.81321865,\n",
      "       -0.0855741 ,  0.22590886,  0.6681875 ,  0.2896325 ,  0.60790646,\n",
      "        0.0394892 ,  0.32443297], dtype=float32)]\n",
      "Epoch 21/10000\n",
      "51/51 [==============================] - 82s 2s/step - loss: 260.4573\n",
      "Beta: [array([ 0.7985593 ,  0.4760029 , -0.5429246 ,  0.79270005,  0.7959676 ,\n",
      "       -0.09205524,  0.26874486,  0.65781504,  0.2769894 ,  0.59448314,\n",
      "        0.09384097,  0.32370493], dtype=float32)]\n",
      "Epoch 22/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 258.3193\n",
      "Beta: [array([ 0.823377  ,  0.4740124 , -0.5589969 ,  0.8224482 ,  0.7763924 ,\n",
      "       -0.09896788,  0.31295046,  0.64758843,  0.26281613,  0.580376  ,\n",
      "        0.14871976,  0.3235161 ], dtype=float32)]\n",
      "Epoch 23/10000\n",
      "51/51 [==============================] - 82s 2s/step - loss: 257.3194\n",
      "Beta: [array([ 0.84718424,  0.47173733, -0.57550985,  0.85114187,  0.75680196,\n",
      "       -0.10634343,  0.35440952,  0.64271647,  0.24596421,  0.56541985,\n",
      "        0.20144218,  0.3267662 ], dtype=float32)]\n",
      "Epoch 24/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 255.9370\n",
      "Beta: [array([ 0.86902624,  0.4692992 , -0.5897799 ,  0.8755936 ,  0.73495954,\n",
      "       -0.11412249,  0.3963995 ,  0.6358374 ,  0.23149964,  0.550011  ,\n",
      "        0.25246984,  0.33165318], dtype=float32)]\n",
      "Epoch 25/10000\n",
      "51/51 [==============================] - 82s 2s/step - loss: 254.5134\n",
      "Beta: [array([ 0.88958925,  0.4666329 , -0.60360104,  0.8984148 ,  0.7128295 ,\n",
      "       -0.12226584,  0.4369228 ,  0.63106745,  0.21281226,  0.5339307 ,\n",
      "        0.30257967,  0.33643124], dtype=float32)]\n",
      "Epoch 26/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 253.5515\n",
      "Beta: [array([ 0.91037023,  0.46387202, -0.6184732 ,  0.920492  ,  0.6903785 ,\n",
      "       -0.13077366,  0.47459504,  0.62988615,  0.1974717 ,  0.5174844 ,\n",
      "        0.34967065,  0.3458978 ], dtype=float32)]\n",
      "Epoch 27/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 252.8401\n",
      "Beta: [array([ 0.9291503 ,  0.4608779 , -0.63182133,  0.93914455,  0.6677042 ,\n",
      "       -0.13958378,  0.51084864,  0.62873244,  0.17931409,  0.5004898 ,\n",
      "        0.39393952,  0.3552321 ], dtype=float32)]\n",
      "Epoch 28/10000\n",
      "51/51 [==============================] - 82s 2s/step - loss: 252.3011\n",
      "Beta: [array([ 0.94647646,  0.45774913, -0.64464015,  0.95513827,  0.6448942 ,\n",
      "       -0.14864926,  0.54443514,  0.62860674,  0.16357733,  0.48320264,\n",
      "        0.4347666 ,  0.36619377], dtype=float32)]\n",
      "Epoch 29/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 251.4498\n",
      "Beta: [array([ 0.96283096,  0.45446068, -0.65595037,  0.9689077 ,  0.62186515,\n",
      "       -0.15808845,  0.57655853,  0.6293065 ,  0.14435798,  0.4653589 ,\n",
      "        0.47574183,  0.3744021 ], dtype=float32)]\n",
      "Epoch 30/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 250.9163\n",
      "Beta: [array([ 0.97941184,  0.45115605, -0.668762  ,  0.982041  ,  0.59879315,\n",
      "       -0.16762133,  0.60691184,  0.6314182 ,  0.12975475,  0.44763365,\n",
      "        0.5123947 ,  0.38805208], dtype=float32)]\n",
      "Epoch 31/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 250.0025\n",
      "Beta: [array([ 0.9937948 ,  0.44767556, -0.6785638 ,  0.9907953 ,  0.5744268 ,\n",
      "       -0.17751454,  0.6370495 ,  0.6307863 ,  0.11502855,  0.42959338,\n",
      "        0.54802   ,  0.3990402 ], dtype=float32)]\n",
      "Epoch 32/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 249.7789\n",
      "Beta: [array([ 1.0096498 ,  0.44412884, -0.69054645,  1.0006653 ,  0.5541522 ,\n",
      "       -0.1875167 ,  0.6625329 ,  0.636627  ,  0.10024051,  0.4112273 ,\n",
      "        0.58253795,  0.4098888 ], dtype=float32)]\n",
      "Epoch 33/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 249.4466\n",
      "Beta: [array([ 1.024239  ,  0.44046256, -0.7015458 ,  1.0075741 ,  0.5331155 ,\n",
      "       -0.19774525,  0.6879313 ,  0.6404749 ,  0.08653926,  0.39279333,\n",
      "        0.61443913,  0.4223295 ], dtype=float32)]\n",
      "Epoch 34/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 249.3246\n",
      "Beta: [array([ 1.0389853 ,  0.43681434, -0.7134395 ,  1.0134039 ,  0.5133863 ,\n",
      "       -0.2079055 ,  0.7095626 ,  0.6461532 ,  0.07786558,  0.37463045,\n",
      "        0.64181495,  0.4384216 ], dtype=float32)]\n",
      "Epoch 35/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 248.8465\n",
      "Beta: [array([ 1.0516944 ,  0.4330397 , -0.72326285,  1.0166808 ,  0.4922799 ,\n",
      "       -0.2183989 ,  0.73168707,  0.6503954 ,  0.06449261,  0.35624224,\n",
      "        0.67092663,  0.4498316 ], dtype=float32)]\n",
      "Epoch 36/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 248.4306\n",
      "Beta: [array([ 1.065432  ,  0.4292448 , -0.73409206,  1.0186986 ,  0.47427893,\n",
      "       -0.22879218,  0.7513455 ,  0.65642273,  0.0575446 ,  0.33813357,\n",
      "        0.69548315,  0.46455175], dtype=float32)]\n",
      "Epoch 37/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 248.1944\n",
      "Beta: [array([ 1.0782175 ,  0.42533785, -0.7434666 ,  1.0204617 ,  0.45540318,\n",
      "       -0.23929133,  0.7715654 ,  0.66162276,  0.04621035,  0.3197342 ,\n",
      "        0.72334945,  0.47599334], dtype=float32)]\n",
      "Epoch 38/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 248.1293\n",
      "Beta: [array([ 1.0911466 ,  0.42142054, -0.75477874,  1.0202996 ,  0.43906823,\n",
      "       -0.24983913,  0.7876585 ,  0.6695058 ,  0.0361923 ,  0.3013918 ,\n",
      "        0.7479268 ,  0.48971894], dtype=float32)]\n",
      "Epoch 39/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 248.2158\n",
      "Beta: [array([ 1.1031632 ,  0.41743323, -0.76317596,  1.018684  ,  0.42045975,\n",
      "       -0.2605649 ,  0.80675524,  0.67328143,  0.02594255,  0.2831757 ,\n",
      "        0.77219355,  0.50126487], dtype=float32)]\n",
      "Epoch 40/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 247.5443\n",
      "Beta: [array([ 1.1160332 ,  0.4134153 , -0.7746152 ,  1.017379  ,  0.40617463,\n",
      "       -0.2712758 ,  0.8200196 ,  0.6831184 ,  0.01674229,  0.26499403,\n",
      "        0.7952635 ,  0.51381963], dtype=float32)]\n",
      "Epoch 41/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 247.6211\n",
      "Beta: [array([ 1.1294067 ,  0.40938517, -0.7850389 ,  1.0163546 ,  0.39032328,\n",
      "       -0.2820705 ,  0.8366948 ,  0.6887953 ,  0.01367653,  0.2470789 ,\n",
      "        0.8145301 ,  0.5298055 ], dtype=float32)]\n",
      "Epoch 42/10000\n",
      "51/51 [==============================] - 79s 2s/step - loss: 247.2602\n",
      "Beta: [array([ 1.1412103 ,  0.4052463 , -0.79647666,  1.0139525 ,  0.3749596 ,\n",
      "       -0.29290026,  0.84998584,  0.6985164 ,  0.00465458,  0.22903979,\n",
      "        0.83542585,  0.5438418 ], dtype=float32)]\n",
      "Epoch 43/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 247.2872\n",
      "Beta: [array([ 1.1532054 ,  0.40111703, -0.8053348 ,  1.009461  ,  0.36191982,\n",
      "       -0.30365053,  0.863372  ,  0.7062252 , -0.00456329,  0.21113582,\n",
      "        0.85972613,  0.5528928 ], dtype=float32)]\n",
      "Epoch 44/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 246.9046\n",
      "Beta: [array([ 1.1650984 ,  0.396912  , -0.81603515,  1.0054791 ,  0.34762698,\n",
      "       -0.31454635,  0.8774069 ,  0.713266  , -0.01192965,  0.19322644,\n",
      "        0.8790824 ,  0.5653043 ], dtype=float32)]\n",
      "Epoch 45/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 247.0769\n",
      "Beta: [array([ 1.1776396 ,  0.3927823 , -0.8267135 ,  1.0009775 ,  0.33335423,\n",
      "       -0.3252926 ,  0.8903723 ,  0.7198564 , -0.01627748,  0.17577828,\n",
      "        0.89632344,  0.5804887 ], dtype=float32)]\n",
      "Epoch 46/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 246.8175\n",
      "Beta: [array([ 1.1903123 ,  0.38852257, -0.8356606 ,  0.9958185 ,  0.3202779 ,\n",
      "       -0.33625615,  0.9056117 ,  0.72700936, -0.02400043,  0.15807913,\n",
      "        0.918337  ,  0.59105533], dtype=float32)]\n",
      "Epoch 47/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 246.3292\n",
      "Beta: [array([ 1.2029269 ,  0.38426343, -0.8484909 ,  0.9914492 ,  0.3097038 ,\n",
      "       -0.34714332,  0.9140177 ,  0.7377691 , -0.02751206,  0.14061178,\n",
      "        0.9346046 ,  0.6052095 ], dtype=float32)]\n",
      "Epoch 48/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 246.2308\n",
      "Beta: [array([ 1.2138041 ,  0.37991402, -0.8579167 ,  0.9855445 ,  0.2957793 ,\n",
      "       -0.35808164,  0.92755115,  0.74520814, -0.03687304,  0.12310927,\n",
      "        0.95506793,  0.61560583], dtype=float32)]\n",
      "Epoch 49/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 246.4219\n",
      "Beta: [array([ 1.2255934 ,  0.3755746 , -0.86787206,  0.9805004 ,  0.28307873,\n",
      "       -0.3691635 ,  0.9413158 ,  0.7515808 , -0.04181096,  0.10573021,\n",
      "        0.9730478 ,  0.6282059 ], dtype=float32)]\n",
      "Epoch 50/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 245.9212\n",
      "Beta: [array([ 1.2380905 ,  0.3711963 , -0.87870944,  0.97449696,  0.2733301 ,\n",
      "       -0.38009325,  0.95120174,  0.76075804, -0.04795544,  0.08838402,\n",
      "        0.989892  ,  0.6392704 ], dtype=float32)]\n",
      "Epoch 51/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 245.9165\n",
      "Beta: [array([ 1.2501217 ,  0.36679688, -0.89099556,  0.9701819 ,  0.26332432,\n",
      "       -0.3910974 ,  0.9603388 ,  0.77063644, -0.05263422,  0.07124738,\n",
      "        1.005522  ,  0.6524514 ], dtype=float32)]\n",
      "Epoch 52/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 245.8267\n",
      "Beta: [array([ 1.2613347 ,  0.36240375, -0.90128464,  0.96409154,  0.25215188,\n",
      "       -0.4019328 ,  0.9732847 ,  0.7776007 , -0.0583845 ,  0.05440105,\n",
      "        1.02182   ,  0.66453093], dtype=float32)]\n",
      "Epoch 53/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 245.7840\n",
      "Beta: [array([ 1.2739701 ,  0.35792485, -0.9130124 ,  0.959152  ,  0.2405942 ,\n",
      "       -0.4131564 ,  0.9843044 ,  0.7860295 , -0.06407323,  0.0371585 ,\n",
      "        1.0383148 ,  0.6766303 ], dtype=float32)]\n",
      "Epoch 54/10000\n",
      "51/51 [==============================] - 84s 2s/step - loss: 245.6802\n",
      "Beta: [array([ 1.283229  ,  0.353409  , -0.92257714,  0.95239973,  0.23034948,\n",
      "       -0.4240529 ,  0.99421835,  0.7940645 , -0.07141311,  0.02022376,\n",
      "        1.0565181 ,  0.6861129 ], dtype=float32)]\n",
      "Epoch 55/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 245.4921\n",
      "Beta: [array([ 1.296456  ,  0.34892887, -0.9339731 ,  0.94811225,  0.22090344,\n",
      "       -0.43519714,  1.0056586 ,  0.80235386, -0.07503697,  0.00340771,\n",
      "        1.0703423 ,  0.69947094], dtype=float32)]\n",
      "Epoch 56/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 244.8845\n",
      "Beta: [array([ 1.3049787 ,  0.3443184 , -0.9426223 ,  0.9406874 ,  0.20883633,\n",
      "       -0.44619128,  1.0181937 ,  0.8085882 , -0.08391804, -0.0134397 ,\n",
      "        1.0894386 ,  0.7081038 ], dtype=float32)]\n",
      "Epoch 57/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 244.7926\n",
      "Beta: [array([ 1.317868  ,  0.33978948, -0.9543466 ,  0.93464226,  0.20238772,\n",
      "       -0.45718342,  1.0270195 ,  0.81696934, -0.08652922, -0.02997933,\n",
      "        1.1023146 ,  0.72111845], dtype=float32)]\n",
      "Epoch 58/10000\n",
      "51/51 [==============================] - 91s 2s/step - loss: 245.1641\n",
      "Beta: [array([ 1.3279634 ,  0.33520442, -0.96529335,  0.92987335,  0.19180174,\n",
      "       -0.46831015,  1.0372903 ,  0.8246458 , -0.09300495, -0.04669346,\n",
      "        1.1181126 ,  0.73180073], dtype=float32)]\n",
      "Epoch 59/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 244.9037\n",
      "Beta: [array([ 1.3392287 ,  0.3305175 , -0.97601336,  0.9248426 ,  0.18409702,\n",
      "       -0.47936004,  1.046854  ,  0.83324915, -0.09891038, -0.06322992,\n",
      "        1.1336763 ,  0.7427625 ], dtype=float32)]\n",
      "Epoch 60/10000\n",
      "51/51 [==============================] - 84s 2s/step - loss: 244.9843\n",
      "Beta: [array([ 1.3469899 ,  0.3258474 , -0.9839556 ,  0.9165363 ,  0.1719409 ,\n",
      "       -0.49032354,  1.0601889 ,  0.8368899 , -0.10518627, -0.07971188,\n",
      "        1.1492246 ,  0.75232625], dtype=float32)]\n",
      "Epoch 61/10000\n",
      "51/51 [==============================] - 87s 2s/step - loss: 244.7305\n",
      "Beta: [array([ 1.3601704 ,  0.32119015, -0.99654096,  0.9134926 ,  0.16716787,\n",
      "       -0.5014046 ,  1.0680958 ,  0.8478361 , -0.10807662, -0.09610122,\n",
      "        1.1628281 ,  0.76475614], dtype=float32)]\n",
      "Epoch 62/10000\n",
      "51/51 [==============================] - 82s 2s/step - loss: 244.4202\n",
      "Beta: [array([ 1.3678731 ,  0.31643042, -1.0042026 ,  0.90687126,  0.15700789,\n",
      "       -0.512412  ,  1.079894  ,  0.8532335 , -0.1160982 , -0.11246184,\n",
      "        1.1804904 ,  0.7722161 ], dtype=float32)]\n",
      "Epoch 63/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 244.4277\n",
      "Beta: [array([ 1.3776156 ,  0.31167856, -1.0159671 ,  0.9002568 ,  0.14927597,\n",
      "       -0.5235292 ,  1.0870953 ,  0.86115134, -0.12185573, -0.12873949,\n",
      "        1.1942499 ,  0.78286666], dtype=float32)]\n",
      "Epoch 64/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 244.3387\n",
      "Beta: [array([ 1.3884186 ,  0.30692917, -1.024726  ,  0.8959432 ,  0.14312376,\n",
      "       -0.53444535,  1.0982727 ,  0.8678046 , -0.12740402, -0.14496247,\n",
      "        1.2099466 ,  0.79258907], dtype=float32)]\n",
      "Epoch 65/10000\n",
      "51/51 [==============================] - 84s 2s/step - loss: 244.0646\n",
      "Beta: [array([ 1.3973154 ,  0.30209586, -1.0360919 ,  0.89094937,  0.13553427,\n",
      "       -0.5455726 ,  1.1062679 ,  0.8763697 , -0.13149337, -0.16125771,\n",
      "        1.2232176 ,  0.8031308 ], dtype=float32)]\n",
      "Epoch 66/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 243.9971\n",
      "Beta: [array([ 1.4083402 ,  0.29729307, -1.0463012 ,  0.8870051 ,  0.13035308,\n",
      "       -0.5564909 ,  1.1156247 ,  0.8838254 , -0.13594946, -0.17714767,\n",
      "        1.2371628 ,  0.81428707], dtype=float32)]\n",
      "Epoch 67/10000\n",
      "51/51 [==============================] - 93s 2s/step - loss: 244.2023\n",
      "Beta: [array([ 1.4163035 ,  0.29240066, -1.054104  ,  0.88134027,  0.12037036,\n",
      "       -0.56752396,  1.1279415 ,  0.8881813 , -0.14010926, -0.19323269,\n",
      "        1.251977  ,  0.8240266 ], dtype=float32)]\n",
      "Epoch 68/10000\n",
      "51/51 [==============================] - 84s 2s/step - loss: 243.8697\n",
      "Beta: [array([ 1.4248307 ,  0.2875396 , -1.0645994 ,  0.8758052 ,  0.11449303,\n",
      "       -0.57844883,  1.1352752 ,  0.89590245, -0.14817289, -0.2092452 ,\n",
      "        1.2660085 ,  0.832626  ], dtype=float32)]\n",
      "Epoch 69/10000\n",
      "51/51 [==============================] - 94s 2s/step - loss: 243.7860\n",
      "Beta: [array([ 1.4345232 ,  0.28264174, -1.0754656 ,  0.87239695,  0.10922094,\n",
      "       -0.5893162 ,  1.1429669 ,  0.9037724 , -0.15023978, -0.22490332,\n",
      "        1.277082  ,  0.8444755 ], dtype=float32)]\n",
      "Epoch 70/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 243.6324\n",
      "Beta: [array([ 1.4415257 ,  0.2776891 , -1.0840693 ,  0.8669778 ,  0.10067736,\n",
      "       -0.60039836,  1.1533746 ,  0.9096119 , -0.15792459, -0.2409152 ,\n",
      "        1.2927046 ,  0.8526946 ], dtype=float32)]\n",
      "Epoch 71/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 243.6344\n",
      "Beta: [array([ 1.448036  ,  0.27270213, -1.091834  ,  0.861013  ,  0.09265814,\n",
      "       -0.6110785 ,  1.1649677 ,  0.91479737, -0.1644956 , -0.25660843,\n",
      "        1.3064364 ,  0.8606285 ], dtype=float32)]\n",
      "Epoch 72/10000\n",
      "51/51 [==============================] - 89s 2s/step - loss: 243.4708\n",
      "Beta: [array([ 1.4576086 ,  0.26771823, -1.1015921 ,  0.8582119 ,  0.08779831,\n",
      "       -0.62212384,  1.1736927 ,  0.92218345, -0.16831496, -0.2723148 ,\n",
      "        1.3201876 ,  0.86989444], dtype=float32)]\n",
      "Epoch 73/10000\n",
      "51/51 [==============================] - 87s 2s/step - loss: 243.4612\n",
      "Beta: [array([ 1.4654685 ,  0.26271406, -1.1129347 ,  0.85307634,  0.08347511,\n",
      "       -0.63273937,  1.1797429 ,  0.9297    , -0.17104408, -0.287732  ,\n",
      "        1.3306879 ,  0.88095653], dtype=float32)]\n",
      "Epoch 74/10000\n",
      "51/51 [==============================] - 90s 2s/step - loss: 243.3431\n",
      "Beta: [array([ 1.4732434 ,  0.2576084 , -1.120437  ,  0.8486884 ,  0.07633174,\n",
      "       -0.64375407,  1.1913772 ,  0.93441206, -0.17600505, -0.30341086,\n",
      "        1.3441285 ,  0.88916314], dtype=float32)]\n",
      "Epoch 75/10000\n",
      "51/51 [==============================] - 89s 2s/step - loss: 243.0895\n",
      "Beta: [array([ 1.4801155 ,  0.25255787, -1.128169  ,  0.8431704 ,  0.07132342,\n",
      "       -0.6543015 ,  1.2011511 ,  0.9392165 , -0.18083163, -0.3188149 ,\n",
      "        1.3573688 ,  0.8976065 ], dtype=float32)]\n",
      "Epoch 76/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 242.8991\n",
      "Beta: [array([ 1.488092  ,  0.24746184, -1.137276  ,  0.8406788 ,  0.06660259,\n",
      "       -0.66504675,  1.2091428 ,  0.946882  , -0.18341726, -0.33419228,\n",
      "        1.3694824 ,  0.9064796 ], dtype=float32)]\n",
      "Epoch 77/10000\n",
      "51/51 [==============================] - 90s 2s/step - loss: 243.2295\n",
      "Beta: [array([ 1.4952147 ,  0.24233674, -1.1469275 ,  0.8353494 ,  0.0627464 ,\n",
      "       -0.67571825,  1.2171808 ,  0.95246583, -0.18659972, -0.34949774,\n",
      "        1.3794225 ,  0.91599685], dtype=float32)]\n",
      "Epoch 78/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 243.3714\n",
      "Beta: [array([ 1.5008281 ,  0.23711628, -1.1539954 ,  0.8307788 ,  0.05812837,\n",
      "       -0.6866422 ,  1.2260183 ,  0.95818794, -0.19352005, -0.36494744,\n",
      "        1.394174  ,  0.92241377], dtype=float32)]\n",
      "Epoch 79/10000\n",
      "51/51 [==============================] - 92s 2s/step - loss: 243.2048\n",
      "Beta: [array([ 1.5074146 ,  0.23191552, -1.1623324 ,  0.82642436,  0.05176912,\n",
      "       -0.69719195,  1.2355549 ,  0.9628277 , -0.19565491, -0.3800889 ,\n",
      "        1.40526   ,  0.9317873 ], dtype=float32)]\n",
      "Epoch 80/10000\n",
      "51/51 [==============================] - 89s 2s/step - loss: 243.1251\n",
      "Beta: [array([ 1.5137932 ,  0.22661616, -1.1720574 ,  0.8237112 ,  0.04795063,\n",
      "       -0.708246  ,  1.2422718 ,  0.9704822 , -0.2020492 , -0.39575118,\n",
      "        1.4169377 ,  0.9394174 ], dtype=float32)]\n",
      "Epoch 81/10000\n",
      "51/51 [==============================] - 91s 2s/step - loss: 242.9309\n",
      "Beta: [array([ 1.5193483 ,  0.22128929, -1.1800278 ,  0.82044214,  0.04416556,\n",
      "       -0.7187847 ,  1.2507993 ,  0.97604614, -0.20450613, -0.41066658,\n",
      "        1.4267058 ,  0.94810474], dtype=float32)]\n",
      "Epoch 82/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 242.7849\n",
      "Beta: [array([ 1.526308  ,  0.21597806, -1.1868013 ,  0.81452435,  0.04103474,\n",
      "       -0.7296237 ,  1.2596916 ,  0.98010015, -0.20788583, -0.4259121 ,\n",
      "        1.4399072 ,  0.95564026], dtype=float32)]\n",
      "Epoch 83/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 242.9445\n",
      "Beta: [array([ 1.5310822 ,  0.21058759, -1.1970108 ,  0.81160426,  0.03726134,\n",
      "       -0.74034274,  1.2644192 ,  0.98795795, -0.21297027, -0.44102207,\n",
      "        1.4505754 ,  0.9628157 ], dtype=float32)]\n",
      "Epoch 84/10000\n",
      "51/51 [==============================] - 92s 2s/step - loss: 242.5727\n",
      "Beta: [array([ 1.5377244 ,  0.2051984 , -1.2050061 ,  0.8082493 ,  0.03346582,\n",
      "       -0.75088394,  1.2737714 ,  0.99297196, -0.21440369, -0.45590758,\n",
      "        1.4604956 ,  0.9722245 ], dtype=float32)]\n",
      "Epoch 85/10000\n",
      "51/51 [==============================] - 95s 2s/step - loss: 242.7822\n",
      "Beta: [array([ 1.5404392 ,  0.19971958, -1.209418  ,  0.8040579 ,  0.02748736,\n",
      "       -0.7619958 ,  1.2852799 ,  0.9959684 , -0.22159365, -0.47116515,\n",
      "        1.4752394 ,  0.9770824 ], dtype=float32)]\n",
      "Epoch 86/10000\n",
      "51/51 [==============================] - 93s 2s/step - loss: 242.7975\n",
      "Beta: [array([ 1.5466478 ,  0.19422829, -1.2169806 ,  0.8004666 ,  0.02637204,\n",
      "       -0.7730089 ,  1.2923931 ,  1.0015441 , -0.22444165, -0.48627523,\n",
      "        1.4858375 ,  0.9847143 ], dtype=float32)]\n",
      "Epoch 87/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 242.5413\n",
      "Beta: [array([ 1.5525867 ,  0.18869273, -1.2260036 ,  0.7970681 ,  0.0221125 ,\n",
      "       -0.7840308 ,  1.3001357 ,  1.0059332 , -0.2232805 , -0.5010892 ,\n",
      "        1.4928259 ,  0.9953    ], dtype=float32)]\n",
      "Epoch 88/10000\n",
      "51/51 [==============================] - 86s 2s/step - loss: 242.6240\n",
      "Beta: [array([ 1.5560669 ,  0.18310921, -1.2331148 ,  0.79339147,  0.01797664,\n",
      "       -0.79521763,  1.3078104 ,  1.0113614 , -0.2294062 , -0.51620173,\n",
      "        1.5042763 ,  1.0014368 ], dtype=float32)]\n",
      "Epoch 89/10000\n",
      "51/51 [==============================] - 90s 2s/step - loss: 242.3536\n",
      "Beta: [array([ 1.5599321 ,  0.17746998, -1.2390405 ,  0.78955764,  0.01479039,\n",
      "       -0.80621415,  1.3166553 ,  1.0152398 , -0.23332652, -0.5311939 ,\n",
      "        1.5154823 ,  1.0076859 ], dtype=float32)]\n",
      "Epoch 90/10000\n",
      "51/51 [==============================] - 87s 2s/step - loss: 242.3265\n",
      "Beta: [array([ 1.5641445 ,  0.17175609, -1.2465136 ,  0.7867521 ,  0.01163147,\n",
      "       -0.81739736,  1.3232889 ,  1.0214763 , -0.23702149, -0.54617774,\n",
      "        1.5263011 ,  1.0137088 ], dtype=float32)]\n",
      "Epoch 91/10000\n",
      "51/51 [==============================] - 86s 2s/step - loss: 242.3484\n",
      "Beta: [array([ 1.5693314 ,  0.16610791, -1.2548066 ,  0.78335583,  0.00993219,\n",
      "       -0.8282311 ,  1.3291533 ,  1.0262696 , -0.23984055, -0.56089365,\n",
      "        1.5355842 ,  1.021475  ], dtype=float32)]\n",
      "Epoch 92/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 242.5317\n",
      "Beta: [array([ 1.5751761 ,  0.1603092 , -1.2631017 ,  0.7830611 ,  0.01061052,\n",
      "       -0.83933115,  1.3346859 ,  1.0339181 , -0.24012825, -0.5757572 ,\n",
      "        1.5441953 ,  1.0298686 ], dtype=float32)]\n",
      "Epoch 93/10000\n",
      "51/51 [==============================] - 84s 2s/step - loss: 242.4717\n",
      "Beta: [array([ 1.5753851 ,  0.15449686, -1.2665825 ,  0.77626026,  0.00288245,\n",
      "       -0.8502755 ,  1.345678  ,  1.0340154 , -0.2472573 , -0.5905358 ,\n",
      "        1.5571642 ,  1.0339811 ], dtype=float32)]\n",
      "Epoch 94/10000\n",
      "51/51 [==============================] - 86s 2s/step - loss: 242.0868\n",
      "Beta: [array([ 1.5810972 ,  0.14859273, -1.2741523 ,  0.772199  ,  0.0047774 ,\n",
      "       -0.8612227 ,  1.3507426 ,  1.0395854 , -0.24784842, -0.6051561 ,\n",
      "        1.5652759 ,  1.0408467 ], dtype=float32)]\n",
      "Epoch 95/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 242.4782\n",
      "Beta: [array([ 1.5855693e+00,  1.4272736e-01, -1.2821645e+00,  7.7162248e-01,\n",
      "       -1.8083025e-04, -8.7210822e-01,  1.3586591e+00,  1.0451452e+00,\n",
      "       -2.4857400e-01, -6.1979461e-01,  1.5734890e+00,  1.0489324e+00],\n",
      "      dtype=float32)]\n",
      "Epoch 96/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 242.2163\n",
      "Beta: [array([ 1.586056  ,  0.13669378, -1.2867181 ,  0.76773286, -0.00400966,\n",
      "       -0.8833164 ,  1.3677489 ,  1.0480309 , -0.2542058 , -0.63463104,\n",
      "        1.583337  ,  1.0545335 ], dtype=float32)]\n",
      "Epoch 97/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 242.3156\n",
      "Beta: [array([ 1.5881763 ,  0.13063101, -1.2931106 ,  0.7641702 , -0.00608601,\n",
      "       -0.89453906,  1.3738781 ,  1.0522821 , -0.258751  , -0.6494972 ,\n",
      "        1.5950167 ,  1.0586413 ], dtype=float32)]\n",
      "Epoch 98/10000\n",
      "51/51 [==============================] - 90s 2s/step - loss: 242.0835\n",
      "Beta: [array([ 1.5924608 ,  0.12449957, -1.2989864 ,  0.7607123 , -0.00581387,\n",
      "       -0.90567625,  1.3801069 ,  1.057583  , -0.25951058, -0.66434157,\n",
      "        1.6043088 ,  1.0646586 ], dtype=float32)]\n",
      "Epoch 99/10000\n",
      "51/51 [==============================] - 87s 2s/step - loss: 241.8307\n",
      "Beta: [array([ 1.5941837 ,  0.1184217 , -1.3043822 ,  0.7551864 , -0.01034252,\n",
      "       -0.9167067 ,  1.3881239 ,  1.0594052 , -0.26169404, -0.67886734,\n",
      "        1.6128733 ,  1.0700881 ], dtype=float32)]\n",
      "Epoch 100/10000\n",
      "51/51 [==============================] - 91s 2s/step - loss: 242.2142\n",
      "Beta: [array([ 1.5997816 ,  0.11222419, -1.3126293 ,  0.7558053 , -0.00861243,\n",
      "       -0.927739  ,  1.3924311 ,  1.0666624 , -0.2624338 , -0.6935113 ,\n",
      "        1.6204883 ,  1.0783119 ], dtype=float32)]\n",
      "Epoch 101/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 242.0702\n",
      "Beta: [array([ 1.600302  ,  0.10602877, -1.3180171 ,  0.75251526, -0.012834  ,\n",
      "       -0.9387479 ,  1.4006481 ,  1.0683768 , -0.26580885, -0.70795494,\n",
      "        1.6294441 ,  1.0845898 ], dtype=float32)]\n",
      "Epoch 102/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 241.9519\n",
      "Beta: [array([ 1.6023993 ,  0.09979267, -1.3235583 ,  0.75076956, -0.01528698,\n",
      "       -0.9497209 ,  1.4061722 ,  1.0735611 , -0.26972795, -0.7222644 ,\n",
      "        1.6401985 ,  1.088309  ], dtype=float32)]\n",
      "Epoch 103/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 241.7661\n",
      "Beta: [array([ 1.6073611 ,  0.09350789, -1.3296894 ,  0.7486049 , -0.01451555,\n",
      "       -0.9605887 ,  1.4133477 ,  1.0782673 , -0.26867756, -0.73662424,\n",
      "        1.646462  ,  1.0956044 ], dtype=float32)]\n",
      "Epoch 104/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 241.9074\n",
      "Beta: [array([ 1.609958  ,  0.08720847, -1.336376  ,  0.74585754, -0.01517321,\n",
      "       -0.97127396,  1.4188676 ,  1.0824262 , -0.2708973 , -0.7508281 ,\n",
      "        1.6534568 ,  1.1017685 ], dtype=float32)]\n",
      "Epoch 105/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 241.9007\n",
      "Beta: [array([ 1.6095473 ,  0.08083175, -1.3387579 ,  0.7417724 , -0.01981524,\n",
      "       -0.98212934,  1.4290516 ,  1.082537  , -0.27365243, -0.76502484,\n",
      "        1.6627905 ,  1.106318  ], dtype=float32)]\n",
      "Epoch 106/10000\n",
      "51/51 [==============================] - 79s 2s/step - loss: 241.7376\n",
      "Beta: [array([ 1.6133559 ,  0.07438746, -1.3464521 ,  0.74079096, -0.01959555,\n",
      "       -0.992829  ,  1.4330583 ,  1.0891541 , -0.27500647, -0.77920324,\n",
      "        1.6708528 ,  1.1124389 ], dtype=float32)]\n",
      "Epoch 107/10000\n",
      "51/51 [==============================] - 79s 2s/step - loss: 241.9695\n",
      "Beta: [array([ 1.6160941 ,  0.06801163, -1.3522605 ,  0.7386859 , -0.01872615,\n",
      "       -1.0035055 ,  1.4372126 ,  1.0934346 , -0.2753727 , -0.7930792 ,\n",
      "        1.6776494 ,  1.1183704 ], dtype=float32)]\n",
      "Epoch 108/10000\n",
      "51/51 [==============================] - 82s 2s/step - loss: 241.6832\n",
      "Beta: [array([ 1.6171654 ,  0.06152159, -1.3581328 ,  0.735132  , -0.02108195,\n",
      "       -1.0141531 ,  1.4431736 ,  1.0970631 , -0.27717555, -0.80702895,\n",
      "        1.6851125 ,  1.123082  ], dtype=float32)]\n",
      "Epoch 109/10000\n",
      "51/51 [==============================] - 92s 2s/step - loss: 241.8211\n",
      "Beta: [array([ 1.6190907 ,  0.05502349, -1.3625127 ,  0.73449636, -0.02086535,\n",
      "       -1.0247213 ,  1.4491905 ,  1.1018977 , -0.2803994 , -0.82098603,\n",
      "        1.6949483 ,  1.1276327 ], dtype=float32)]\n",
      "Epoch 110/10000\n",
      "51/51 [==============================] - 87s 2s/step - loss: 241.7707\n",
      "Beta: [array([ 1.6212226 ,  0.04852046, -1.3668678 ,  0.7305046 , -0.02313927,\n",
      "       -1.0353286 ,  1.456181  ,  1.103815  , -0.28155512, -0.83473414,\n",
      "        1.7029756 ,  1.1320567 ], dtype=float32)]\n",
      "Epoch 111/10000\n",
      "51/51 [==============================] - 89s 2s/step - loss: 241.7769\n",
      "Beta: [array([ 1.6223314 ,  0.04197736, -1.3745881 ,  0.73008835, -0.02473297,\n",
      "       -1.0457598 ,  1.4600283 ,  1.1082579 , -0.28225628, -0.8483448 ,\n",
      "        1.7070378 ,  1.1404731 ], dtype=float32)]\n",
      "Epoch 112/10000\n",
      "51/51 [==============================] - 81s 2s/step - loss: 241.4853\n",
      "Beta: [array([ 1.620621  ,  0.03535433, -1.3752599 ,  0.72388506, -0.02830586,\n",
      "       -1.05625   ,  1.4693578 ,  1.1085272 , -0.28760397, -0.8618927 ,\n",
      "        1.7185179 ,  1.1407433 ], dtype=float32)]\n",
      "Epoch 113/10000\n",
      "51/51 [==============================] - 90s 2s/step - loss: 241.8047\n",
      "Beta: [array([ 1.6262782 ,  0.02875451, -1.3835499 ,  0.7251961 , -0.02427097,\n",
      "       -1.066602  ,  1.4703591 ,  1.1157734 , -0.2861096 , -0.87537354,\n",
      "        1.7233806 ,  1.1481504 ], dtype=float32)]\n",
      "Epoch 114/10000\n",
      "51/51 [==============================] - 89s 2s/step - loss: 241.6680\n",
      "Beta: [array([ 1.626242  ,  0.02206535, -1.386788  ,  0.7213865 , -0.02665859,\n",
      "       -1.0769368 ,  1.4787692 ,  1.1162047 , -0.2864105 , -0.88896155,\n",
      "        1.7299143 ,  1.1535758 ], dtype=float32)]\n",
      "Epoch 115/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 241.8174\n",
      "Beta: [array([ 1.6260831 ,  0.01537102, -1.3932239 ,  0.71937686, -0.02744516,\n",
      "       -1.0872848 ,  1.4811748 ,  1.1219984 , -0.28991958, -0.90236753,\n",
      "        1.7371445 ,  1.1570313 ], dtype=float32)]\n",
      "Epoch 116/10000\n",
      "51/51 [==============================] - 89s 2s/step - loss: 241.7734\n",
      "Beta: [array([ 1.6281573 ,  0.00864858, -1.3957158 ,  0.716101  , -0.02735337,\n",
      "       -1.0975679 ,  1.4881314 ,  1.1229553 , -0.2886111 , -0.91587293,\n",
      "        1.7443324 ,  1.1617146 ], dtype=float32)]\n",
      "Epoch 117/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 241.5000\n",
      "Beta: [array([ 1.6296927 ,  0.00188406, -1.4004742 ,  0.7162049 , -0.02907653,\n",
      "       -1.1079848 ,  1.4945546 ,  1.1265724 , -0.29147878, -0.9291978 ,\n",
      "        1.7528716 ,  1.1662468 ], dtype=float32)]\n",
      "Epoch 118/10000\n",
      "51/51 [==============================] - 85s 2s/step - loss: 241.2817\n",
      "Beta: [array([ 1.6306766 , -0.00501634, -1.4060742 ,  0.71500325, -0.03007105,\n",
      "       -1.1183158 ,  1.4990585 ,  1.1299393 , -0.2913434 , -0.94239515,\n",
      "        1.7570416 ,  1.1725154 ], dtype=float32)]\n",
      "Epoch 119/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 241.6266\n",
      "Beta: [array([ 1.6328188 , -0.01175513, -1.4106147 ,  0.71168786, -0.02799176,\n",
      "       -1.1285136 ,  1.5026845 ,  1.1333746 , -0.29176375, -0.95556355,\n",
      "        1.763758  ,  1.1768032 ], dtype=float32)]\n",
      "Epoch 120/10000\n",
      "51/51 [==============================] - 84s 2s/step - loss: 241.4648\n",
      "Beta: [array([ 1.6301348 , -0.01869175, -1.4125092 ,  0.7083193 , -0.03096288,\n",
      "       -1.1388681 ,  1.5092036 ,  1.1359098 , -0.29657257, -0.96893317,\n",
      "        1.7747536 ,  1.1768744 ], dtype=float32)]\n",
      "Epoch 121/10000\n",
      "51/51 [==============================] - 80s 2s/step - loss: 241.6228\n",
      "Beta: [array([ 1.6323429 , -0.02562426, -1.4201363 ,  0.7080327 , -0.0318759 ,\n",
      "       -1.149221  ,  1.5125968 ,  1.139643  , -0.29571474, -0.9821689 ,\n",
      "        1.7770394 ,  1.18551   ], dtype=float32)]\n",
      "Epoch 122/10000\n",
      "51/51 [==============================] - 88s 2s/step - loss: 241.4070\n",
      "Beta: [array([ 1.6328213 , -0.03261819, -1.4219692 ,  0.70720375, -0.03078676,\n",
      "       -1.1594528 ,  1.519484  ,  1.1419046 , -0.29657328, -0.99527127,\n",
      "        1.7848742 ,  1.1887946 ], dtype=float32)]\n",
      "Epoch 123/10000\n",
      "51/51 [==============================] - 83s 2s/step - loss: 241.3727\n",
      "Beta: [array([ 1.6332155 , -0.03964011, -1.4285934 ,  0.7054306 , -0.03143323,\n",
      "       -1.1698008 ,  1.5214592 ,  1.1460407 , -0.29695314, -1.0084885 ,\n",
      "        1.7890543 ,  1.1945065 ], dtype=float32)]\n",
      "Epoch 124/10000\n",
      "33/51 [==================>...........] - ETA: 29s - loss: 244.2448"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m      6\u001B[0m     loss \u001B[38;5;241m=\u001B[39m compiler\u001B[38;5;241m.\u001B[39mloss,\n\u001B[1;32m      7\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m compiler\u001B[38;5;241m.\u001B[39moptimizer,\n\u001B[1;32m      8\u001B[0m     run_eagerly \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#fit_model(model, adstock, output)\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[43mfit_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madstock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/AttributionSingleUser/src/hmm_package/generate_hmm.py:321\u001B[0m, in \u001B[0;36mfit_model\u001B[0;34m(model, adstock, emission_real)\u001B[0m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_model\u001B[39m(model, adstock, emission_real):\n\u001B[1;32m    320\u001B[0m     print_weights \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mLambdaCallback(on_epoch_begin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m batch, logs: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBeta: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(model\u001B[38;5;241m.\u001B[39mget_weights())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 321\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43madstock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m                     \u001B[49m\u001B[43memission_real\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mprint_weights\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:1384\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1378\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1379\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   1380\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m   1381\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   1382\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1383\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1384\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1385\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1386\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:1021\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m   1019\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_function\u001B[39m(iterator):\n\u001B[1;32m   1020\u001B[0m   \u001B[38;5;124;03m\"\"\"Runs a training execution with a single step.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1021\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mstep_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:1010\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function\u001B[0;34m(model, iterator)\u001B[0m\n\u001B[1;32m   1007\u001B[0m   run_step \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mfunction(\n\u001B[1;32m   1008\u001B[0m       run_step, jit_compile\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, experimental_relax_shapes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1009\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m-> 1010\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1011\u001B[0m outputs \u001B[38;5;241m=\u001B[39m reduce_per_replica(\n\u001B[1;32m   1012\u001B[0m     outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirst\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   1013\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1312\u001B[0m, in \u001B[0;36mStrategyBase.run\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscope():\n\u001B[1;32m   1308\u001B[0m   \u001B[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001B[39;00m\n\u001B[1;32m   1309\u001B[0m   \u001B[38;5;66;03m# applied when the caller is also in Eager mode.\u001B[39;00m\n\u001B[1;32m   1310\u001B[0m   fn \u001B[38;5;241m=\u001B[39m autograph\u001B[38;5;241m.\u001B[39mtf_convert(\n\u001B[1;32m   1311\u001B[0m       fn, autograph_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx(), convert_by_default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m-> 1312\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extended\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_for_each_replica\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2888\u001B[0m, in \u001B[0;36mStrategyExtendedV1.call_for_each_replica\u001B[0;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[1;32m   2886\u001B[0m   kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2887\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy()\u001B[38;5;241m.\u001B[39mscope():\n\u001B[0;32m-> 2888\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_for_each_replica\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3689\u001B[0m, in \u001B[0;36m_DefaultDistributionExtended._call_for_each_replica\u001B[0;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[1;32m   3687\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_for_each_replica\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn, args, kwargs):\n\u001B[1;32m   3688\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ReplicaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy(), replica_id_in_sync_group\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m-> 3689\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:595\u001B[0m, in \u001B[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    594\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ag_ctx\u001B[38;5;241m.\u001B[39mControlStatusCtx(status\u001B[38;5;241m=\u001B[39mag_ctx\u001B[38;5;241m.\u001B[39mStatus\u001B[38;5;241m.\u001B[39mUNSPECIFIED):\n\u001B[0;32m--> 595\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:1000\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    999\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_step\u001B[39m(data):\n\u001B[0;32m-> 1000\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1001\u001B[0m   \u001B[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:863\u001B[0m, in \u001B[0;36mModel.train_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    861\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_target_and_loss(y, loss)\n\u001B[1;32m    862\u001B[0m \u001B[38;5;66;03m# Run backwards pass.\u001B[39;00m\n\u001B[0;32m--> 863\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainable_variables\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:530\u001B[0m, in \u001B[0;36mOptimizerV2.minimize\u001B[0;34m(self, loss, var_list, grad_loss, name, tape)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mminimize\u001B[39m(\u001B[38;5;28mself\u001B[39m, loss, var_list, grad_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, tape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    500\u001B[0m   \u001B[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001B[39;00m\n\u001B[1;32m    501\u001B[0m \n\u001B[1;32m    502\u001B[0m \u001B[38;5;124;03m  This method simply computes gradient using `tf.GradientTape` and calls\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    528\u001B[0m \n\u001B[1;32m    529\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 530\u001B[0m   grads_and_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute_gradients\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m      \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvar_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    532\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_gradients(grads_and_vars, name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:583\u001B[0m, in \u001B[0;36mOptimizerV2._compute_gradients\u001B[0;34m(self, loss, var_list, grad_loss, tape)\u001B[0m\n\u001B[1;32m    581\u001B[0m var_list \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(var_list)\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mname_scope(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/gradients\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 583\u001B[0m   grads_and_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assert_valid_dtypes([\n\u001B[1;32m    586\u001B[0m     v \u001B[38;5;28;01mfor\u001B[39;00m g, v \u001B[38;5;129;01min\u001B[39;00m grads_and_vars\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m g \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m v\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m tf\u001B[38;5;241m.\u001B[39mresource\n\u001B[1;32m    588\u001B[0m ])\n\u001B[1;32m    590\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grads_and_vars\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:464\u001B[0m, in \u001B[0;36mOptimizerV2._get_gradients\u001B[0;34m(self, tape, loss, var_list, grad_loss)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_gradients\u001B[39m(\u001B[38;5;28mself\u001B[39m, tape, loss, var_list, grad_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    463\u001B[0m   \u001B[38;5;124;03m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 464\u001B[0m   grads \u001B[38;5;241m=\u001B[39m \u001B[43mtape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    465\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(grads, var_list))\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py:1081\u001B[0m, in \u001B[0;36mGradientTape.gradient\u001B[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[1;32m   1077\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_gradients \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1078\u001B[0m   output_gradients \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x)\n\u001B[1;32m   1079\u001B[0m                       \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m nest\u001B[38;5;241m.\u001B[39mflatten(output_gradients)]\n\u001B[0;32m-> 1081\u001B[0m flat_grad \u001B[38;5;241m=\u001B[39m \u001B[43mimperative_grad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimperative_grad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1082\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_targets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1084\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_sources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1085\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_gradients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_gradients\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1086\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources_raw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mflat_sources_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1087\u001B[0m \u001B[43m    \u001B[49m\u001B[43munconnected_gradients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munconnected_gradients\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1089\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_persistent:\n\u001B[1;32m   1090\u001B[0m   \u001B[38;5;66;03m# Keep track of watched variables before setting tape to None\u001B[39;00m\n\u001B[1;32m   1091\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_watched_variables \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tape\u001B[38;5;241m.\u001B[39mwatched_variables()\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001B[0m, in \u001B[0;36mimperative_grad\u001B[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     65\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown value for unconnected_gradients: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m unconnected_gradients)\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_TapeGradient\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_gradients\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43munconnected_gradients\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py:156\u001B[0m, in \u001B[0;36m_gradient_function\u001B[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[1;32m    154\u001B[0m     gradient_name_scope \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m forward_pass_name_scope \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mname_scope(gradient_name_scope):\n\u001B[0;32m--> 156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgrad_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmock_op\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mout_grads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    158\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m grad_fn(mock_op, \u001B[38;5;241m*\u001B[39mout_grads)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_grad.py:520\u001B[0m, in \u001B[0;36m_MatrixSetDiagGradV3\u001B[0;34m(op, grad)\u001B[0m\n\u001B[1;32m    514\u001B[0m   \u001B[38;5;66;03m# pyformat: enable\u001B[39;00m\n\u001B[1;32m    515\u001B[0m   \u001B[38;5;66;03m# pylint: enable=g-long-lambda\u001B[39;00m\n\u001B[1;32m    516\u001B[0m   diag_shape \u001B[38;5;241m=\u001B[39m array_ops\u001B[38;5;241m.\u001B[39mconcat([batch_shape, postfix], \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    518\u001B[0m grad_input \u001B[38;5;241m=\u001B[39m array_ops\u001B[38;5;241m.\u001B[39mmatrix_set_diag(\n\u001B[1;32m    519\u001B[0m     grad,\n\u001B[0;32m--> 520\u001B[0m     \u001B[43marray_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiag_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    521\u001B[0m     k\u001B[38;5;241m=\u001B[39mop\u001B[38;5;241m.\u001B[39minputs[\u001B[38;5;241m2\u001B[39m],\n\u001B[1;32m    522\u001B[0m     align\u001B[38;5;241m=\u001B[39malign)\n\u001B[1;32m    523\u001B[0m grad_diag \u001B[38;5;241m=\u001B[39m array_ops\u001B[38;5;241m.\u001B[39mmatrix_diag_part(grad, k\u001B[38;5;241m=\u001B[39mop\u001B[38;5;241m.\u001B[39minputs[\u001B[38;5;241m2\u001B[39m], align\u001B[38;5;241m=\u001B[39malign)\n\u001B[1;32m    524\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (grad_input, grad_diag, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1082\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch_target\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1083\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   1084\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[1;32m   1085\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[1;32m   1086\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2927\u001B[0m, in \u001B[0;36m_tag_zeros_tensor.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   2926\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 2927\u001B[0m   tensor \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2928\u001B[0m   tensor\u001B[38;5;241m.\u001B[39m_is_zeros_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2929\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m tensor\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2988\u001B[0m, in \u001B[0;36mzeros\u001B[0;34m(shape, dtype, name)\u001B[0m\n\u001B[1;32m   2986\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m shape\u001B[38;5;241m.\u001B[39m_shape_tuple():\n\u001B[1;32m   2987\u001B[0m     shape \u001B[38;5;241m=\u001B[39m reshape(shape, [\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])  \u001B[38;5;66;03m# Ensure it's a vector\u001B[39;00m\n\u001B[0;32m-> 2988\u001B[0m   output \u001B[38;5;241m=\u001B[39m \u001B[43mfill\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconstant\u001B[49m\u001B[43m(\u001B[49m\u001B[43mzero\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2989\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m output\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mbase_dtype \u001B[38;5;241m==\u001B[39m dtype\n\u001B[1;32m   2990\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:139\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21merror_handler\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 139\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_traceback_filtering_enabled():\n\u001B[1;32m    141\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "MU = [ -2.7419195, -7.4647417, -1.1010361, -5.92728  ]\n",
    "model = build_hmm_to_fit_beta( states_observable=STATES_ARE_OBSERVABLE, mu=MU )\n",
    "\n",
    "compiler = CompilerInfoBeta(LR_EXPONENTIAL_DECAY)\n",
    "model.compile(\n",
    "    loss = compiler.loss,\n",
    "    optimizer = compiler.optimizer,\n",
    "    run_eagerly = True\n",
    ")\n",
    "\n",
    "#fit_model(model, adstock, output)\n",
    "fit_model(model, adstock, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adstock_with_empty' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m fit_model(model, \u001B[43madstock_with_empty\u001B[49m, output_with_empty)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'adstock_with_empty' is not defined"
     ]
    }
   ],
   "source": [
    "fit_model(model, adstock_with_empty, output_with_empty)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "t = make_transition_matrix(model.weights[0], model.weights[1], adstock_with_empty[-2:-1,:,:])\n",
    "hmm = tfd.HiddenMarkovModel(\n",
    "            initial_distribution=generate_hmm_distributions(transition_matrix=t, states_observable=False)['initial_distribution'],\n",
    "            transition_distribution=tfd.Categorical(probs=t),\n",
    "            observation_distribution=generate_hmm_distributions(transition_matrix=t, states_observable=False)['observation_distribution'],\n",
    "            time_varying_transition_distribution=True,\n",
    "            num_steps=execution_duration + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(30, 3, 3), dtype=float32, numpy=\narray([[[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]],\n\n       [[9.9954051e-01, 3.6151949e-04, 9.7974400e-05],\n        [4.9749774e-01, 4.9749786e-01, 5.0044274e-03],\n        [1.0000000e-06, 1.0000000e-06, 9.9999797e-01]]], dtype=float32)>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_non_exposed_user_transtion_matrix(model.weights[0],1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 30, 3, 3), dtype=float32, numpy=\narray([[[[7.58311272e-01, 2.41560563e-01, 1.28200336e-04],\n         [2.64219254e-01, 7.23513424e-01, 1.22673288e-02],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[1.42074943e-01, 8.57896984e-01, 2.80693694e-05],\n         [1.91862099e-02, 9.36125278e-01, 4.46885377e-02],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[2.91064441e-01, 7.08864391e-01, 7.11511311e-05],\n         [3.36762033e-02, 9.41095591e-01, 2.52282154e-02],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[4.59102154e-01, 5.40764749e-01, 1.33071298e-04],\n         [5.20960540e-02, 9.32154953e-01, 1.57490131e-02],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[6.02743745e-01, 3.97056073e-01, 2.00213530e-04],\n         [7.31455162e-02, 9.16155457e-01, 1.06990281e-02],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[7.07116604e-01, 2.92621493e-01, 2.61939073e-04],\n         [9.52590480e-02, 8.96945715e-01, 7.79523747e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[7.77787745e-01, 2.21897900e-01, 3.14376026e-04],\n         [1.17019616e-01, 8.76963198e-01, 6.01720577e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[8.24874640e-01, 1.74767822e-01, 3.57502635e-04],\n         [1.37386426e-01, 8.57742190e-01, 4.87136468e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[8.56566668e-01, 1.43040791e-01, 3.92549962e-04],\n         [1.55738726e-01, 8.40159595e-01, 4.10166103e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[8.78335357e-01, 1.21243723e-01, 4.20909142e-04],\n         [1.71810031e-01, 8.24622989e-01, 3.56694846e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[8.93631458e-01, 1.05924748e-01, 4.43814468e-04],\n         [1.85585260e-01, 8.11229527e-01, 3.18522332e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.04614091e-01, 9.49236453e-02, 4.62294294e-04],\n         [1.97203711e-01, 7.99889684e-01, 2.90663191e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.12651956e-01, 8.68708342e-02, 4.77188616e-04],\n         [2.06884846e-01, 7.90415525e-01, 2.69964337e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.18631673e-01, 8.08791146e-02, 4.89181723e-04],\n         [2.14877903e-01, 7.82578468e-01, 2.54361704e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.23141479e-01, 7.63596818e-02, 4.98829468e-04],\n         [2.21431136e-01, 7.76144266e-01, 2.42462382e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.26581264e-01, 7.29121789e-02, 5.06583892e-04],\n         [2.26775244e-01, 7.70891726e-01, 2.33301753e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.29229140e-01, 7.02580288e-02, 5.12811705e-04],\n         [2.31115282e-01, 7.66622782e-01, 2.26195762e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.31282878e-01, 6.81993142e-02, 5.17810055e-04],\n         [2.34628707e-01, 7.63164759e-01, 2.20649783e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.32885468e-01, 6.65927380e-02, 5.21819107e-04],\n         [2.37465844e-01, 7.60371149e-01, 2.16300134e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.34142113e-01, 6.53328672e-02, 5.25033625e-04],\n         [2.39752382e-01, 7.58118868e-01, 2.12875335e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.35131431e-01, 6.43409714e-02, 5.27609838e-04],\n         [2.41592363e-01, 7.56305933e-01, 2.10170029e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.35912788e-01, 6.35575578e-02, 5.29673416e-04],\n         [2.43071243e-01, 7.54848480e-01, 2.08027847e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.36531425e-01, 6.29372522e-02, 5.31326456e-04],\n         [2.44258717e-01, 7.53678024e-01, 2.06328020e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.37022269e-01, 6.24450706e-02, 5.32650156e-04],\n         [2.45211527e-01, 7.52738714e-01, 2.04976974e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.37412381e-01, 6.20539077e-02, 5.33709768e-04],\n         [2.45975539e-01, 7.51985431e-01, 2.03901879e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.37722802e-01, 6.17426336e-02, 5.34558028e-04],\n         [2.46587917e-01, 7.51381636e-01, 2.03045295e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.37970102e-01, 6.14946596e-02, 5.35236963e-04],\n         [2.47078553e-01, 7.50897825e-01, 2.02362332e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.38167274e-01, 6.12969622e-02, 5.35780273e-04],\n         [2.47471526e-01, 7.50510335e-01, 2.01817439e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.38324571e-01, 6.11392260e-02, 5.36214851e-04],\n         [2.47786209e-01, 7.50199974e-01, 2.01382511e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]],\n\n        [[9.38450158e-01, 6.10133037e-02, 5.36562991e-04],\n         [2.48038143e-01, 7.49951482e-01, 2.01035128e-03],\n         [1.00000001e-10, 1.00000001e-10, 1.00000000e+00]]]],\n      dtype=float32)>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_transition_matrix(MU, model.weights, adstock[10:11,:,:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'transition_prob_layer_beta_2/beta:0' shape=(12,) dtype=float32, numpy=\n array([ 1.630846  , -0.04413975, -1.4307394 ,  0.7025837 , -0.03290017,\n        -1.1763734 ,  1.5244356 ,  1.147517  , -0.30141446, -1.0168557 ,\n         1.7960523 ,  1.1943513 ], dtype=float32)>]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[9.9624962e-01, 7.1688538e-04, 3.0336005e-03],\n       [9.8652732e-01, 7.0989016e-04, 1.2762990e-02],\n       [5.7610592e-09, 2.0286492e-10, 1.0000000e+00]], dtype=float32)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = make_transition_matrix(model.weights[0], model.weights[1], adstock_with_empty[-2:-1,:,:])\n",
    "np.linalg.matrix_power( a[0,0,:].numpy(), 30 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_with_empty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0030336005"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_power( a[0,0,:].numpy(), 30 )[0,2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}