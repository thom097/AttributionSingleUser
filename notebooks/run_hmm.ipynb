{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HMM to estimate Funnel position"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries and constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config.CONSTANTS_HMM\n",
    "from config.CONSTANTS_HMM import *\n",
    "from config.execution_parameters import *\n",
    "\n",
    "# Project libraries\n",
    "import src.hmm_package.generate_hmm\n",
    "from src.hmm_package.generate_hmm import *\n",
    "from src.plot_and_print_info.plots_and_print_info import *\n",
    "\n",
    "# Built in libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import importlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "importlib.reload(src.hmm_package.generate_hmm)\n",
    "from src.hmm_package.generate_hmm import *\n",
    "importlib.reload(config.CONSTANTS_HMM)\n",
    "from config.CONSTANTS_HMM import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute Observation and Adstock"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To study the behaviour of the Hidden Markov Model, we generate a random exposition of the users to some campaigns. The parameters for the simulation are in config/CONSTANTS_HMM.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 13:14:48.190483: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Generate Test observation\n",
    "observation = simulate_observations()\n",
    "\n",
    "# Compute Adstock\n",
    "adstock = compute_adstock(observation=observation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Real HMM to Estimate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Generate the distributions to build the Real HMM.\n",
    "# The parameter MU, describing the behaviour of a user unexposed, has been fitted in a separate notebook and is considered known.\n",
    "#NB The variable STATES_ARE_OBSERVABLE is set in config/execution_parameters.\n",
    "hmm_distributions = generate_hmm_distributions(states_observable=False, initial_state_prob_vector= INITIAL_STATE_PROB, adstock=adstock)\n",
    "\n",
    "# Create Real HMM to fit\n",
    "real_hmm = tfd.HiddenMarkovModel(\n",
    "    initial_distribution=hmm_distributions['initial_distribution'],\n",
    "    transition_distribution=hmm_distributions['transition_distribution'],\n",
    "    observation_distribution=hmm_distributions['observation_distribution'],\n",
    "    time_varying_transition_distribution=True,\n",
    "    num_steps=time+1\n",
    ")\n",
    "\n",
    "# Sample emissions\n",
    "emission_real = real_hmm.sample().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the Real HMM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of conversion is: 0.23025%.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x72 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count conversions from sampled data\n",
    "# We aim to see a conversion rate around 20/25%.\n",
    "\n",
    "tot_conversions = count_conversions(emission_real, STATES_ARE_OBSERVABLE)\n",
    "plot_sample_emissions(real_hmm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit model with real parameter for Sanity Check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 15s 367ms/step - loss: 611.1879\n",
      "Weights: [0.824744, -0.16319953, 0.1440471, 0.4043196, -0.3993249, 0.8141909, <tf.Tensor: shape=(), dtype=float32, numpy=0.8231753>]\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 611.1735\n",
      "Weights: [0.8221232, -0.1621868, 0.14460564, 0.40076667, -0.39703462, 0.8140885, <tf.Tensor: shape=(), dtype=float32, numpy=0.823174>]\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 611.1702\n",
      "Weights: [0.8211792, -0.16074081, 0.14406146, 0.4025219, -0.39767098, 0.81396717, <tf.Tensor: shape=(), dtype=float32, numpy=0.82314056>]\n",
      "Epoch 192/1000\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 611.1754\n",
      "Weights: [0.8223274, -0.16371247, 0.14501344, 0.40240315, -0.40024707, 0.8137088, <tf.Tensor: shape=(), dtype=float32, numpy=0.8233549>]\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 611.1766\n",
      "Weights: [0.82096136, -0.16149496, 0.14359011, 0.40077522, -0.39828348, 0.8130821, <tf.Tensor: shape=(), dtype=float32, numpy=0.82358825>]\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 611.1782\n",
      "Weights: [0.8197325, -0.15984571, 0.14333776, 0.3999259, -0.3969444, 0.81253636, <tf.Tensor: shape=(), dtype=float32, numpy=0.8231503>]\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - 14s 362ms/step - loss: 611.1791\n",
      "Weights: [0.8203014, -0.15936968, 0.14328672, 0.40018633, -0.39617512, 0.8126477, <tf.Tensor: shape=(), dtype=float32, numpy=0.8244355>]\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 611.2056\n",
      "Weights: [0.8210745, -0.15970004, 0.14296202, 0.399914, -0.39689502, 0.8126128, <tf.Tensor: shape=(), dtype=float32, numpy=0.82380253>]\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 611.1740\n",
      "Weights: [0.8206446, -0.1598751, 0.14327517, 0.40205976, -0.39931273, 0.8141469, <tf.Tensor: shape=(), dtype=float32, numpy=0.82412755>]\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 611.1711\n",
      "Weights: [0.8237884, -0.16117717, 0.1445374, 0.40068108, -0.39701346, 0.8117565, <tf.Tensor: shape=(), dtype=float32, numpy=0.82414186>]\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 611.1736\n",
      "Weights: [0.82177657, -0.15961693, 0.14358225, 0.40146637, -0.39740407, 0.81282806, <tf.Tensor: shape=(), dtype=float32, numpy=0.82402575>]\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 611.1774\n",
      "Weights: [0.82217085, -0.15968606, 0.1431376, 0.40251532, -0.39897004, 0.8134671, <tf.Tensor: shape=(), dtype=float32, numpy=0.82424456>]\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - 15s 370ms/step - loss: 611.1920\n",
      "Weights: [0.8241599, -0.16239955, 0.1462165, 0.40188417, -0.40025145, 0.81499994, <tf.Tensor: shape=(), dtype=float32, numpy=0.8244474>]\n",
      "Epoch 202/1000\n",
      " 2/40 [>.............................] - ETA: 14s - loss: 626.8931"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [56]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m model_test\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m      8\u001B[0m     loss \u001B[38;5;241m=\u001B[39m compiler\u001B[38;5;241m.\u001B[39mloss,\n\u001B[1;32m      9\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m compiler\u001B[38;5;241m.\u001B[39moptimizer,\n\u001B[1;32m     10\u001B[0m     run_eagerly \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     12\u001B[0m BETA \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0.7\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.3\u001B[39m, \u001B[38;5;241m0.15\u001B[39m, \u001B[38;5;241m0.4\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.4\u001B[39m, \u001B[38;5;241m0.8\u001B[39m]\n\u001B[0;32m---> 13\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mfit_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madstock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43memission_real\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/AttributionSingleUser/src/hmm_package/generate_hmm.py:373\u001B[0m, in \u001B[0;36mfit_model\u001B[0;34m(model, adstock, emission_real)\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_model\u001B[39m(model, adstock, emission_real):\n\u001B[1;32m    372\u001B[0m     print_weights \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mLambdaCallback(on_epoch_begin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m batch, logs: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeights: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(model\u001B[38;5;241m.\u001B[39mget_weights()[\u001B[38;5;241m0\u001B[39m], tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mactivations\u001B[38;5;241m.\u001B[39msigmoid(model\u001B[38;5;241m.\u001B[39mget_weights()[\u001B[38;5;241m1\u001B[39m]))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 373\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43madstock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m                     \u001B[49m\u001B[43memission_real\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mprint_weights\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:1384\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1378\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1379\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   1380\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m   1381\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   1382\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1383\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1384\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1385\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1386\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:1021\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m   1019\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_function\u001B[39m(iterator):\n\u001B[1;32m   1020\u001B[0m   \u001B[38;5;124;03m\"\"\"Runs a training execution with a single step.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1021\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mstep_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:1010\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function\u001B[0;34m(model, iterator)\u001B[0m\n\u001B[1;32m   1007\u001B[0m   run_step \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mfunction(\n\u001B[1;32m   1008\u001B[0m       run_step, jit_compile\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, experimental_relax_shapes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1009\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m-> 1010\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1011\u001B[0m outputs \u001B[38;5;241m=\u001B[39m reduce_per_replica(\n\u001B[1;32m   1012\u001B[0m     outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirst\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   1013\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1312\u001B[0m, in \u001B[0;36mStrategyBase.run\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscope():\n\u001B[1;32m   1308\u001B[0m   \u001B[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001B[39;00m\n\u001B[1;32m   1309\u001B[0m   \u001B[38;5;66;03m# applied when the caller is also in Eager mode.\u001B[39;00m\n\u001B[1;32m   1310\u001B[0m   fn \u001B[38;5;241m=\u001B[39m autograph\u001B[38;5;241m.\u001B[39mtf_convert(\n\u001B[1;32m   1311\u001B[0m       fn, autograph_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx(), convert_by_default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m-> 1312\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extended\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_for_each_replica\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2888\u001B[0m, in \u001B[0;36mStrategyExtendedV1.call_for_each_replica\u001B[0;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[1;32m   2886\u001B[0m   kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2887\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy()\u001B[38;5;241m.\u001B[39mscope():\n\u001B[0;32m-> 2888\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_for_each_replica\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3689\u001B[0m, in \u001B[0;36m_DefaultDistributionExtended._call_for_each_replica\u001B[0;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[1;32m   3687\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_for_each_replica\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn, args, kwargs):\n\u001B[1;32m   3688\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ReplicaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy(), replica_id_in_sync_group\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m-> 3689\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:595\u001B[0m, in \u001B[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    594\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ag_ctx\u001B[38;5;241m.\u001B[39mControlStatusCtx(status\u001B[38;5;241m=\u001B[39mag_ctx\u001B[38;5;241m.\u001B[39mStatus\u001B[38;5;241m.\u001B[39mUNSPECIFIED):\n\u001B[0;32m--> 595\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:1000\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    999\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_step\u001B[39m(data):\n\u001B[0;32m-> 1000\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1001\u001B[0m   \u001B[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/training.py:859\u001B[0m, in \u001B[0;36mModel.train_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;66;03m# Run forward pass.\u001B[39;00m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[0;32m--> 859\u001B[0m   y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    860\u001B[0m   loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001B[1;32m    861\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_target_and_loss(y, loss)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/base_layer.py:1096\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1092\u001B[0m   inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_cast_inputs(inputs, input_list)\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m autocast_variable\u001B[38;5;241m.\u001B[39menable_auto_cast_variables(\n\u001B[1;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_dtype_object):\n\u001B[0;32m-> 1096\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_activity_regularizer:\n\u001B[1;32m   1099\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:92\u001B[0m, in \u001B[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     90\u001B[0m bound_signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 92\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     94\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_keras_call_info_injected\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;66;03m# Only inject info for the innermost failing call\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/functional.py:451\u001B[0m, in \u001B[0;36mFunctional.call\u001B[0;34m(self, inputs, training, mask)\u001B[0m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;129m@doc_controls\u001B[39m\u001B[38;5;241m.\u001B[39mdo_not_doc_inheritable\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    434\u001B[0m   \u001B[38;5;124;03m\"\"\"Calls the model on new inputs.\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \n\u001B[1;32m    436\u001B[0m \u001B[38;5;124;03m  In this case `call` just reapplies\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;124;03m      a list of tensors if there are more than one outputs.\u001B[39;00m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 451\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_internal_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/functional.py:589\u001B[0m, in \u001B[0;36mFunctional._run_internal_graph\u001B[0;34m(self, inputs, training, mask)\u001B[0m\n\u001B[1;32m    586\u001B[0m   \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# Node is not computable, try skipping.\u001B[39;00m\n\u001B[1;32m    588\u001B[0m args, kwargs \u001B[38;5;241m=\u001B[39m node\u001B[38;5;241m.\u001B[39mmap_arguments(tensor_dict)\n\u001B[0;32m--> 589\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;66;03m# Update tensor_dict.\u001B[39;00m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_id, y \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(node\u001B[38;5;241m.\u001B[39mflat_output_ids, tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(outputs)):\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/layers/distribution_layer.py:220\u001B[0m, in \u001B[0;36mDistributionLambda.__call__\u001B[0;34m(self, inputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    219\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enter_dunder_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 220\u001B[0m   distribution, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mDistributionLambda\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enter_dunder_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    223\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m distribution\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/engine/base_layer.py:1096\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1092\u001B[0m   inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_cast_inputs(inputs, input_list)\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m autocast_variable\u001B[38;5;241m.\u001B[39menable_auto_cast_variables(\n\u001B[1;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_dtype_object):\n\u001B[0;32m-> 1096\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_activity_regularizer:\n\u001B[1;32m   1099\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:92\u001B[0m, in \u001B[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     90\u001B[0m bound_signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 92\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     94\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_keras_call_info_injected\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;66;03m# Only inject info for the innermost failing call\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/layers/distribution_layer.py:226\u001B[0m, in \u001B[0;36mDistributionLambda.call\u001B[0;34m(self, inputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 226\u001B[0m   distribution, value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mDistributionLambda\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    228\u001B[0m   \u001B[38;5;66;03m# We always save the most recently built distribution variables for tracking\u001B[39;00m\n\u001B[1;32m    229\u001B[0m   \u001B[38;5;66;03m# purposes.\u001B[39;00m\n\u001B[1;32m    230\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_most_recently_built_distribution_vars \u001B[38;5;241m=\u001B[39m distribution\u001B[38;5;241m.\u001B[39mvariables\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/keras/layers/core/lambda_layer.py:196\u001B[0m, in \u001B[0;36mLambda.call\u001B[0;34m(self, inputs, mask, training)\u001B[0m\n\u001B[1;32m    192\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m var\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape(watch_accessed_variables\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m tape,\\\n\u001B[1;32m    195\u001B[0m     tf\u001B[38;5;241m.\u001B[39mvariable_creator_scope(_variable_creator):\n\u001B[0;32m--> 196\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_variables(created_variables, tape\u001B[38;5;241m.\u001B[39mwatched_variables())\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/layers/distribution_layer.py:180\u001B[0m, in \u001B[0;36mDistributionLambda.__init__.<locals>._fn\u001B[0;34m(*fargs, **fkwargs)\u001B[0m\n\u001B[1;32m    171\u001B[0m distribution \u001B[38;5;241m=\u001B[39m dtc\u001B[38;5;241m.\u001B[39m_TensorCoercible(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    172\u001B[0m     distribution\u001B[38;5;241m=\u001B[39md,\n\u001B[1;32m    173\u001B[0m     convert_to_tensor_fn\u001B[38;5;241m=\u001B[39mmaybe_composite_convert_to_tensor_fn)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;66;03m# Calling `distrbution._value()` is equivalent to:\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;66;03m# from tensorflow.python.framework import ops\u001B[39;00m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# value = ops.convert_to_tensor_or_composite(distribution)\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;66;03m# We'd prefer to call ops.convert_to_tensor_or_composite but do not,\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;66;03m# favoring our own non-public API over TF's.\u001B[39;00m\n\u001B[0;32m--> 180\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[43mdistribution\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;66;03m# TODO(b/126056144): Remove silent handle once we identify how/why Keras\u001B[39;00m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;66;03m# is losing the distribution handle for activity_regularizer.\u001B[39;00m\n\u001B[1;32m    184\u001B[0m value\u001B[38;5;241m.\u001B[39m_tfp_distribution \u001B[38;5;241m=\u001B[39m distribution  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/layers/internal/distribution_tensor_coercible.py:213\u001B[0m, in \u001B[0;36m_TensorCoercible._value\u001B[0;34m(self, dtype, name, as_ref)\u001B[0m\n\u001B[1;32m    204\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m    205\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFailed to convert object of type \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m to Tensor. Contents: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    206\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCall `distribution.set_tensor_conversion(lambda self: ...)` to \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    209\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m results in `tf.convert_to_tensor(x)` being identical to \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    210\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`x.mean()`.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m), \u001B[38;5;28mself\u001B[39m))\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name_and_control_scope(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    212\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_concrete_value \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 213\u001B[0m       \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_to_tensor_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor_distribution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m       \u001B[38;5;28;01mif\u001B[39;00m callable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_to_tensor_fn)\n\u001B[1;32m    215\u001B[0m       \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_to_tensor_fn)\n\u001B[1;32m    216\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mis_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_concrete_value) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    217\u001B[0m       \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_concrete_value,\n\u001B[1;32m    218\u001B[0m                      composite_tensor\u001B[38;5;241m.\u001B[39mCompositeTensor)):\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_concrete_value \u001B[38;5;241m=\u001B[39m nest_util\u001B[38;5;241m.\u001B[39mconvert_to_nested_tensor(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_concrete_value,\n\u001B[1;32m    221\u001B[0m         name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconcrete_value\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    222\u001B[0m         dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[1;32m    223\u001B[0m         dtype_hint\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensor_distribution\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:1234\u001B[0m, in \u001B[0;36mDistribution.sample\u001B[0;34m(self, sample_shape, seed, name, **kwargs)\u001B[0m\n\u001B[1;32m   1219\u001B[0m \u001B[38;5;124;03m\"\"\"Generate samples of the specified shape.\u001B[39;00m\n\u001B[1;32m   1220\u001B[0m \n\u001B[1;32m   1221\u001B[0m \u001B[38;5;124;03mNote that a call to `sample()` without arguments will generate a single\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;124;03m  samples: a `Tensor` with prepended dimensions `sample_shape`.\u001B[39;00m\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1233\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name_and_control_scope(name):\n\u001B[0;32m-> 1234\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_sample_n\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:1211\u001B[0m, in \u001B[0;36mDistribution._call_sample_n\u001B[0;34m(self, sample_shape, seed, **kwargs)\u001B[0m\n\u001B[1;32m   1207\u001B[0m sample_shape \u001B[38;5;241m=\u001B[39m ps\u001B[38;5;241m.\u001B[39mconvert_to_shape_tensor(\n\u001B[1;32m   1208\u001B[0m     ps\u001B[38;5;241m.\u001B[39mcast(sample_shape, tf\u001B[38;5;241m.\u001B[39mint32), name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msample_shape\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   1209\u001B[0m sample_shape, n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_sample_shape_to_vector(\n\u001B[1;32m   1210\u001B[0m     sample_shape, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msample_shape\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1211\u001B[0m samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample_n\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1212\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcallable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1213\u001B[0m samples \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m   1214\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m x: tf\u001B[38;5;241m.\u001B[39mreshape(x, ps\u001B[38;5;241m.\u001B[39mconcat([sample_shape, ps\u001B[38;5;241m.\u001B[39mshape(x)[\u001B[38;5;241m1\u001B[39m:]], \u001B[38;5;241m0\u001B[39m)),\n\u001B[1;32m   1215\u001B[0m     samples)\n\u001B[1;32m   1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_sample_static_shape(samples, sample_shape)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/distributions/hidden_markov_model.py:389\u001B[0m, in \u001B[0;36mHiddenMarkovModel._sample_n\u001B[0;34m(self, n, seed)\u001B[0m\n\u001B[1;32m    384\u001B[0m   multiples \u001B[38;5;241m=\u001B[39m ps\u001B[38;5;241m.\u001B[39mconcat([[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_steps],\n\u001B[1;32m    385\u001B[0m                          ps\u001B[38;5;241m.\u001B[39mones(ps\u001B[38;5;241m.\u001B[39mrank(init_state), tf\u001B[38;5;241m.\u001B[39mint32)],\n\u001B[1;32m    386\u001B[0m                         axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    387\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mtile(init_state[tf\u001B[38;5;241m.\u001B[39mnewaxis, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], multiples)\n\u001B[0;32m--> 389\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mps\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcond\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_steps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_scan_multiple_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_scan_one_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    394\u001B[0m hidden_one_hot \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mone_hot(hidden_states, num_states,\n\u001B[1;32m    395\u001B[0m                             dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_observation_distribution\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m    396\u001B[0m \u001B[38;5;66;03m# hidden_one_hot :: num_steps n batch_size num_states\u001B[39;00m\n\u001B[1;32m    397\u001B[0m \n\u001B[1;32m    398\u001B[0m \u001B[38;5;66;03m# The observation distribution batch size might not match\u001B[39;00m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;66;03m# the required batch size so as with the initial and\u001B[39;00m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;66;03m# transition distributions we generate more samples and\u001B[39;00m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;66;03m# reshape.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/prefer_static.py:260\u001B[0m, in \u001B[0;36mcond\u001B[0;34m(pred, true_fn, false_fn, name)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pred_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    259\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m pred_value:\n\u001B[0;32m--> 260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrue_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    261\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m false_fn()\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/distributions/hidden_markov_model.py:376\u001B[0m, in \u001B[0;36mHiddenMarkovModel._sample_n.<locals>._scan_multiple_steps\u001B[0;34m()\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;124;03m\"\"\"Take multiple steps with tf.scan.\"\"\"\u001B[39;00m\n\u001B[1;32m    375\u001B[0m step \u001B[38;5;241m=\u001B[39m ps\u001B[38;5;241m.\u001B[39mrange(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_steps \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mint32)\n\u001B[0;32m--> 376\u001B[0m hidden_states, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerate_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m                           \u001B[49m\u001B[43minitializer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minit_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscan_seed\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;66;03m# TODO(b/115618503): add/use prepend_initializer to tf.scan\u001B[39;00m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mconcat([[init_state],\n\u001B[1;32m    381\u001B[0m                   hidden_states], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1082\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch_target\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1083\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   1084\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[1;32m   1085\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[1;32m   1086\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:616\u001B[0m, in \u001B[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    609\u001B[0m           _PRINTED_WARNING[(func, arg_name)] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    610\u001B[0m         logging\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m    611\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFrom \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: calling \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m (from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) with \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is deprecated and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    612\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwill be removed \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mInstructions for updating:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    613\u001B[0m             _call_location(), decorator_utils\u001B[38;5;241m.\u001B[39mget_qualified_name(func),\n\u001B[1;32m    614\u001B[0m             func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m, arg_name, arg_value, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124min a future version\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    615\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m date \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m date), instructions)\n\u001B[0;32m--> 616\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/functional_ops.py:805\u001B[0m, in \u001B[0;36mscan_v2\u001B[0;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001B[0m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscan\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m    694\u001B[0m \u001B[38;5;129m@dispatch\u001B[39m\u001B[38;5;241m.\u001B[39madd_dispatch_support\n\u001B[1;32m    695\u001B[0m \u001B[38;5;129m@deprecation\u001B[39m\u001B[38;5;241m.\u001B[39mdeprecated_arg_values(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    711\u001B[0m             reverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    712\u001B[0m             name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    713\u001B[0m   \u001B[38;5;124;03m\"\"\"scan on the list of tensors unpacked from `elems` on dimension 0.\u001B[39;00m\n\u001B[1;32m    714\u001B[0m \n\u001B[1;32m    715\u001B[0m \u001B[38;5;124;03m  The simplest version of `scan` repeatedly applies the callable `fn` to a\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    803\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 805\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mscan\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    806\u001B[0m \u001B[43m      \u001B[49m\u001B[43mfn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    807\u001B[0m \u001B[43m      \u001B[49m\u001B[43melems\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43melems\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    808\u001B[0m \u001B[43m      \u001B[49m\u001B[43minitializer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    809\u001B[0m \u001B[43m      \u001B[49m\u001B[43mparallel_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    810\u001B[0m \u001B[43m      \u001B[49m\u001B[43mback_prop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mback_prop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    811\u001B[0m \u001B[43m      \u001B[49m\u001B[43mswap_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswap_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[43m      \u001B[49m\u001B[43minfer_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_shape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    813\u001B[0m \u001B[43m      \u001B[49m\u001B[43mreverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreverse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    814\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1082\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch_target\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1083\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   1084\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[1;32m   1085\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[1;32m   1086\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/functional_ops.py:663\u001B[0m, in \u001B[0;36mscan\u001B[0;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001B[0m\n\u001B[1;32m    661\u001B[0m   initial_i \u001B[38;5;241m=\u001B[39m i\n\u001B[1;32m    662\u001B[0m   condition \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m i, _1, _2: i \u001B[38;5;241m<\u001B[39m n\n\u001B[0;32m--> 663\u001B[0m _, _, r_a \u001B[38;5;241m=\u001B[39m \u001B[43mcontrol_flow_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhile_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcondition\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma_flat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccs_ta\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparallel_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mback_prop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mback_prop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m    \u001B[49m\u001B[43mswap_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswap_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    671\u001B[0m results_flat \u001B[38;5;241m=\u001B[39m [r\u001B[38;5;241m.\u001B[39mstack() \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m r_a]\n\u001B[1;32m    673\u001B[0m n_static \u001B[38;5;241m=\u001B[39m tensor_shape\u001B[38;5;241m.\u001B[39mDimension(\n\u001B[1;32m    674\u001B[0m     tensor_shape\u001B[38;5;241m.\u001B[39mdimension_value(\n\u001B[1;32m    675\u001B[0m         elems_flat[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_shape()\u001B[38;5;241m.\u001B[39mwith_rank_at_least(\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]))\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:2795\u001B[0m, in \u001B[0;36mwhile_loop\u001B[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001B[0m\n\u001B[1;32m   2792\u001B[0m loop_var_structure \u001B[38;5;241m=\u001B[39m nest\u001B[38;5;241m.\u001B[39mmap_structure(type_spec\u001B[38;5;241m.\u001B[39mtype_spec_from_value,\n\u001B[1;32m   2793\u001B[0m                                         \u001B[38;5;28mlist\u001B[39m(loop_vars))\n\u001B[1;32m   2794\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m cond(\u001B[38;5;241m*\u001B[39mloop_vars):\n\u001B[0;32m-> 2795\u001B[0m   loop_vars \u001B[38;5;241m=\u001B[39m \u001B[43mbody\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mloop_vars\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2796\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m try_to_pack \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(loop_vars, (\u001B[38;5;28mlist\u001B[39m, _basetuple)):\n\u001B[1;32m   2797\u001B[0m     packed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:2786\u001B[0m, in \u001B[0;36mwhile_loop.<locals>.<lambda>\u001B[0;34m(i, lv)\u001B[0m\n\u001B[1;32m   2783\u001B[0m     loop_vars \u001B[38;5;241m=\u001B[39m (counter, loop_vars)\n\u001B[1;32m   2784\u001B[0m     cond \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m i, lv: (  \u001B[38;5;66;03m# pylint: disable=g-long-lambda\u001B[39;00m\n\u001B[1;32m   2785\u001B[0m         math_ops\u001B[38;5;241m.\u001B[39mlogical_and(i \u001B[38;5;241m<\u001B[39m maximum_iterations, orig_cond(\u001B[38;5;241m*\u001B[39mlv)))\n\u001B[0;32m-> 2786\u001B[0m     body \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m i, lv: (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[43morig_body\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mlv\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   2787\u001B[0m   try_to_pack \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   2789\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m executing_eagerly:\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/functional_ops.py:646\u001B[0m, in \u001B[0;36mscan.<locals>.compute\u001B[0;34m(i, a_flat, tas)\u001B[0m\n\u001B[1;32m    644\u001B[0m packed_elems \u001B[38;5;241m=\u001B[39m input_pack([elem_ta\u001B[38;5;241m.\u001B[39mread(i) \u001B[38;5;28;01mfor\u001B[39;00m elem_ta \u001B[38;5;129;01min\u001B[39;00m elems_ta])\n\u001B[1;32m    645\u001B[0m packed_a \u001B[38;5;241m=\u001B[39m output_pack(a_flat)\n\u001B[0;32m--> 646\u001B[0m a_out \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpacked_a\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpacked_elems\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    647\u001B[0m nest\u001B[38;5;241m.\u001B[39massert_same_structure(elems \u001B[38;5;28;01mif\u001B[39;00m initializer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m initializer,\n\u001B[1;32m    648\u001B[0m                            a_out)\n\u001B[1;32m    649\u001B[0m flat_a_out \u001B[38;5;241m=\u001B[39m output_flatten(a_out)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/distributions/hidden_markov_model.py:348\u001B[0m, in \u001B[0;36mHiddenMarkovModel._sample_n.<locals>.generate_step\u001B[0;34m(state_and_seed, step)\u001B[0m\n\u001B[1;32m    345\u001B[0m sample_seed, next_seed \u001B[38;5;241m=\u001B[39m samplers\u001B[38;5;241m.\u001B[39msplit_seed(seed)\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time_varying_transition_distribution:\n\u001B[0;32m--> 348\u001B[0m   gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transition_distribution\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39msample(\n\u001B[1;32m    349\u001B[0m       n \u001B[38;5;241m*\u001B[39m transition_repeat, seed\u001B[38;5;241m=\u001B[39msample_seed)\n\u001B[1;32m    350\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    351\u001B[0m   gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transition_distribution\u001B[38;5;241m.\u001B[39msample(n \u001B[38;5;241m*\u001B[39m transition_repeat,\n\u001B[1;32m    352\u001B[0m                                              seed\u001B[38;5;241m=\u001B[39msample_seed)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:881\u001B[0m, in \u001B[0;36mDistribution.__getitem__\u001B[0;34m(self, slices)\u001B[0m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, slices):\n\u001B[1;32m    855\u001B[0m   \u001B[38;5;124;03m\"\"\"Slices the batch axes of this distribution, returning a new instance.\u001B[39;00m\n\u001B[1;32m    856\u001B[0m \n\u001B[1;32m    857\u001B[0m \u001B[38;5;124;03m  ```python\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;124;03m    dist: A new `tfd.Distribution` instance with sliced parameters.\u001B[39;00m\n\u001B[1;32m    880\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 881\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mslicing\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_slice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslices\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/slicing.py:217\u001B[0m, in \u001B[0;36mbatch_slice\u001B[0;34m(batch_object, params_overrides, slices, bijector_x_event_ndims)\u001B[0m\n\u001B[1;32m    214\u001B[0m slice_overrides_seq \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [(slices, params_overrides)]\n\u001B[1;32m    215\u001B[0m \u001B[38;5;66;03m# Re-doing the full sequence of slice+copy override work here enables\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;66;03m# gradients all the way back to the original batch_objectribution's arguments.\u001B[39;00m\n\u001B[0;32m--> 217\u001B[0m batch_object \u001B[38;5;241m=\u001B[39m \u001B[43m_apply_slice_sequence\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m    \u001B[49m\u001B[43morig_batch_object\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mslice_overrides_seq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbijector_x_event_ndims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbijector_x_event_ndims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28msetattr\u001B[39m(batch_object,\n\u001B[1;32m    222\u001B[0m         PROVENANCE_ATTR,\n\u001B[1;32m    223\u001B[0m         batch_object\u001B[38;5;241m.\u001B[39m_no_dependency((orig_batch_object, slice_overrides_seq)))  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m batch_object\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/slicing.py:179\u001B[0m, in \u001B[0;36m_apply_slice_sequence\u001B[0;34m(batch_object, slice_overrides_seq, bijector_x_event_ndims)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;124;03m\"\"\"Applies a sequence of slice or copy-with-overrides operations to `batch_object`.\"\"\"\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m slices, overrides \u001B[38;5;129;01min\u001B[39;00m slice_overrides_seq:\n\u001B[0;32m--> 179\u001B[0m   batch_object \u001B[38;5;241m=\u001B[39m \u001B[43m_apply_single_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbatch_object\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m      \u001B[49m\u001B[43mslices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m      \u001B[49m\u001B[43moverrides\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbijector_x_event_ndims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbijector_x_event_ndims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m batch_object\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/slicing.py:168\u001B[0m, in \u001B[0;36m_apply_single_step\u001B[0;34m(batch_object, slices, params_overrides, bijector_x_event_ndims)\u001B[0m\n\u001B[1;32m    166\u001B[0m   override_dict \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 168\u001B[0m   override_dict \u001B[38;5;241m=\u001B[39m \u001B[43m_slice_params_to_dict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbatch_object\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbijector_x_event_ndims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbijector_x_event_ndims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m override_dict\u001B[38;5;241m.\u001B[39mupdate(params_overrides)\n\u001B[1;32m    171\u001B[0m parameters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(batch_object\u001B[38;5;241m.\u001B[39mparameters, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverride_dict)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/slicing.py:153\u001B[0m, in \u001B[0;36m_slice_params_to_dict\u001B[0;34m(batch_object, slices, bijector_x_event_ndims)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m   batch_shape \u001B[38;5;241m=\u001B[39m batch_object\u001B[38;5;241m.\u001B[39mexperimental_batch_shape_tensor(\n\u001B[1;32m    152\u001B[0m       x_event_ndims\u001B[38;5;241m=\u001B[39mbijector_x_event_ndims)\n\u001B[0;32m--> 153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbatch_shape_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_fn_over_parameters_with_event_ndims\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_object\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunctools\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_slice_single_param\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mslices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mslices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_shape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbijector_x_event_ndims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbijector_x_event_ndims\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/batch_shape_lib.py:361\u001B[0m, in \u001B[0;36mmap_fn_over_parameters_with_event_ndims\u001B[0;34m(batch_object, fn, bijector_x_event_ndims, require_static, **parameter_kwargs)\u001B[0m\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m (properties\u001B[38;5;241m.\u001B[39mis_tensor\n\u001B[1;32m    356\u001B[0m           \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mis_tensor(param)\n\u001B[1;32m    357\u001B[0m           \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mis_nested(param_event_ndims)):\n\u001B[1;32m    358\u001B[0m       \u001B[38;5;66;03m# As a last resort, try an explicit conversion.\u001B[39;00m\n\u001B[1;32m    359\u001B[0m       param \u001B[38;5;241m=\u001B[39m tensor_util\u001B[38;5;241m.\u001B[39mconvert_nonref_to_tensor(param, name\u001B[38;5;241m=\u001B[39mparam_name)\n\u001B[0;32m--> 361\u001B[0m   results[param_name] \u001B[38;5;241m=\u001B[39m \u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure_up_to\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m      \u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_event_ndims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1425\u001B[0m, in \u001B[0;36mmap_structure_up_to\u001B[0;34m(shallow_tree, func, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1351\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__internal__.nest.map_structure_up_to\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m   1352\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap_structure_up_to\u001B[39m(shallow_tree, func, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1353\u001B[0m   \u001B[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001B[39;00m\n\u001B[1;32m   1354\u001B[0m \n\u001B[1;32m   1355\u001B[0m \u001B[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1423\u001B[0m \u001B[38;5;124;03m    `shallow_tree`.\u001B[39;00m\n\u001B[1;32m   1424\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1425\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmap_structure_with_tuple_paths_up_to\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1426\u001B[0m \u001B[43m      \u001B[49m\u001B[43mshallow_tree\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1427\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Discards the path arg.\u001B[39;49;00m\n\u001B[1;32m   1428\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1429\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1525\u001B[0m, in \u001B[0;36mmap_structure_with_tuple_paths_up_to\u001B[0;34m(shallow_tree, func, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m flat_value_gen \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1517\u001B[0m     flatten_up_to(  \u001B[38;5;66;03m# pylint: disable=g-complex-comprehension\u001B[39;00m\n\u001B[1;32m   1518\u001B[0m         shallow_tree,\n\u001B[1;32m   1519\u001B[0m         input_tree,\n\u001B[1;32m   1520\u001B[0m         check_types,\n\u001B[1;32m   1521\u001B[0m         expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites) \u001B[38;5;28;01mfor\u001B[39;00m input_tree \u001B[38;5;129;01min\u001B[39;00m inputs)\n\u001B[1;32m   1522\u001B[0m flat_path_gen \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1523\u001B[0m     path\n\u001B[1;32m   1524\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m path, _ \u001B[38;5;129;01min\u001B[39;00m _yield_flat_up_to(shallow_tree, inputs[\u001B[38;5;241m0\u001B[39m], is_nested_fn))\n\u001B[0;32m-> 1525\u001B[0m results \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   1526\u001B[0m     func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(flat_path_gen, \u001B[38;5;241m*\u001B[39mflat_value_gen)\n\u001B[1;32m   1527\u001B[0m ]\n\u001B[1;32m   1528\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(structure\u001B[38;5;241m=\u001B[39mshallow_tree, flat_sequence\u001B[38;5;241m=\u001B[39mresults,\n\u001B[1;32m   1529\u001B[0m                         expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1526\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1516\u001B[0m flat_value_gen \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1517\u001B[0m     flatten_up_to(  \u001B[38;5;66;03m# pylint: disable=g-complex-comprehension\u001B[39;00m\n\u001B[1;32m   1518\u001B[0m         shallow_tree,\n\u001B[1;32m   1519\u001B[0m         input_tree,\n\u001B[1;32m   1520\u001B[0m         check_types,\n\u001B[1;32m   1521\u001B[0m         expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites) \u001B[38;5;28;01mfor\u001B[39;00m input_tree \u001B[38;5;129;01min\u001B[39;00m inputs)\n\u001B[1;32m   1522\u001B[0m flat_path_gen \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1523\u001B[0m     path\n\u001B[1;32m   1524\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m path, _ \u001B[38;5;129;01min\u001B[39;00m _yield_flat_up_to(shallow_tree, inputs[\u001B[38;5;241m0\u001B[39m], is_nested_fn))\n\u001B[1;32m   1525\u001B[0m results \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m-> 1526\u001B[0m     \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(flat_path_gen, \u001B[38;5;241m*\u001B[39mflat_value_gen)\n\u001B[1;32m   1527\u001B[0m ]\n\u001B[1;32m   1528\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(structure\u001B[38;5;241m=\u001B[39mshallow_tree, flat_sequence\u001B[38;5;241m=\u001B[39mresults,\n\u001B[1;32m   1529\u001B[0m                         expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1427\u001B[0m, in \u001B[0;36mmap_structure_up_to.<locals>.<lambda>\u001B[0;34m(_, *values)\u001B[0m\n\u001B[1;32m   1351\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__internal__.nest.map_structure_up_to\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m   1352\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap_structure_up_to\u001B[39m(shallow_tree, func, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1353\u001B[0m   \u001B[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001B[39;00m\n\u001B[1;32m   1354\u001B[0m \n\u001B[1;32m   1355\u001B[0m \u001B[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1423\u001B[0m \u001B[38;5;124;03m    `shallow_tree`.\u001B[39;00m\n\u001B[1;32m   1424\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m   1425\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m map_structure_with_tuple_paths_up_to(\n\u001B[1;32m   1426\u001B[0m       shallow_tree,\n\u001B[0;32m-> 1427\u001B[0m       \u001B[38;5;28;01mlambda\u001B[39;00m _, \u001B[38;5;241m*\u001B[39mvalues: \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m,  \u001B[38;5;66;03m# Discards the path arg.\u001B[39;00m\n\u001B[1;32m   1428\u001B[0m       \u001B[38;5;241m*\u001B[39minputs,\n\u001B[1;32m   1429\u001B[0m       \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/slicing.py:101\u001B[0m, in \u001B[0;36m_slice_single_param\u001B[0;34m(param, param_event_ndims, slices, batch_shape)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03m\"\"\"Slices into the batch shape of a single parameter.\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03m    `slices`.\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# Broadcast the parmameter to have full batch rank.\u001B[39;00m\n\u001B[0;32m--> 101\u001B[0m param \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_shape_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_parameter_with_batch_shape\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_event_ndims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mps\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m param_batch_shape \u001B[38;5;241m=\u001B[39m batch_shape_lib\u001B[38;5;241m.\u001B[39mget_batch_shape_tensor_part(\n\u001B[1;32m    104\u001B[0m     param, param_event_ndims)\n\u001B[1;32m    105\u001B[0m \u001B[38;5;66;03m# At this point the param should have full batch rank, *unless* it's an\u001B[39;00m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;66;03m# atomic object like `tfb.Identity()` incapable of having any batch rank.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/batch_shape_lib.py:278\u001B[0m, in \u001B[0;36mbroadcast_parameter_with_batch_shape\u001B[0;34m(param, param_event_ndims, batch_shape)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(param, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatmul\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    276\u001B[0m   \u001B[38;5;66;03m# TODO(davmre): support broadcasting LinearOperator parameters.\u001B[39;00m\n\u001B[1;32m    277\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m param\n\u001B[0;32m--> 278\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_to\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mps\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_shape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mps\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:858\u001B[0m, in \u001B[0;36mbroadcast_to\u001B[0;34m(input, shape, name)\u001B[0m\n\u001B[1;32m    856\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m _result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m--> 858\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbroadcast_to_eager_fallback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_ctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_SymbolicException:\n\u001B[1;32m    861\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:907\u001B[0m, in \u001B[0;36mbroadcast_to_eager_fallback\u001B[0;34m(input, shape, name, ctx)\u001B[0m\n\u001B[1;32m    904\u001B[0m _result \u001B[38;5;241m=\u001B[39m _execute\u001B[38;5;241m.\u001B[39mexecute(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBroadcastTo\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m, inputs\u001B[38;5;241m=\u001B[39m_inputs_flat,\n\u001B[1;32m    905\u001B[0m                            attrs\u001B[38;5;241m=\u001B[39m_attrs, ctx\u001B[38;5;241m=\u001B[39mctx, name\u001B[38;5;241m=\u001B[39mname)\n\u001B[1;32m    906\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _execute\u001B[38;5;241m.\u001B[39mmust_record_gradient():\n\u001B[0;32m--> 907\u001B[0m   \u001B[43m_execute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecord_gradient\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    908\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mBroadcastTo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_inputs_flat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_attrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_result\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    909\u001B[0m _result, \u001B[38;5;241m=\u001B[39m _result\n\u001B[1;32m    910\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _result\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py:180\u001B[0m, in \u001B[0;36mrecord_gradient\u001B[0;34m(op_name, inputs, attrs, outputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__internal__.record_gradient\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrecord_gradient\u001B[39m(op_name, inputs, attrs, outputs):\n\u001B[1;32m    170\u001B[0m   \u001B[38;5;124;03m\"\"\"Explicitly record the gradient for a given op.\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \n\u001B[1;32m    172\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;124;03m    outputs: A list of tensor outputs from the op.\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m    179\u001B[0m   pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_RecordGradient(op_name, inputs, attrs, outputs,\n\u001B[0;32m--> 180\u001B[0m                                    \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_name_scope\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6335\u001B[0m, in \u001B[0;36mget_name_scope\u001B[0;34m()\u001B[0m\n\u001B[1;32m   6319\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__internal__.get_name_scope\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m   6320\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_name_scope\u001B[39m():\n\u001B[1;32m   6321\u001B[0m   \u001B[38;5;124;03m\"\"\"Returns the current name scope in the default_graph.\u001B[39;00m\n\u001B[1;32m   6322\u001B[0m \n\u001B[1;32m   6323\u001B[0m \u001B[38;5;124;03m  For example:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   6333\u001B[0m \u001B[38;5;124;03m    A string representing the current name scope.\u001B[39;00m\n\u001B[1;32m   6334\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 6335\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecuting_eagerly\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   6336\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m context\u001B[38;5;241m.\u001B[39mcontext()\u001B[38;5;241m.\u001B[39mscope_name\u001B[38;5;241m.\u001B[39mrstrip(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6337\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m get_default_graph()\u001B[38;5;241m.\u001B[39mget_name_scope()\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/eager/context.py:2141\u001B[0m, in \u001B[0;36mexecuting_eagerly\u001B[0;34m()\u001B[0m\n\u001B[1;32m   2087\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexecuting_eagerly\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m   2088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexecuting_eagerly\u001B[39m():\n\u001B[1;32m   2089\u001B[0m   \u001B[38;5;124;03m\"\"\"Checks whether the current thread has eager execution enabled.\u001B[39;00m\n\u001B[1;32m   2090\u001B[0m \n\u001B[1;32m   2091\u001B[0m \u001B[38;5;124;03m  Eager execution is enabled by default and this API returns `True`\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2139\u001B[0m \u001B[38;5;124;03m    `True` if the current thread has eager execution enabled.\u001B[39;00m\n\u001B[1;32m   2140\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2141\u001B[0m   ctx \u001B[38;5;241m=\u001B[39m \u001B[43mcontext_safe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2142\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m ctx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m default_execution_mode \u001B[38;5;241m==\u001B[39m EAGER_MODE\n",
      "File \u001B[0;32m~/Library/Caches/JetBrains/PyCharm2021.3/demo/PyCharmLearningProject/venv/lib/python3.8/site-packages/tensorflow/python/eager/context.py:2057\u001B[0m, in \u001B[0;36mcontext_safe\u001B[0;34m()\u001B[0m\n\u001B[1;32m   2053\u001B[0m     _create_context()\n\u001B[1;32m   2054\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _context\n\u001B[0;32m-> 2057\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontext_safe\u001B[39m():\n\u001B[1;32m   2058\u001B[0m   \u001B[38;5;124;03m\"\"\"Returns current context (or None if one hasn't been initialized).\"\"\"\u001B[39;00m\n\u001B[1;32m   2059\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _context\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# STATES_ARE_OBSERVABLE is defined in CONSTANTS_HMM\n",
    "# The initializer here is the true parameter. The goal is to compute the optimal loss, and see whether the parameters move away from the optimum.\n",
    "initializer=tf.keras.initializers.Constant(BETA),\n",
    "model_test = build_hmm_to_fit_beta( states_observable=STATES_ARE_OBSERVABLE, mu=MU, initializer=initializer )\n",
    "\n",
    "compiler = CompilerInfoBeta(LR_EXPONENTIAL_DECAY)\n",
    "model_test.compile(\n",
    "    loss = compiler.loss,\n",
    "    optimizer = compiler.optimizer,\n",
    "    run_eagerly = True\n",
    ")\n",
    "BETA = [0.7, -0.3, 0.15, 0.4, -0.4, 0.8]\n",
    "history = fit_model(model_test, adstock, emission_real)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see above, the parameters slightly change, although the loss remains the same. Below, you can see the average difference for each transition and each user."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m matrix_diff \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m usr: tf\u001B[38;5;241m.\u001B[39mreduce_sum(make_transition_matrix(MU, model_test\u001B[38;5;241m.\u001B[39mweights[\u001B[38;5;241m0\u001B[39m], adstock[usr:usr\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m-\u001B[39m make_transition_matrix(MU,BETA, adstock[usr:usr\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m30\u001B[39m\n\u001B[0;32m----> 2\u001B[0m avg_diff \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(matrix_diff(usr) \u001B[38;5;28;01mfor\u001B[39;00m usr \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[43mN_users\u001B[49m))\u001B[38;5;241m/\u001B[39mN_users\n",
      "\u001B[0;31mNameError\u001B[0m: name 'N_users' is not defined"
     ]
    }
   ],
   "source": [
    "matrix_diff = lambda usr: tf.reduce_sum(make_transition_matrix(MU, model_test.weights[0], adstock[usr:usr+1]) - make_transition_matrix(MU,BETA, adstock[usr:usr+1]), axis=1)/30\n",
    "avg_diff = sum(matrix_diff(usr) for usr in range(N_users))/N_users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'usr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [58]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m make_transition_matrix(MU, model_test\u001B[38;5;241m.\u001B[39mweights[\u001B[38;5;241m0\u001B[39m], adstock[\u001B[43musr\u001B[49m:usr\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'usr' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and fit the model starting from a random initializer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, <tf.Tensor: shape=(), dtype=float32, numpy=0.27747494>]\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 617.8444\n",
      "Weights: [1.0346726, -0.03517405, 0.035998248, 0.034705944, -0.03403668, 1.0335585, <tf.Tensor: shape=(), dtype=float32, numpy=0.28433898>]\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 14s 355ms/step - loss: 614.8325\n",
      "Weights: [1.0502917, -0.0520285, 0.054607805, 0.051758304, -0.046256, 1.0402039, <tf.Tensor: shape=(), dtype=float32, numpy=0.29241714>]\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 14s 358ms/step - loss: 614.4141\n",
      "Weights: [1.0555738, -0.05937262, 0.06412122, 0.058210034, -0.045352146, 1.0303253, <tf.Tensor: shape=(), dtype=float32, numpy=0.30048382>]\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 614.1545\n",
      "Weights: [1.0596819, -0.06541621, 0.071698256, 0.06700132, -0.045421276, 1.0198566, <tf.Tensor: shape=(), dtype=float32, numpy=0.30825233>]\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 613.9286\n",
      "Weights: [1.065062, -0.07096504, 0.07833408, 0.07608523, -0.04499219, 1.0088546, <tf.Tensor: shape=(), dtype=float32, numpy=0.31608427>]\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 14s 353ms/step - loss: 613.7336\n",
      "Weights: [1.0700756, -0.07646218, 0.08480656, 0.08687932, -0.04608092, 0.9989619, <tf.Tensor: shape=(), dtype=float32, numpy=0.32422155>]\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 613.5386\n",
      "Weights: [1.0748044, -0.081695944, 0.09058435, 0.09726034, -0.047709525, 0.98984295, <tf.Tensor: shape=(), dtype=float32, numpy=0.33217323>]\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 613.3838\n",
      "Weights: [1.0801706, -0.08660271, 0.09565103, 0.107950896, -0.049541734, 0.9807438, <tf.Tensor: shape=(), dtype=float32, numpy=0.33974633>]\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 613.2388\n",
      "Weights: [1.0842662, -0.08990002, 0.09905761, 0.12034728, -0.052136708, 0.9717159, <tf.Tensor: shape=(), dtype=float32, numpy=0.3474512>]\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 14s 359ms/step - loss: 613.1357\n",
      "Weights: [1.0904045, -0.095768616, 0.10471861, 0.1333588, -0.05720683, 0.9652835, <tf.Tensor: shape=(), dtype=float32, numpy=0.3555594>]\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 14s 349ms/step - loss: 612.9938\n",
      "Weights: [1.0937396, -0.09830998, 0.10668845, 0.14388178, -0.05954767, 0.9568553, <tf.Tensor: shape=(), dtype=float32, numpy=0.36315626>]\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 612.8961\n",
      "Weights: [1.0960014, -0.10150108, 0.10962766, 0.15344939, -0.061983414, 0.94900334, <tf.Tensor: shape=(), dtype=float32, numpy=0.37067634>]\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 612.7866\n",
      "Weights: [1.1000224, -0.104989685, 0.111746244, 0.16550823, -0.06709019, 0.9427192, <tf.Tensor: shape=(), dtype=float32, numpy=0.37877858>]\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 14s 359ms/step - loss: 612.7194\n",
      "Weights: [1.1039397, -0.10838932, 0.114234574, 0.1767327, -0.07166632, 0.9367156, <tf.Tensor: shape=(), dtype=float32, numpy=0.3869806>]\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 612.6140\n",
      "Weights: [1.1072062, -0.110231936, 0.114232264, 0.18828769, -0.07603374, 0.9304043, <tf.Tensor: shape=(), dtype=float32, numpy=0.39451987>]\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 612.5416\n",
      "Weights: [1.1081221, -0.11254807, 0.11638465, 0.19706762, -0.07880836, 0.9231221, <tf.Tensor: shape=(), dtype=float32, numpy=0.40237254>]\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 15s 381ms/step - loss: 612.4732\n",
      "Weights: [1.1100746, -0.11486685, 0.11739355, 0.206748, -0.08289, 0.9178906, <tf.Tensor: shape=(), dtype=float32, numpy=0.4102456>]\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 15s 379ms/step - loss: 612.4090\n",
      "Weights: [1.1118311, -0.11782142, 0.11886989, 0.21647486, -0.08815219, 0.9134853, <tf.Tensor: shape=(), dtype=float32, numpy=0.4180817>]\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 16s 399ms/step - loss: 612.3616\n",
      "Weights: [1.1141988, -0.11998171, 0.119915664, 0.22577171, -0.092575036, 0.90820515, <tf.Tensor: shape=(), dtype=float32, numpy=0.4259564>]\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 612.3207\n",
      "Weights: [1.1133693, -0.12134183, 0.11979389, 0.23311199, -0.0965259, 0.9040971, <tf.Tensor: shape=(), dtype=float32, numpy=0.43375507>]\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 612.2432\n",
      "Weights: [1.115176, -0.12461929, 0.12270717, 0.2425174, -0.101558395, 0.90030706, <tf.Tensor: shape=(), dtype=float32, numpy=0.44114515>]\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 612.2009\n",
      "Weights: [1.1151533, -0.1265466, 0.122590296, 0.2505046, -0.106296204, 0.896545, <tf.Tensor: shape=(), dtype=float32, numpy=0.4490485>]\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 612.1398\n",
      "Weights: [1.1134943, -0.12734614, 0.122898735, 0.25841346, -0.111489974, 0.89366, <tf.Tensor: shape=(), dtype=float32, numpy=0.4570448>]\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 612.1105\n",
      "Weights: [1.1136587, -0.12878287, 0.123186894, 0.26526284, -0.114844725, 0.8888066, <tf.Tensor: shape=(), dtype=float32, numpy=0.46433392>]\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 15s 376ms/step - loss: 612.0549\n",
      "Weights: [1.1151605, -0.13109681, 0.12416608, 0.27331856, -0.120091334, 0.8873253, <tf.Tensor: shape=(), dtype=float32, numpy=0.47168803>]\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 612.0181\n",
      "Weights: [1.1129655, -0.13344695, 0.12523083, 0.27952784, -0.12499399, 0.8845484, <tf.Tensor: shape=(), dtype=float32, numpy=0.47989756>]\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 15s 376ms/step - loss: 611.9857\n",
      "Weights: [1.1126908, -0.13501106, 0.12639399, 0.28635985, -0.1291344, 0.88180315, <tf.Tensor: shape=(), dtype=float32, numpy=0.48681566>]\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 14s 355ms/step - loss: 611.9655\n",
      "Weights: [1.1097003, -0.13669966, 0.12772885, 0.28958297, -0.132412, 0.8785014, <tf.Tensor: shape=(), dtype=float32, numpy=0.49450287>]\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 611.9181\n",
      "Weights: [1.1074529, -0.1369198, 0.12612087, 0.29712105, -0.13787885, 0.8764776, <tf.Tensor: shape=(), dtype=float32, numpy=0.50221217>]\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 15s 382ms/step - loss: 611.8865\n",
      "Weights: [1.1050868, -0.13772935, 0.1260199, 0.3028005, -0.1409899, 0.8723327, <tf.Tensor: shape=(), dtype=float32, numpy=0.5091424>]\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 15s 366ms/step - loss: 611.8596\n",
      "Weights: [1.1051977, -0.14161126, 0.13006312, 0.30910957, -0.14713295, 0.8730241, <tf.Tensor: shape=(), dtype=float32, numpy=0.5162945>]\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 15s 385ms/step - loss: 611.8348\n",
      "Weights: [1.10154, -0.14101486, 0.12881097, 0.31315312, -0.15022403, 0.8703282, <tf.Tensor: shape=(), dtype=float32, numpy=0.52384406>]\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 611.8325\n",
      "Weights: [1.096824, -0.14017867, 0.12697628, 0.31718004, -0.15286972, 0.8659676, <tf.Tensor: shape=(), dtype=float32, numpy=0.5314163>]\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 15s 375ms/step - loss: 611.7955\n",
      "Weights: [1.095463, -0.14237331, 0.12839217, 0.32224962, -0.15721415, 0.8642528, <tf.Tensor: shape=(), dtype=float32, numpy=0.53733784>]\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 15s 369ms/step - loss: 611.7591\n",
      "Weights: [1.0939621, -0.14551775, 0.13104908, 0.32779858, -0.16333744, 0.86601716, <tf.Tensor: shape=(), dtype=float32, numpy=0.5446348>]\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 611.7332\n",
      "Weights: [1.0888195, -0.14472948, 0.12954958, 0.33158284, -0.16657233, 0.86291516, <tf.Tensor: shape=(), dtype=float32, numpy=0.5514516>]\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 611.7422\n",
      "Weights: [1.0870011, -0.14718778, 0.13167743, 0.33445334, -0.1701805, 0.86152995, <tf.Tensor: shape=(), dtype=float32, numpy=0.5583138>]\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 14s 361ms/step - loss: 611.7307\n",
      "Weights: [1.0804235, -0.14555508, 0.12951183, 0.33702254, -0.1728959, 0.8591791, <tf.Tensor: shape=(), dtype=float32, numpy=0.5643745>]\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 611.6782\n",
      "Weights: [1.0801793, -0.1488356, 0.13183147, 0.34272987, -0.17802012, 0.8589283, <tf.Tensor: shape=(), dtype=float32, numpy=0.571147>]\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 611.6457\n",
      "Weights: [1.0740225, -0.14815983, 0.13217387, 0.3449018, -0.1808533, 0.8569906, <tf.Tensor: shape=(), dtype=float32, numpy=0.577322>]\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 15s 368ms/step - loss: 611.6274\n",
      "Weights: [1.0702633, -0.14857867, 0.1315498, 0.34818923, -0.18350615, 0.8545013, <tf.Tensor: shape=(), dtype=float32, numpy=0.5829564>]\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 611.6177\n",
      "Weights: [1.0654804, -0.14909427, 0.13234334, 0.3498226, -0.18687984, 0.85404813, <tf.Tensor: shape=(), dtype=float32, numpy=0.5889536>]\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 14s 361ms/step - loss: 611.6036\n",
      "Weights: [1.0632001, -0.15013471, 0.13249238, 0.35264102, -0.19023567, 0.8516541, <tf.Tensor: shape=(), dtype=float32, numpy=0.5952882>]\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 611.5912\n",
      "Weights: [1.0582271, -0.15067609, 0.13330011, 0.35502872, -0.19373909, 0.85167396, <tf.Tensor: shape=(), dtype=float32, numpy=0.6008791>]\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 14s 359ms/step - loss: 611.5660\n",
      "Weights: [1.0509866, -0.14919618, 0.13157079, 0.35567784, -0.19577277, 0.84932184, <tf.Tensor: shape=(), dtype=float32, numpy=0.6060421>]\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 611.5591\n",
      "Weights: [1.0476592, -0.15038265, 0.13286643, 0.35965315, -0.2002743, 0.8490815, <tf.Tensor: shape=(), dtype=float32, numpy=0.61150473>]\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.5436\n",
      "Weights: [1.0438555, -0.15233512, 0.13480774, 0.36266795, -0.20433885, 0.8487621, <tf.Tensor: shape=(), dtype=float32, numpy=0.61770236>]\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 611.5280\n",
      "Weights: [1.0404817, -0.15226734, 0.13389482, 0.36569923, -0.2081495, 0.8486029, <tf.Tensor: shape=(), dtype=float32, numpy=0.62261015>]\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 15s 365ms/step - loss: 611.5099\n",
      "Weights: [1.0347047, -0.15252036, 0.13397492, 0.36505216, -0.20946476, 0.84555626, <tf.Tensor: shape=(), dtype=float32, numpy=0.62814295>]\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 611.4982\n",
      "Weights: [1.0313704, -0.15277134, 0.13328494, 0.36685607, -0.21209823, 0.8440126, <tf.Tensor: shape=(), dtype=float32, numpy=0.63293743>]\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 16s 408ms/step - loss: 611.4902\n",
      "Weights: [1.0265195, -0.15334861, 0.13391232, 0.36930975, -0.21627297, 0.8448789, <tf.Tensor: shape=(), dtype=float32, numpy=0.63780236>]\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 14s 358ms/step - loss: 611.4727\n",
      "Weights: [1.0212734, -0.15314315, 0.1335089, 0.37116262, -0.21866845, 0.84310126, <tf.Tensor: shape=(), dtype=float32, numpy=0.6428164>]\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 15s 372ms/step - loss: 611.4531\n",
      "Weights: [1.018462, -0.15394342, 0.13417728, 0.37384567, -0.22243913, 0.8429306, <tf.Tensor: shape=(), dtype=float32, numpy=0.64671934>]\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 16s 391ms/step - loss: 611.4496\n",
      "Weights: [1.0153775, -0.15593636, 0.13577282, 0.37682393, -0.22737318, 0.84385365, <tf.Tensor: shape=(), dtype=float32, numpy=0.6517195>]\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 15s 377ms/step - loss: 611.4277\n",
      "Weights: [1.0080554, -0.15426604, 0.13511465, 0.37604237, -0.2281255, 0.84142995, <tf.Tensor: shape=(), dtype=float32, numpy=0.65633464>]\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 15s 371ms/step - loss: 611.4769\n",
      "Weights: [1.0067772, -0.15755475, 0.13748063, 0.37858728, -0.23294058, 0.8432642, <tf.Tensor: shape=(), dtype=float32, numpy=0.66073394>]\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 16s 396ms/step - loss: 611.4155\n",
      "Weights: [1.0002333, -0.15554245, 0.13532493, 0.37887752, -0.23448063, 0.8412036, <tf.Tensor: shape=(), dtype=float32, numpy=0.6650686>]\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 611.4069\n",
      "Weights: [0.9941721, -0.15602577, 0.13628319, 0.37770057, -0.23632745, 0.83979225, <tf.Tensor: shape=(), dtype=float32, numpy=0.66923845>]\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 611.3910\n",
      "Weights: [0.9921269, -0.15600416, 0.13554686, 0.3820314, -0.24085969, 0.8407221, <tf.Tensor: shape=(), dtype=float32, numpy=0.6730131>]\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 611.3779\n",
      "Weights: [0.9875599, -0.1564459, 0.13519847, 0.3829966, -0.24324521, 0.83856463, <tf.Tensor: shape=(), dtype=float32, numpy=0.6767424>]\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 611.3842\n",
      "Weights: [0.98188484, -0.15588982, 0.13454671, 0.38263974, -0.24480195, 0.83733946, <tf.Tensor: shape=(), dtype=float32, numpy=0.68090236>]\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 611.3823\n",
      "Weights: [0.9801211, -0.15852039, 0.13876292, 0.38433677, -0.24834028, 0.83887154, <tf.Tensor: shape=(), dtype=float32, numpy=0.6844237>]\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 15s 373ms/step - loss: 611.3750\n",
      "Weights: [0.9765228, -0.15838327, 0.13715267, 0.38620642, -0.25101247, 0.83729357, <tf.Tensor: shape=(), dtype=float32, numpy=0.6884287>]\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - 15s 371ms/step - loss: 611.3450\n",
      "Weights: [0.97114784, -0.15836464, 0.13728204, 0.38662806, -0.25375593, 0.83763915, <tf.Tensor: shape=(), dtype=float32, numpy=0.6921844>]\n",
      "Epoch 65/1000\n",
      "40/40 [==============================] - 15s 387ms/step - loss: 611.3349\n",
      "Weights: [0.96600497, -0.15681069, 0.1356536, 0.38908193, -0.256826, 0.83651316, <tf.Tensor: shape=(), dtype=float32, numpy=0.6958542>]\n",
      "Epoch 66/1000\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 611.3363\n",
      "Weights: [0.96262664, -0.15883231, 0.13717371, 0.38885644, -0.25914723, 0.8360302, <tf.Tensor: shape=(), dtype=float32, numpy=0.69952625>]\n",
      "Epoch 67/1000\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 611.3267\n",
      "Weights: [0.95925003, -0.15798888, 0.13630515, 0.38901278, -0.26004767, 0.83402807, <tf.Tensor: shape=(), dtype=float32, numpy=0.7017708>]\n",
      "Epoch 68/1000\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 611.3099\n",
      "Weights: [0.957526, -0.16090359, 0.13903315, 0.3907349, -0.26417875, 0.83563805, <tf.Tensor: shape=(), dtype=float32, numpy=0.7051266>]\n",
      "Epoch 69/1000\n",
      "40/40 [==============================] - 15s 374ms/step - loss: 611.3170\n",
      "Weights: [0.9511728, -0.15855594, 0.1374519, 0.39056918, -0.26474434, 0.83299756, <tf.Tensor: shape=(), dtype=float32, numpy=0.70814764>]\n",
      "Epoch 70/1000\n",
      "40/40 [==============================] - 15s 367ms/step - loss: 611.3073\n",
      "Weights: [0.9485459, -0.16060609, 0.13822792, 0.3903967, -0.2675991, 0.8327528, <tf.Tensor: shape=(), dtype=float32, numpy=0.7116008>]\n",
      "Epoch 71/1000\n",
      "40/40 [==============================] - 15s 370ms/step - loss: 611.3079\n",
      "Weights: [0.94356555, -0.15982078, 0.1373306, 0.3913731, -0.2704873, 0.8327344, <tf.Tensor: shape=(), dtype=float32, numpy=0.71471184>]\n",
      "Epoch 72/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 611.2978\n",
      "Weights: [0.9411686, -0.16081679, 0.13838704, 0.39226395, -0.272787, 0.831715, <tf.Tensor: shape=(), dtype=float32, numpy=0.7179656>]\n",
      "Epoch 73/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 611.3057\n",
      "Weights: [0.9370661, -0.16045035, 0.13728803, 0.3920189, -0.27496895, 0.8318021, <tf.Tensor: shape=(), dtype=float32, numpy=0.72015536>]\n",
      "Epoch 74/1000\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 611.3093\n",
      "Weights: [0.93142784, -0.15873902, 0.13656303, 0.3906111, -0.27432534, 0.8285059, <tf.Tensor: shape=(), dtype=float32, numpy=0.72315305>]\n",
      "Epoch 75/1000\n",
      "40/40 [==============================] - 14s 358ms/step - loss: 611.2781\n",
      "Weights: [0.9319197, -0.16166148, 0.13933933, 0.39371833, -0.27963698, 0.8310362, <tf.Tensor: shape=(), dtype=float32, numpy=0.7255845>]\n",
      "Epoch 76/1000\n",
      "40/40 [==============================] - 14s 355ms/step - loss: 611.2728\n",
      "Weights: [0.92987067, -0.16251962, 0.13988163, 0.397934, -0.28410473, 0.8323436, <tf.Tensor: shape=(), dtype=float32, numpy=0.72819644>]\n",
      "Epoch 77/1000\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 611.2708\n",
      "Weights: [0.9219769, -0.16052012, 0.1379743, 0.39433467, -0.2832966, 0.8279062, <tf.Tensor: shape=(), dtype=float32, numpy=0.7314123>]\n",
      "Epoch 78/1000\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 611.2735\n",
      "Weights: [0.9204011, -0.16159894, 0.13837704, 0.39497152, -0.28624716, 0.83002436, <tf.Tensor: shape=(), dtype=float32, numpy=0.73362434>]\n",
      "Epoch 79/1000\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 611.2642\n",
      "Weights: [0.9182922, -0.16217102, 0.13838865, 0.39534235, -0.2877926, 0.8286451, <tf.Tensor: shape=(), dtype=float32, numpy=0.73579895>]\n",
      "Epoch 80/1000\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 611.2633\n",
      "Weights: [0.91592646, -0.16350351, 0.13934757, 0.39747438, -0.29209414, 0.8296913, <tf.Tensor: shape=(), dtype=float32, numpy=0.7384331>]\n",
      "Epoch 81/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 611.2577\n",
      "Weights: [0.9140288, -0.16302212, 0.13873132, 0.39821008, -0.29339683, 0.82953167, <tf.Tensor: shape=(), dtype=float32, numpy=0.7405144>]\n",
      "Epoch 82/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.2542\n",
      "Weights: [0.91139716, -0.16356151, 0.14010906, 0.39943367, -0.2954586, 0.8292282, <tf.Tensor: shape=(), dtype=float32, numpy=0.7425747>]\n",
      "Epoch 83/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.2546\n",
      "Weights: [0.9090084, -0.16317378, 0.13851306, 0.3993649, -0.29750827, 0.82834256, <tf.Tensor: shape=(), dtype=float32, numpy=0.7446406>]\n",
      "Epoch 84/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2504\n",
      "Weights: [0.90655035, -0.16470397, 0.14086288, 0.39865738, -0.29970223, 0.8290375, <tf.Tensor: shape=(), dtype=float32, numpy=0.74706835>]\n",
      "Epoch 85/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.2493\n",
      "Weights: [0.9024259, -0.16456792, 0.14063479, 0.3979511, -0.29995933, 0.8270316, <tf.Tensor: shape=(), dtype=float32, numpy=0.7487029>]\n",
      "Epoch 86/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2421\n",
      "Weights: [0.89950866, -0.16352808, 0.13963428, 0.4004093, -0.3031167, 0.8284728, <tf.Tensor: shape=(), dtype=float32, numpy=0.75089014>]\n",
      "Epoch 87/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.2555\n",
      "Weights: [0.89737505, -0.16360237, 0.13990931, 0.39869577, -0.30340734, 0.8267833, <tf.Tensor: shape=(), dtype=float32, numpy=0.75262517>]\n",
      "Epoch 88/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2374\n",
      "Weights: [0.8939759, -0.16181915, 0.13673818, 0.40115282, -0.3054282, 0.8246382, <tf.Tensor: shape=(), dtype=float32, numpy=0.7550427>]\n",
      "Epoch 89/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.2330\n",
      "Weights: [0.8943211, -0.16439298, 0.13828847, 0.4003911, -0.3079039, 0.82565045, <tf.Tensor: shape=(), dtype=float32, numpy=0.75637186>]\n",
      "Epoch 90/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.2203\n",
      "Weights: [0.8910948, -0.16506481, 0.1399978, 0.40143067, -0.31053793, 0.82684815, <tf.Tensor: shape=(), dtype=float32, numpy=0.75825834>]\n",
      "Epoch 91/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.2245\n",
      "Weights: [0.8914082, -0.16628234, 0.14117351, 0.4028245, -0.31296402, 0.8268969, <tf.Tensor: shape=(), dtype=float32, numpy=0.75978965>]\n",
      "Epoch 92/1000\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 611.2263\n",
      "Weights: [0.8859745, -0.16483052, 0.13978486, 0.39992815, -0.3130833, 0.8247052, <tf.Tensor: shape=(), dtype=float32, numpy=0.7622235>]\n",
      "Epoch 93/1000\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 611.2273\n",
      "Weights: [0.8865871, -0.16746259, 0.14099179, 0.40152314, -0.3160965, 0.82591546, <tf.Tensor: shape=(), dtype=float32, numpy=0.7632594>]\n",
      "Epoch 94/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.2095\n",
      "Weights: [0.88302296, -0.16624957, 0.13989547, 0.39983326, -0.31584176, 0.8237716, <tf.Tensor: shape=(), dtype=float32, numpy=0.7653655>]\n",
      "Epoch 95/1000\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 611.2169\n",
      "Weights: [0.88299674, -0.16660677, 0.14033176, 0.40457663, -0.3199048, 0.8258454, <tf.Tensor: shape=(), dtype=float32, numpy=0.7664631>]\n",
      "Epoch 96/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.2147\n",
      "Weights: [0.87776464, -0.1666484, 0.14049357, 0.4002404, -0.319536, 0.8242747, <tf.Tensor: shape=(), dtype=float32, numpy=0.7680729>]\n",
      "Epoch 97/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2144\n",
      "Weights: [0.8751456, -0.16430658, 0.13854016, 0.40162545, -0.32054123, 0.8228773, <tf.Tensor: shape=(), dtype=float32, numpy=0.76938343>]\n",
      "Epoch 98/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.2124\n",
      "Weights: [0.8759493, -0.16597344, 0.13980553, 0.40269873, -0.3237362, 0.8248904, <tf.Tensor: shape=(), dtype=float32, numpy=0.77103186>]\n",
      "Epoch 99/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 611.2057\n",
      "Weights: [0.8720568, -0.16617148, 0.14152852, 0.40194893, -0.32426685, 0.8239023, <tf.Tensor: shape=(), dtype=float32, numpy=0.7723763>]\n",
      "Epoch 100/1000\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 611.2222\n",
      "Weights: [0.87138456, -0.16636188, 0.14091547, 0.40273347, -0.32534486, 0.8230067, <tf.Tensor: shape=(), dtype=float32, numpy=0.77400863>]\n",
      "Epoch 101/1000\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 611.2013\n",
      "Weights: [0.8715847, -0.16798131, 0.1413736, 0.40309864, -0.32633305, 0.82148135, <tf.Tensor: shape=(), dtype=float32, numpy=0.77528954>]\n",
      "Epoch 102/1000\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 611.2356\n",
      "Weights: [0.87130404, -0.16923764, 0.14246641, 0.4042894, -0.33130354, 0.8252796, <tf.Tensor: shape=(), dtype=float32, numpy=0.77629906>]\n",
      "Epoch 103/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1965\n",
      "Weights: [0.8676085, -0.16733964, 0.14155442, 0.4029765, -0.3295534, 0.82132334, <tf.Tensor: shape=(), dtype=float32, numpy=0.77782094>]\n",
      "Epoch 104/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.2012\n",
      "Weights: [0.8664669, -0.1670964, 0.14010379, 0.4046565, -0.33223534, 0.8225129, <tf.Tensor: shape=(), dtype=float32, numpy=0.77902997>]\n",
      "Epoch 105/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1997\n",
      "Weights: [0.8654108, -0.16848429, 0.14164543, 0.40293947, -0.332913, 0.8225132, <tf.Tensor: shape=(), dtype=float32, numpy=0.7806514>]\n",
      "Epoch 106/1000\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 611.2016\n",
      "Weights: [0.8653539, -0.1687782, 0.14133644, 0.4059492, -0.3355328, 0.82264733, <tf.Tensor: shape=(), dtype=float32, numpy=0.7819078>]\n",
      "Epoch 107/1000\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 611.1942\n",
      "Weights: [0.8621895, -0.16775085, 0.14090557, 0.40610802, -0.33681214, 0.82246774, <tf.Tensor: shape=(), dtype=float32, numpy=0.7825335>]\n",
      "Epoch 108/1000\n",
      "40/40 [==============================] - 14s 348ms/step - loss: 611.2056\n",
      "Weights: [0.8583872, -0.16676289, 0.14075841, 0.4024307, -0.335905, 0.8219775, <tf.Tensor: shape=(), dtype=float32, numpy=0.78386736>]\n",
      "Epoch 109/1000\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 611.2077\n",
      "Weights: [0.8608193, -0.16857474, 0.14088623, 0.40601742, -0.34011713, 0.82274264, <tf.Tensor: shape=(), dtype=float32, numpy=0.7848149>]\n",
      "Epoch 110/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1927\n",
      "Weights: [0.8575721, -0.16798876, 0.1423978, 0.4037068, -0.33972684, 0.8221454, <tf.Tensor: shape=(), dtype=float32, numpy=0.7858418>]\n",
      "Epoch 111/1000\n",
      "40/40 [==============================] - 14s 350ms/step - loss: 611.2119\n",
      "Weights: [0.8571777, -0.16802254, 0.1400008, 0.40598196, -0.341033, 0.82034415, <tf.Tensor: shape=(), dtype=float32, numpy=0.7869617>]\n",
      "Epoch 112/1000\n",
      "40/40 [==============================] - 15s 365ms/step - loss: 611.2009\n",
      "Weights: [0.854683, -0.16784553, 0.14095353, 0.4036405, -0.340693, 0.8194132, <tf.Tensor: shape=(), dtype=float32, numpy=0.7879517>]\n",
      "Epoch 113/1000\n",
      "40/40 [==============================] - 15s 363ms/step - loss: 611.2009\n",
      "Weights: [0.85770184, -0.16945803, 0.14139289, 0.4072434, -0.3447815, 0.82205135, <tf.Tensor: shape=(), dtype=float32, numpy=0.7890166>]\n",
      "Epoch 114/1000\n",
      "40/40 [==============================] - 15s 367ms/step - loss: 611.1976\n",
      "Weights: [0.85649925, -0.16988567, 0.14194885, 0.40631518, -0.3456149, 0.82183564, <tf.Tensor: shape=(), dtype=float32, numpy=0.78987753>]\n",
      "Epoch 115/1000\n",
      "40/40 [==============================] - 15s 371ms/step - loss: 611.1832\n",
      "Weights: [0.8528886, -0.1679986, 0.14068256, 0.40622258, -0.34517264, 0.8206196, <tf.Tensor: shape=(), dtype=float32, numpy=0.7907632>]\n",
      "Epoch 116/1000\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 611.1956\n",
      "Weights: [0.85328794, -0.16961083, 0.14298508, 0.40581244, -0.34637597, 0.8203791, <tf.Tensor: shape=(), dtype=float32, numpy=0.7917748>]\n",
      "Epoch 117/1000\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 611.1952\n",
      "Weights: [0.8502007, -0.16831747, 0.1417004, 0.4052423, -0.34746853, 0.82039034, <tf.Tensor: shape=(), dtype=float32, numpy=0.7925908>]\n",
      "Epoch 118/1000\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 611.1962\n",
      "Weights: [0.8500072, -0.16867022, 0.14104614, 0.40654376, -0.34995887, 0.82075435, <tf.Tensor: shape=(), dtype=float32, numpy=0.7933684>]\n",
      "Epoch 119/1000\n",
      "40/40 [==============================] - 15s 381ms/step - loss: 611.1893\n",
      "Weights: [0.84959525, -0.16923304, 0.14179806, 0.40427104, -0.34902486, 0.8194977, <tf.Tensor: shape=(), dtype=float32, numpy=0.7946452>]\n",
      "Epoch 120/1000\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 611.1986\n",
      "Weights: [0.84672934, -0.16865455, 0.14150907, 0.40672413, -0.35102728, 0.82012427, <tf.Tensor: shape=(), dtype=float32, numpy=0.79512244>]\n",
      "Epoch 121/1000\n",
      "40/40 [==============================] - 15s 364ms/step - loss: 611.1912\n",
      "Weights: [0.84706426, -0.16870877, 0.14161304, 0.40496328, -0.35147297, 0.8191719, <tf.Tensor: shape=(), dtype=float32, numpy=0.79619527>]\n",
      "Epoch 122/1000\n",
      "40/40 [==============================] - 14s 349ms/step - loss: 611.1771\n",
      "Weights: [0.8466924, -0.1703227, 0.14325997, 0.40542647, -0.35253146, 0.8191118, <tf.Tensor: shape=(), dtype=float32, numpy=0.7967136>]\n",
      "Epoch 123/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 611.1915\n",
      "Weights: [0.8463326, -0.17074949, 0.14305188, 0.407465, -0.35664505, 0.82071686, <tf.Tensor: shape=(), dtype=float32, numpy=0.7977825>]\n",
      "Epoch 124/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 611.1812\n",
      "Weights: [0.8415801, -0.16761069, 0.14107642, 0.40415147, -0.35383365, 0.81843865, <tf.Tensor: shape=(), dtype=float32, numpy=0.7983539>]\n",
      "Epoch 125/1000\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 611.1800\n",
      "Weights: [0.84415424, -0.17003141, 0.1425342, 0.4063731, -0.35655624, 0.81973255, <tf.Tensor: shape=(), dtype=float32, numpy=0.7990595>]\n",
      "Epoch 126/1000\n",
      "40/40 [==============================] - 15s 386ms/step - loss: 611.1993\n",
      "Weights: [0.8438887, -0.17002669, 0.1430969, 0.4067505, -0.35691214, 0.8186705, <tf.Tensor: shape=(), dtype=float32, numpy=0.799302>]\n",
      "Epoch 127/1000\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 611.1994\n",
      "Weights: [0.84189767, -0.16883875, 0.14199771, 0.40605718, -0.35716605, 0.8181269, <tf.Tensor: shape=(), dtype=float32, numpy=0.8000703>]\n",
      "Epoch 128/1000\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 611.1790\n",
      "Weights: [0.8401673, -0.16764022, 0.1398802, 0.40600786, -0.35853964, 0.81804156, <tf.Tensor: shape=(), dtype=float32, numpy=0.80112875>]\n",
      "Epoch 129/1000\n",
      "40/40 [==============================] - 15s 379ms/step - loss: 611.2021\n",
      "Weights: [0.8401052, -0.16881654, 0.14210953, 0.40494913, -0.35751927, 0.81694275, <tf.Tensor: shape=(), dtype=float32, numpy=0.8016908>]\n",
      "Epoch 130/1000\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 611.1859\n",
      "Weights: [0.8405656, -0.16744298, 0.14054726, 0.40649834, -0.35931534, 0.81793606, <tf.Tensor: shape=(), dtype=float32, numpy=0.8024409>]\n",
      "Epoch 131/1000\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 611.1989\n",
      "Weights: [0.8389704, -0.1683414, 0.14212526, 0.40684974, -0.36148578, 0.8193211, <tf.Tensor: shape=(), dtype=float32, numpy=0.8028745>]\n",
      "Epoch 132/1000\n",
      "40/40 [==============================] - 14s 348ms/step - loss: 611.1772\n",
      "Weights: [0.84043527, -0.16957854, 0.14179942, 0.40484616, -0.36076713, 0.8176324, <tf.Tensor: shape=(), dtype=float32, numpy=0.8037329>]\n",
      "Epoch 133/1000\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 611.1951\n",
      "Weights: [0.8383428, -0.1676791, 0.14160162, 0.40702957, -0.36250773, 0.8184956, <tf.Tensor: shape=(), dtype=float32, numpy=0.80432796>]\n",
      "Epoch 134/1000\n",
      "40/40 [==============================] - 16s 388ms/step - loss: 611.1962\n",
      "Weights: [0.8389451, -0.17079973, 0.14384452, 0.40673947, -0.36426193, 0.81843483, <tf.Tensor: shape=(), dtype=float32, numpy=0.8047629>]\n",
      "Epoch 135/1000\n",
      "40/40 [==============================] - 15s 371ms/step - loss: 611.1755\n",
      "Weights: [0.8370902, -0.168061, 0.14210464, 0.40730277, -0.36436373, 0.81897974, <tf.Tensor: shape=(), dtype=float32, numpy=0.8055923>]\n",
      "Epoch 136/1000\n",
      "40/40 [==============================] - 15s 363ms/step - loss: 611.1745\n",
      "Weights: [0.8361894, -0.16807333, 0.14221, 0.40729475, -0.36377445, 0.81756824, <tf.Tensor: shape=(), dtype=float32, numpy=0.8062878>]\n",
      "Epoch 137/1000\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 611.1899\n",
      "Weights: [0.83644116, -0.1686604, 0.14226373, 0.40643024, -0.3660579, 0.8187496, <tf.Tensor: shape=(), dtype=float32, numpy=0.80668664>]\n",
      "Epoch 138/1000\n",
      "40/40 [==============================] - 14s 358ms/step - loss: 611.1807\n",
      "Weights: [0.83525777, -0.1691179, 0.14237277, 0.4066775, -0.366904, 0.8173471, <tf.Tensor: shape=(), dtype=float32, numpy=0.80703336>]\n",
      "Epoch 139/1000\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 611.2067\n",
      "Weights: [0.8318511, -0.16691583, 0.14215909, 0.4035905, -0.3650136, 0.8161814, <tf.Tensor: shape=(), dtype=float32, numpy=0.80742997>]\n",
      "Epoch 140/1000\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 611.1812\n",
      "Weights: [0.8327629, -0.16713896, 0.14085789, 0.40423575, -0.36527485, 0.81503916, <tf.Tensor: shape=(), dtype=float32, numpy=0.8080344>]\n",
      "Epoch 141/1000\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 611.1951\n",
      "Weights: [0.8357493, -0.16963194, 0.144408, 0.40692207, -0.36943176, 0.81899875, <tf.Tensor: shape=(), dtype=float32, numpy=0.8084738>]\n",
      "Epoch 142/1000\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 611.1837\n",
      "Weights: [0.8327686, -0.16648056, 0.14067005, 0.40541455, -0.36641148, 0.8146262, <tf.Tensor: shape=(), dtype=float32, numpy=0.80919695>]\n",
      "Epoch 143/1000\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 611.1816\n",
      "Weights: [0.8327604, -0.16769283, 0.14156309, 0.4072032, -0.36940533, 0.8169925, <tf.Tensor: shape=(), dtype=float32, numpy=0.8092731>]\n",
      "Epoch 144/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.1832\n",
      "Weights: [0.8308422, -0.16667126, 0.14177313, 0.4040865, -0.36870593, 0.8165412, <tf.Tensor: shape=(), dtype=float32, numpy=0.809949>]\n",
      "Epoch 145/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1951\n",
      "Weights: [0.8334335, -0.16924466, 0.14220816, 0.40760592, -0.3727842, 0.81831867, <tf.Tensor: shape=(), dtype=float32, numpy=0.8107084>]\n",
      "Epoch 146/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1960\n",
      "Weights: [0.83188534, -0.16815564, 0.14261907, 0.40460822, -0.3701754, 0.81585175, <tf.Tensor: shape=(), dtype=float32, numpy=0.81055474>]\n",
      "Epoch 147/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1832\n",
      "Weights: [0.83117956, -0.16788618, 0.14238328, 0.40728778, -0.37273344, 0.8165748, <tf.Tensor: shape=(), dtype=float32, numpy=0.81126744>]\n",
      "Epoch 148/1000\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 611.1783\n",
      "Weights: [0.82976896, -0.16580155, 0.14021161, 0.40525252, -0.37086168, 0.81563646, <tf.Tensor: shape=(), dtype=float32, numpy=0.8115271>]\n",
      "Epoch 149/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1816\n",
      "Weights: [0.82932603, -0.16697006, 0.14109503, 0.40492368, -0.37268373, 0.8160102, <tf.Tensor: shape=(), dtype=float32, numpy=0.8119403>]\n",
      "Epoch 150/1000\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 611.1760\n",
      "Weights: [0.832525, -0.16991903, 0.14364068, 0.40879515, -0.37598827, 0.8181901, <tf.Tensor: shape=(), dtype=float32, numpy=0.81232166>]\n",
      "Epoch 151/1000\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 611.1710\n",
      "Weights: [0.82992816, -0.16756803, 0.14220576, 0.40674147, -0.37371978, 0.81588954, <tf.Tensor: shape=(), dtype=float32, numpy=0.81259453>]\n",
      "Epoch 152/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.1928\n",
      "Weights: [0.83007306, -0.16871817, 0.14429997, 0.40707418, -0.37504983, 0.81740755, <tf.Tensor: shape=(), dtype=float32, numpy=0.8134448>]\n",
      "Epoch 153/1000\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 611.1788\n",
      "Weights: [0.8279411, -0.1674285, 0.14278124, 0.40537205, -0.37544551, 0.8164236, <tf.Tensor: shape=(), dtype=float32, numpy=0.8142856>]\n",
      "Epoch 154/1000\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 611.2078\n",
      "Weights: [0.8297826, -0.16790879, 0.14282693, 0.40654615, -0.37517685, 0.81600845, <tf.Tensor: shape=(), dtype=float32, numpy=0.81410325>]\n",
      "Epoch 155/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1761\n",
      "Weights: [0.82820845, -0.16628832, 0.14120042, 0.40602335, -0.37639293, 0.8161689, <tf.Tensor: shape=(), dtype=float32, numpy=0.814137>]\n",
      "Epoch 156/1000\n",
      "40/40 [==============================] - 14s 353ms/step - loss: 611.1876\n",
      "Weights: [0.82836294, -0.16644768, 0.14106481, 0.40693757, -0.37669903, 0.8156854, <tf.Tensor: shape=(), dtype=float32, numpy=0.8146735>]\n",
      "Epoch 157/1000\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 611.1840\n",
      "Weights: [0.82756317, -0.16722703, 0.14289768, 0.40534052, -0.37601936, 0.8150208, <tf.Tensor: shape=(), dtype=float32, numpy=0.8148291>]\n",
      "Epoch 158/1000\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 611.1925\n",
      "Weights: [0.82839787, -0.16729794, 0.14247362, 0.40517905, -0.3767968, 0.8159862, <tf.Tensor: shape=(), dtype=float32, numpy=0.8150157>]\n",
      "Epoch 159/1000\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 611.1767\n",
      "Weights: [0.82691485, -0.16611703, 0.14233123, 0.40502506, -0.37632042, 0.8147429, <tf.Tensor: shape=(), dtype=float32, numpy=0.81575966>]\n",
      "Epoch 160/1000\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 611.1785\n",
      "Weights: [0.82864887, -0.16882804, 0.14505886, 0.40539908, -0.3797112, 0.81848764, <tf.Tensor: shape=(), dtype=float32, numpy=0.8155179>]\n",
      "Epoch 161/1000\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 611.1730\n",
      "Weights: [0.8269073, -0.16629337, 0.14223135, 0.4062952, -0.3792705, 0.815648, <tf.Tensor: shape=(), dtype=float32, numpy=0.81616676>]\n",
      "Epoch 162/1000\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 611.1783\n",
      "Weights: [0.828408, -0.16846862, 0.14523128, 0.4047028, -0.3785664, 0.8152864, <tf.Tensor: shape=(), dtype=float32, numpy=0.8166072>]\n",
      "Epoch 163/1000\n",
      "40/40 [==============================] - 16s 396ms/step - loss: 611.1804\n",
      "Weights: [0.8281281, -0.16762452, 0.1438657, 0.40511018, -0.3798699, 0.8159318, <tf.Tensor: shape=(), dtype=float32, numpy=0.8166288>]\n",
      "Epoch 164/1000\n",
      "40/40 [==============================] - 17s 414ms/step - loss: 611.1810\n",
      "Weights: [0.8281517, -0.16745403, 0.14314881, 0.4062688, -0.3805653, 0.81587034, <tf.Tensor: shape=(), dtype=float32, numpy=0.8171595>]\n",
      "Epoch 165/1000\n",
      "40/40 [==============================] - 15s 369ms/step - loss: 611.1676\n",
      "Weights: [0.8268874, -0.16749714, 0.14383683, 0.40463945, -0.38033938, 0.81554115, <tf.Tensor: shape=(), dtype=float32, numpy=0.8173783>]\n",
      "Epoch 166/1000\n",
      "40/40 [==============================] - 15s 374ms/step - loss: 611.1855\n",
      "Weights: [0.82793164, -0.16804036, 0.14474483, 0.40639868, -0.38224944, 0.81732875, <tf.Tensor: shape=(), dtype=float32, numpy=0.81725174>]\n",
      "Epoch 167/1000\n",
      "40/40 [==============================] - 15s 375ms/step - loss: 611.1829\n",
      "Weights: [0.8269749, -0.16699234, 0.14413628, 0.4085956, -0.38415727, 0.8173017, <tf.Tensor: shape=(), dtype=float32, numpy=0.8178973>]\n",
      "Epoch 168/1000\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 611.1810\n",
      "Weights: [0.8256643, -0.16526285, 0.14199129, 0.40592718, -0.38182038, 0.81612206, <tf.Tensor: shape=(), dtype=float32, numpy=0.81817627>]\n",
      "Epoch 169/1000\n",
      "40/40 [==============================] - 14s 354ms/step - loss: 611.1827\n",
      "Weights: [0.8268406, -0.16773388, 0.14582755, 0.40733093, -0.38311058, 0.81674623, <tf.Tensor: shape=(), dtype=float32, numpy=0.8183019>]\n",
      "Epoch 170/1000\n",
      "40/40 [==============================] - 14s 337ms/step - loss: 611.1846\n",
      "Weights: [0.8258775, -0.16604336, 0.14444841, 0.40543604, -0.38162813, 0.815344, <tf.Tensor: shape=(), dtype=float32, numpy=0.8185465>]\n",
      "Epoch 171/1000\n",
      "40/40 [==============================] - 14s 349ms/step - loss: 611.1710\n",
      "Weights: [0.8254113, -0.16693835, 0.14384791, 0.40799442, -0.3842596, 0.81581813, <tf.Tensor: shape=(), dtype=float32, numpy=0.81873584>]\n",
      "Epoch 172/1000\n",
      "40/40 [==============================] - 15s 364ms/step - loss: 611.1996\n",
      "Weights: [0.82703006, -0.16701613, 0.14453092, 0.40807262, -0.38487187, 0.81767607, <tf.Tensor: shape=(), dtype=float32, numpy=0.8187636>]\n",
      "Epoch 173/1000\n",
      "40/40 [==============================] - 15s 362ms/step - loss: 611.1833\n",
      "Weights: [0.8253937, -0.16606422, 0.14247255, 0.40552396, -0.38382825, 0.81463075, <tf.Tensor: shape=(), dtype=float32, numpy=0.8192013>]\n",
      "Epoch 174/1000\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 611.1782\n",
      "Weights: [0.8283465, -0.16751896, 0.14438127, 0.40746996, -0.38542128, 0.81588453, <tf.Tensor: shape=(), dtype=float32, numpy=0.8193376>]\n",
      "Epoch 175/1000\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 611.1832\n",
      "Weights: [0.8248234, -0.16535076, 0.14281124, 0.405012, -0.38376948, 0.8146542, <tf.Tensor: shape=(), dtype=float32, numpy=0.8196091>]\n",
      "Epoch 176/1000\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 611.1902\n",
      "Weights: [0.8252048, -0.16541958, 0.14229836, 0.40599948, -0.3856619, 0.8146197, <tf.Tensor: shape=(), dtype=float32, numpy=0.82013845>]\n",
      "Epoch 177/1000\n",
      "40/40 [==============================] - 14s 358ms/step - loss: 611.1738\n",
      "Weights: [0.82513946, -0.16483893, 0.14310105, 0.40633395, -0.3852361, 0.81551224, <tf.Tensor: shape=(), dtype=float32, numpy=0.8196669>]\n",
      "Epoch 178/1000\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 611.2007\n",
      "Weights: [0.824151, -0.16562393, 0.14333293, 0.40442914, -0.3842577, 0.8135458, <tf.Tensor: shape=(), dtype=float32, numpy=0.8199475>]\n",
      "Epoch 179/1000\n",
      "40/40 [==============================] - 15s 378ms/step - loss: 611.1860\n",
      "Weights: [0.82559264, -0.1662447, 0.1446211, 0.40405044, -0.3849953, 0.8152035, <tf.Tensor: shape=(), dtype=float32, numpy=0.82101756>]\n",
      "Epoch 180/1000\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 611.1744\n",
      "Weights: [0.8244224, -0.16472921, 0.1438782, 0.4058383, -0.38571388, 0.8157877, <tf.Tensor: shape=(), dtype=float32, numpy=0.82077444>]\n",
      "Epoch 181/1000\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 611.2139\n",
      "Weights: [0.8220222, -0.16221902, 0.1414797, 0.40392128, -0.38463086, 0.8127837, <tf.Tensor: shape=(), dtype=float32, numpy=0.820814>]\n",
      "Epoch 182/1000\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 611.1733\n",
      "Weights: [0.82464045, -0.1637402, 0.14309034, 0.40688387, -0.38740453, 0.816057, <tf.Tensor: shape=(), dtype=float32, numpy=0.820639>]\n",
      "Epoch 183/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1885\n",
      "Weights: [0.82318103, -0.1624463, 0.1417384, 0.4030919, -0.3835752, 0.8132359, <tf.Tensor: shape=(), dtype=float32, numpy=0.8216584>]\n",
      "Epoch 184/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1733\n",
      "Weights: [0.82449114, -0.16487525, 0.14328982, 0.40587637, -0.3865349, 0.81412756, <tf.Tensor: shape=(), dtype=float32, numpy=0.8213591>]\n",
      "Epoch 185/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1760\n",
      "Weights: [0.82539576, -0.1655954, 0.14498149, 0.40655947, -0.38814002, 0.8155765, <tf.Tensor: shape=(), dtype=float32, numpy=0.8215463>]\n",
      "Epoch 186/1000\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 611.1791\n",
      "Weights: [0.8243954, -0.16394433, 0.14355265, 0.4069873, -0.3889228, 0.8160007, <tf.Tensor: shape=(), dtype=float32, numpy=0.82146686>]\n",
      "Epoch 187/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1722\n",
      "Weights: [0.82396096, -0.16331667, 0.14185803, 0.40625173, -0.38804814, 0.8144785, <tf.Tensor: shape=(), dtype=float32, numpy=0.8215691>]\n",
      "Epoch 188/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1749\n",
      "Weights: [0.82228893, -0.16163772, 0.14158118, 0.4046208, -0.3862681, 0.8134436, <tf.Tensor: shape=(), dtype=float32, numpy=0.8219568>]\n",
      "Epoch 189/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1889\n",
      "Weights: [0.82446283, -0.16421452, 0.14299156, 0.40425974, -0.3878399, 0.81292725, <tf.Tensor: shape=(), dtype=float32, numpy=0.82239807>]\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1731\n",
      "Weights: [0.8259813, -0.1646529, 0.1443446, 0.40631327, -0.38859487, 0.8149036, <tf.Tensor: shape=(), dtype=float32, numpy=0.8221407>]\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1866\n",
      "Weights: [0.824716, -0.1644133, 0.14491852, 0.40720052, -0.3912553, 0.8167349, <tf.Tensor: shape=(), dtype=float32, numpy=0.822668>]\n",
      "Epoch 192/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1861\n",
      "Weights: [0.8249632, -0.16480106, 0.14442205, 0.40453556, -0.38942698, 0.8150737, <tf.Tensor: shape=(), dtype=float32, numpy=0.8228756>]\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1699\n",
      "Weights: [0.8231072, -0.1628851, 0.1429846, 0.4040037, -0.38835418, 0.81406444, <tf.Tensor: shape=(), dtype=float32, numpy=0.82285917>]\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1672\n",
      "Weights: [0.8244776, -0.16422239, 0.14418681, 0.40443948, -0.38920683, 0.81446207, <tf.Tensor: shape=(), dtype=float32, numpy=0.8227319>]\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 611.1777\n",
      "Weights: [0.8245529, -0.16499089, 0.14623441, 0.40615577, -0.39001006, 0.814963, <tf.Tensor: shape=(), dtype=float32, numpy=0.82298833>]\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.1716\n",
      "Weights: [0.8238351, -0.16370563, 0.14444399, 0.40365314, -0.3892589, 0.8130444, <tf.Tensor: shape=(), dtype=float32, numpy=0.82309294>]\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1733\n",
      "Weights: [0.8242758, -0.16406359, 0.14435592, 0.40506607, -0.39175272, 0.815557, <tf.Tensor: shape=(), dtype=float32, numpy=0.8234708>]\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1802\n",
      "Weights: [0.8225462, -0.16135536, 0.14250925, 0.40538785, -0.38981256, 0.8137171, <tf.Tensor: shape=(), dtype=float32, numpy=0.8233739>]\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1677\n",
      "Weights: [0.8237728, -0.16340052, 0.1436661, 0.40580112, -0.39185464, 0.814681, <tf.Tensor: shape=(), dtype=float32, numpy=0.82339144>]\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1723\n",
      "Weights: [0.82376903, -0.16268738, 0.14396471, 0.40283754, -0.3901057, 0.8141185, <tf.Tensor: shape=(), dtype=float32, numpy=0.82334757>]\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1674\n",
      "Weights: [0.82438576, -0.1626933, 0.14406526, 0.40550652, -0.3921109, 0.8151761, <tf.Tensor: shape=(), dtype=float32, numpy=0.8236415>]\n",
      "Epoch 202/1000\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 611.1784\n",
      "Weights: [0.8204649, -0.16010755, 0.14264552, 0.40209448, -0.38987437, 0.81331956, <tf.Tensor: shape=(), dtype=float32, numpy=0.82375777>]\n",
      "Epoch 203/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1908\n",
      "Weights: [0.8235097, -0.16338807, 0.14528906, 0.4051816, -0.39258793, 0.8158431, <tf.Tensor: shape=(), dtype=float32, numpy=0.8241174>]\n",
      "Epoch 204/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1785\n",
      "Weights: [0.8239129, -0.16311106, 0.14425145, 0.40524298, -0.39279988, 0.81415814, <tf.Tensor: shape=(), dtype=float32, numpy=0.82389355>]\n",
      "Epoch 205/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1697\n",
      "Weights: [0.82623136, -0.16353618, 0.14432314, 0.4045247, -0.3929878, 0.81456375, <tf.Tensor: shape=(), dtype=float32, numpy=0.8237699>]\n",
      "Epoch 206/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1793\n",
      "Weights: [0.8251591, -0.16265461, 0.14446913, 0.40505737, -0.39265934, 0.81494814, <tf.Tensor: shape=(), dtype=float32, numpy=0.82446086>]\n",
      "Epoch 207/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1775\n",
      "Weights: [0.8250604, -0.16296875, 0.14509161, 0.4053307, -0.39291018, 0.81516546, <tf.Tensor: shape=(), dtype=float32, numpy=0.82417905>]\n",
      "Epoch 208/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1675\n",
      "Weights: [0.8212096, -0.15958352, 0.14187074, 0.4036569, -0.39108115, 0.81286955, <tf.Tensor: shape=(), dtype=float32, numpy=0.8245698>]\n",
      "Epoch 209/1000\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 611.1730\n",
      "Weights: [0.8236023, -0.16148306, 0.1439024, 0.40455967, -0.3926237, 0.81473744, <tf.Tensor: shape=(), dtype=float32, numpy=0.82465315>]\n",
      "Epoch 210/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1778\n",
      "Weights: [0.8233497, -0.16220246, 0.14393264, 0.40550324, -0.3940598, 0.8146686, <tf.Tensor: shape=(), dtype=float32, numpy=0.82453096>]\n",
      "Epoch 211/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1790\n",
      "Weights: [0.8256693, -0.16290414, 0.14478143, 0.40422657, -0.3930549, 0.81474453, <tf.Tensor: shape=(), dtype=float32, numpy=0.8244106>]\n",
      "Epoch 212/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1755\n",
      "Weights: [0.8216901, -0.15973108, 0.1424952, 0.40247965, -0.39144844, 0.81326383, <tf.Tensor: shape=(), dtype=float32, numpy=0.8247337>]\n",
      "Epoch 213/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1780\n",
      "Weights: [0.8236209, -0.1623715, 0.14500457, 0.40490377, -0.39469793, 0.81487626, <tf.Tensor: shape=(), dtype=float32, numpy=0.824573>]\n",
      "Epoch 214/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1738\n",
      "Weights: [0.82395464, -0.1619615, 0.14487818, 0.4039747, -0.39295232, 0.81397146, <tf.Tensor: shape=(), dtype=float32, numpy=0.8249039>]\n",
      "Epoch 215/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1896\n",
      "Weights: [0.8230621, -0.16119593, 0.14301483, 0.40288952, -0.39315894, 0.8136377, <tf.Tensor: shape=(), dtype=float32, numpy=0.8249254>]\n",
      "Epoch 216/1000\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 611.1888\n",
      "Weights: [0.821054, -0.15855202, 0.14114764, 0.40130392, -0.39134505, 0.81092376, <tf.Tensor: shape=(), dtype=float32, numpy=0.82537675>]\n",
      "Epoch 217/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2214\n",
      "Weights: [0.82583123, -0.16313963, 0.14506531, 0.4031703, -0.39526778, 0.8154356, <tf.Tensor: shape=(), dtype=float32, numpy=0.8252671>]\n",
      "Epoch 218/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1708\n",
      "Weights: [0.8236946, -0.16072702, 0.14495571, 0.40292588, -0.39403456, 0.8145928, <tf.Tensor: shape=(), dtype=float32, numpy=0.8251573>]\n",
      "Epoch 219/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1899\n",
      "Weights: [0.8250319, -0.16168551, 0.1444985, 0.40574384, -0.3960432, 0.81624955, <tf.Tensor: shape=(), dtype=float32, numpy=0.82560325>]\n",
      "Epoch 220/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1769\n",
      "Weights: [0.82626903, -0.16299838, 0.14532329, 0.40546873, -0.39707556, 0.81472516, <tf.Tensor: shape=(), dtype=float32, numpy=0.8252742>]\n",
      "Epoch 221/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1893\n",
      "Weights: [0.823169, -0.16042861, 0.1444625, 0.40197298, -0.39373407, 0.8137164, <tf.Tensor: shape=(), dtype=float32, numpy=0.8257685>]\n",
      "Epoch 222/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1842\n",
      "Weights: [0.82345897, -0.16077316, 0.14487801, 0.40155694, -0.39418688, 0.81386966, <tf.Tensor: shape=(), dtype=float32, numpy=0.82577896>]\n",
      "Epoch 223/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1807\n",
      "Weights: [0.82287663, -0.16108735, 0.14426251, 0.40040812, -0.39364144, 0.8123833, <tf.Tensor: shape=(), dtype=float32, numpy=0.826179>]\n",
      "Epoch 224/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1851\n",
      "Weights: [0.82629293, -0.16247797, 0.14535415, 0.40404904, -0.39638856, 0.81536376, <tf.Tensor: shape=(), dtype=float32, numpy=0.8255364>]\n",
      "Epoch 225/1000\n",
      "40/40 [==============================] - 15s 363ms/step - loss: 611.1699\n",
      "Weights: [0.8237867, -0.16006409, 0.14365533, 0.40349263, -0.39542913, 0.81412107, <tf.Tensor: shape=(), dtype=float32, numpy=0.82577085>]\n",
      "Epoch 226/1000\n",
      "40/40 [==============================] - 14s 349ms/step - loss: 611.1826\n",
      "Weights: [0.8241149, -0.15988556, 0.14342664, 0.4026209, -0.39447212, 0.8126514, <tf.Tensor: shape=(), dtype=float32, numpy=0.82615674>]\n",
      "Epoch 227/1000\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 611.2037\n",
      "Weights: [0.82377434, -0.1597305, 0.14389876, 0.4023758, -0.39506873, 0.812932, <tf.Tensor: shape=(), dtype=float32, numpy=0.8258244>]\n",
      "Epoch 228/1000\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 611.1745\n",
      "Weights: [0.8235038, -0.16025405, 0.14423464, 0.40385228, -0.3966445, 0.8135476, <tf.Tensor: shape=(), dtype=float32, numpy=0.8262971>]\n",
      "Epoch 229/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1727\n",
      "Weights: [0.8248567, -0.1603604, 0.14426297, 0.40488416, -0.39626506, 0.8141278, <tf.Tensor: shape=(), dtype=float32, numpy=0.8256867>]\n",
      "Epoch 230/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1938\n",
      "Weights: [0.8248759, -0.16103019, 0.14546767, 0.40425164, -0.39762828, 0.81493145, <tf.Tensor: shape=(), dtype=float32, numpy=0.8258089>]\n",
      "Epoch 231/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1982\n",
      "Weights: [0.8215511, -0.15664671, 0.14121209, 0.4028706, -0.3945501, 0.81202704, <tf.Tensor: shape=(), dtype=float32, numpy=0.82622087>]\n",
      "Epoch 232/1000\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 611.1786\n",
      "Weights: [0.82391715, -0.15922232, 0.14398938, 0.40563643, -0.3980276, 0.81522024, <tf.Tensor: shape=(), dtype=float32, numpy=0.82632697>]\n",
      "Epoch 233/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1821\n",
      "Weights: [0.82410973, -0.15936, 0.14439914, 0.40335578, -0.3962355, 0.81371486, <tf.Tensor: shape=(), dtype=float32, numpy=0.8266859>]\n",
      "Epoch 234/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1929\n",
      "Weights: [0.82701427, -0.16269292, 0.1475658, 0.40512788, -0.39813963, 0.8157493, <tf.Tensor: shape=(), dtype=float32, numpy=0.8263054>]\n",
      "Epoch 235/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1752\n",
      "Weights: [0.82371885, -0.1590726, 0.14424026, 0.40074578, -0.39452448, 0.81228334, <tf.Tensor: shape=(), dtype=float32, numpy=0.8264087>]\n",
      "Epoch 236/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1846\n",
      "Weights: [0.8267087, -0.16046803, 0.14526252, 0.4031634, -0.3961775, 0.81298447, <tf.Tensor: shape=(), dtype=float32, numpy=0.8259206>]\n",
      "Epoch 237/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1841\n",
      "Weights: [0.821887, -0.1582768, 0.14391746, 0.4015284, -0.39598995, 0.81200504, <tf.Tensor: shape=(), dtype=float32, numpy=0.82684886>]\n",
      "Epoch 238/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1676\n",
      "Weights: [0.8245305, -0.1604512, 0.1452444, 0.4016544, -0.39680928, 0.81212634, <tf.Tensor: shape=(), dtype=float32, numpy=0.82660556>]\n",
      "Epoch 239/1000\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 611.1749\n",
      "Weights: [0.8230719, -0.15812536, 0.14398415, 0.40325633, -0.39730215, 0.8137437, <tf.Tensor: shape=(), dtype=float32, numpy=0.8265475>]\n",
      "Epoch 240/1000\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 611.1891\n",
      "Weights: [0.8241935, -0.15771805, 0.14302988, 0.40144318, -0.3952401, 0.81218284, <tf.Tensor: shape=(), dtype=float32, numpy=0.8263879>]\n",
      "Epoch 241/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1619\n",
      "Weights: [0.8238687, -0.15861881, 0.1451751, 0.4035691, -0.3986585, 0.81616867, <tf.Tensor: shape=(), dtype=float32, numpy=0.8266996>]\n",
      "Epoch 242/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1733\n",
      "Weights: [0.8252639, -0.15815198, 0.14371733, 0.40348333, -0.39752838, 0.8133923, <tf.Tensor: shape=(), dtype=float32, numpy=0.8261422>]\n",
      "Epoch 243/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1829\n",
      "Weights: [0.8233768, -0.1581134, 0.14354166, 0.4009353, -0.39707717, 0.812228, <tf.Tensor: shape=(), dtype=float32, numpy=0.82652>]\n",
      "Epoch 244/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.1677\n",
      "Weights: [0.82391506, -0.15698197, 0.14460813, 0.40384403, -0.39849937, 0.81481713, <tf.Tensor: shape=(), dtype=float32, numpy=0.8266044>]\n",
      "Epoch 245/1000\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 611.1806\n",
      "Weights: [0.8228422, -0.15828101, 0.14496998, 0.4010113, -0.39773196, 0.81259507, <tf.Tensor: shape=(), dtype=float32, numpy=0.8269948>]\n",
      "Epoch 246/1000\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 611.1824\n",
      "Weights: [0.8254678, -0.15726449, 0.14411916, 0.40442738, -0.39921322, 0.81372595, <tf.Tensor: shape=(), dtype=float32, numpy=0.8270328>]\n",
      "Epoch 247/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1835\n",
      "Weights: [0.8258248, -0.15822758, 0.14410836, 0.40279773, -0.39900303, 0.8140422, <tf.Tensor: shape=(), dtype=float32, numpy=0.8264899>]\n",
      "Epoch 248/1000\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 611.1734\n",
      "Weights: [0.82489085, -0.15774064, 0.14505658, 0.40198597, -0.39644825, 0.81245524, <tf.Tensor: shape=(), dtype=float32, numpy=0.82660174>]\n",
      "Epoch 249/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1733\n",
      "Weights: [0.82395184, -0.15672071, 0.14422947, 0.3999119, -0.39636135, 0.8122615, <tf.Tensor: shape=(), dtype=float32, numpy=0.8268739>]\n",
      "Epoch 250/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1814\n",
      "Weights: [0.82294524, -0.15659137, 0.14367321, 0.40212047, -0.3981636, 0.8121789, <tf.Tensor: shape=(), dtype=float32, numpy=0.8266417>]\n",
      "Epoch 251/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1882\n",
      "Weights: [0.8265418, -0.1598073, 0.14569731, 0.40425757, -0.4009485, 0.81518686, <tf.Tensor: shape=(), dtype=float32, numpy=0.8265954>]\n",
      "Epoch 252/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1863\n",
      "Weights: [0.82425845, -0.15594319, 0.14305325, 0.40022522, -0.39751813, 0.81279933, <tf.Tensor: shape=(), dtype=float32, numpy=0.8268622>]\n",
      "Epoch 253/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.1771\n",
      "Weights: [0.8240166, -0.15728527, 0.14417046, 0.4010187, -0.39823133, 0.8127969, <tf.Tensor: shape=(), dtype=float32, numpy=0.82688886>]\n",
      "Epoch 254/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1712\n",
      "Weights: [0.82566845, -0.15818858, 0.14475161, 0.40158352, -0.3995103, 0.8133243, <tf.Tensor: shape=(), dtype=float32, numpy=0.8269628>]\n",
      "Epoch 255/1000\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 611.1702\n",
      "Weights: [0.82433623, -0.15700229, 0.14430888, 0.40138978, -0.3984853, 0.8126281, <tf.Tensor: shape=(), dtype=float32, numpy=0.8270524>]\n",
      "Epoch 256/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1705\n",
      "Weights: [0.8253753, -0.15657212, 0.14429788, 0.402697, -0.3999915, 0.8141516, <tf.Tensor: shape=(), dtype=float32, numpy=0.82725364>]\n",
      "Epoch 257/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1963\n",
      "Weights: [0.8249552, -0.15646297, 0.14430685, 0.39991373, -0.39743948, 0.81238616, <tf.Tensor: shape=(), dtype=float32, numpy=0.8275693>]\n",
      "Epoch 258/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1838\n",
      "Weights: [0.8278197, -0.15878054, 0.14611964, 0.40098614, -0.39880154, 0.813924, <tf.Tensor: shape=(), dtype=float32, numpy=0.82744825>]\n",
      "Epoch 259/1000\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 611.1760\n",
      "Weights: [0.82467186, -0.15635201, 0.1438073, 0.40057024, -0.39832988, 0.8125329, <tf.Tensor: shape=(), dtype=float32, numpy=0.8273231>]\n",
      "Epoch 260/1000\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 611.1729\n",
      "Weights: [0.825567, -0.15672415, 0.1441108, 0.4017725, -0.39955753, 0.81336033, <tf.Tensor: shape=(), dtype=float32, numpy=0.827124>]\n",
      "Epoch 261/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1876\n",
      "Weights: [0.82561886, -0.1575335, 0.14472331, 0.40154052, -0.3998001, 0.81223994, <tf.Tensor: shape=(), dtype=float32, numpy=0.8271229>]\n",
      "Epoch 262/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.2062\n",
      "Weights: [0.82740253, -0.15965074, 0.14711618, 0.40148118, -0.40017623, 0.81424165, <tf.Tensor: shape=(), dtype=float32, numpy=0.82730377>]\n",
      "Epoch 263/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1995\n",
      "Weights: [0.82440835, -0.15551588, 0.14392354, 0.4002726, -0.39883214, 0.8122964, <tf.Tensor: shape=(), dtype=float32, numpy=0.8281731>]\n",
      "Epoch 264/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1910\n",
      "Weights: [0.8255174, -0.15587254, 0.14355616, 0.4007887, -0.39969447, 0.81343204, <tf.Tensor: shape=(), dtype=float32, numpy=0.82755136>]\n",
      "Epoch 265/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1738\n",
      "Weights: [0.8243746, -0.15646121, 0.14397785, 0.40053698, -0.39978412, 0.8121457, <tf.Tensor: shape=(), dtype=float32, numpy=0.8275157>]\n",
      "Epoch 266/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1702\n",
      "Weights: [0.82697237, -0.15729794, 0.14415163, 0.4018249, -0.40122473, 0.81321764, <tf.Tensor: shape=(), dtype=float32, numpy=0.82731146>]\n",
      "Epoch 267/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1820\n",
      "Weights: [0.826866, -0.15730287, 0.14519893, 0.40295547, -0.40217182, 0.8144095, <tf.Tensor: shape=(), dtype=float32, numpy=0.8276974>]\n",
      "Epoch 268/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1964\n",
      "Weights: [0.82787067, -0.15854345, 0.14668843, 0.40330988, -0.40271243, 0.81533176, <tf.Tensor: shape=(), dtype=float32, numpy=0.8273802>]\n",
      "Epoch 269/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1863\n",
      "Weights: [0.82635725, -0.15733683, 0.14561623, 0.40156743, -0.40177527, 0.8150506, <tf.Tensor: shape=(), dtype=float32, numpy=0.82745206>]\n",
      "Epoch 270/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1659\n",
      "Weights: [0.82591146, -0.1571898, 0.14488907, 0.39958268, -0.4001439, 0.8123512, <tf.Tensor: shape=(), dtype=float32, numpy=0.82780147>]\n",
      "Epoch 271/1000\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 611.1974\n",
      "Weights: [0.827008, -0.15733215, 0.1457161, 0.4023395, -0.40142167, 0.81391865, <tf.Tensor: shape=(), dtype=float32, numpy=0.8273847>]\n",
      "Epoch 272/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1652\n",
      "Weights: [0.8243422, -0.15436272, 0.14305432, 0.39967084, -0.39963737, 0.81220263, <tf.Tensor: shape=(), dtype=float32, numpy=0.82741666>]\n",
      "Epoch 273/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.2189\n",
      "Weights: [0.827204, -0.15801585, 0.14684118, 0.4025106, -0.40308234, 0.8161061, <tf.Tensor: shape=(), dtype=float32, numpy=0.82730097>]\n",
      "Epoch 274/1000\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 611.2035\n",
      "Weights: [0.8265192, -0.15646638, 0.14522786, 0.40123588, -0.40014356, 0.81327575, <tf.Tensor: shape=(), dtype=float32, numpy=0.8271385>]\n",
      "Epoch 275/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2287\n",
      "Weights: [0.8247389, -0.15474753, 0.1426788, 0.3987543, -0.39917552, 0.8108231, <tf.Tensor: shape=(), dtype=float32, numpy=0.82793725>]\n",
      "Epoch 276/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1807\n",
      "Weights: [0.8240882, -0.15526909, 0.14443567, 0.39974025, -0.400709, 0.812864, <tf.Tensor: shape=(), dtype=float32, numpy=0.8275095>]\n",
      "Epoch 277/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2135\n",
      "Weights: [0.82361996, -0.15446219, 0.14296421, 0.39735883, -0.39883548, 0.81044, <tf.Tensor: shape=(), dtype=float32, numpy=0.82745856>]\n",
      "Epoch 278/1000\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 611.1777\n",
      "Weights: [0.82742256, -0.1567386, 0.14478296, 0.40076432, -0.40105987, 0.8123964, <tf.Tensor: shape=(), dtype=float32, numpy=0.82771575>]\n",
      "Epoch 279/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1790\n",
      "Weights: [0.8282773, -0.15778969, 0.14621645, 0.4026495, -0.40308982, 0.81486213, <tf.Tensor: shape=(), dtype=float32, numpy=0.827703>]\n",
      "Epoch 280/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1703\n",
      "Weights: [0.8272965, -0.1555559, 0.14355221, 0.40188956, -0.4020569, 0.8125779, <tf.Tensor: shape=(), dtype=float32, numpy=0.82793206>]\n",
      "Epoch 281/1000\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 611.1770\n",
      "Weights: [0.8273395, -0.15670542, 0.14600241, 0.4013211, -0.40193707, 0.81328523, <tf.Tensor: shape=(), dtype=float32, numpy=0.8278163>]\n",
      "Epoch 282/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1762\n",
      "Weights: [0.82598394, -0.15668896, 0.14505698, 0.3982664, -0.40095383, 0.8128762, <tf.Tensor: shape=(), dtype=float32, numpy=0.8278929>]\n",
      "Epoch 283/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1946\n",
      "Weights: [0.82653725, -0.15500715, 0.14464016, 0.3993398, -0.40076602, 0.81280535, <tf.Tensor: shape=(), dtype=float32, numpy=0.82806814>]\n",
      "Epoch 284/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1808\n",
      "Weights: [0.82722616, -0.15669996, 0.14508678, 0.4017387, -0.4039493, 0.81443745, <tf.Tensor: shape=(), dtype=float32, numpy=0.8278677>]\n",
      "Epoch 285/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1785\n",
      "Weights: [0.82572657, -0.15512055, 0.14439656, 0.39805034, -0.40032315, 0.81149936, <tf.Tensor: shape=(), dtype=float32, numpy=0.82776326>]\n",
      "Epoch 286/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1904\n",
      "Weights: [0.8266745, -0.15538529, 0.14383435, 0.4008384, -0.40186277, 0.8115708, <tf.Tensor: shape=(), dtype=float32, numpy=0.82774293>]\n",
      "Epoch 287/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1715\n",
      "Weights: [0.82557446, -0.15440996, 0.14439048, 0.39877954, -0.39988402, 0.8120567, <tf.Tensor: shape=(), dtype=float32, numpy=0.828267>]\n",
      "Epoch 288/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1758\n",
      "Weights: [0.8278697, -0.15562381, 0.14505033, 0.40104318, -0.40228757, 0.8133858, <tf.Tensor: shape=(), dtype=float32, numpy=0.8278897>]\n",
      "Epoch 289/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1876\n",
      "Weights: [0.82592356, -0.15398404, 0.14231847, 0.3984501, -0.40061298, 0.81108767, <tf.Tensor: shape=(), dtype=float32, numpy=0.8278539>]\n",
      "Epoch 290/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1746\n",
      "Weights: [0.82652026, -0.15542027, 0.14545761, 0.40026695, -0.40263212, 0.8141189, <tf.Tensor: shape=(), dtype=float32, numpy=0.82762194>]\n",
      "Epoch 291/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1804\n",
      "Weights: [0.82842135, -0.15550552, 0.1441013, 0.40092903, -0.4024345, 0.81212014, <tf.Tensor: shape=(), dtype=float32, numpy=0.8277811>]\n",
      "Epoch 292/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1824\n",
      "Weights: [0.8258837, -0.15378986, 0.14375415, 0.39911392, -0.40169892, 0.81299216, <tf.Tensor: shape=(), dtype=float32, numpy=0.82789314>]\n",
      "Epoch 293/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1874\n",
      "Weights: [0.8282113, -0.15649213, 0.1462233, 0.39940104, -0.4028903, 0.8134956, <tf.Tensor: shape=(), dtype=float32, numpy=0.82774687>]\n",
      "Epoch 294/1000\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 611.1966\n",
      "Weights: [0.82616836, -0.1549427, 0.1450298, 0.39790577, -0.4006875, 0.812218, <tf.Tensor: shape=(), dtype=float32, numpy=0.82754165>]\n",
      "Epoch 295/1000\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 611.1812\n",
      "Weights: [0.8262704, -0.15530372, 0.14530092, 0.39916188, -0.40267658, 0.81287044, <tf.Tensor: shape=(), dtype=float32, numpy=0.82796943>]\n",
      "Epoch 296/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1862\n",
      "Weights: [0.82985306, -0.15613528, 0.14606099, 0.40310237, -0.4055445, 0.81603485, <tf.Tensor: shape=(), dtype=float32, numpy=0.82746613>]\n",
      "Epoch 297/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2053\n",
      "Weights: [0.82302773, -0.1519988, 0.1445104, 0.3979509, -0.40058562, 0.81196254, <tf.Tensor: shape=(), dtype=float32, numpy=0.8277959>]\n",
      "Epoch 298/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1724\n",
      "Weights: [0.82726103, -0.15528733, 0.14585763, 0.4003938, -0.40278846, 0.8122445, <tf.Tensor: shape=(), dtype=float32, numpy=0.82768273>]\n",
      "Epoch 299/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1671\n",
      "Weights: [0.8284525, -0.15591542, 0.14531292, 0.40109038, -0.40447918, 0.81335866, <tf.Tensor: shape=(), dtype=float32, numpy=0.82779133>]\n",
      "Epoch 300/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1761\n",
      "Weights: [0.8275922, -0.15548466, 0.14568402, 0.39843947, -0.40281442, 0.8128775, <tf.Tensor: shape=(), dtype=float32, numpy=0.82854545>]\n",
      "Epoch 301/1000\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 611.1709\n",
      "Weights: [0.8266865, -0.15536498, 0.14567445, 0.39911062, -0.4041185, 0.8134968, <tf.Tensor: shape=(), dtype=float32, numpy=0.8276917>]\n",
      "Epoch 302/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1677\n",
      "Weights: [0.82735926, -0.1544253, 0.14546247, 0.40038875, -0.40300795, 0.81374705, <tf.Tensor: shape=(), dtype=float32, numpy=0.8275378>]\n",
      "Epoch 303/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1719\n",
      "Weights: [0.8267256, -0.1530093, 0.14418808, 0.39935777, -0.4028216, 0.81340265, <tf.Tensor: shape=(), dtype=float32, numpy=0.82837725>]\n",
      "Epoch 304/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1732\n",
      "Weights: [0.82861453, -0.155115, 0.14528137, 0.4002302, -0.40360072, 0.81234604, <tf.Tensor: shape=(), dtype=float32, numpy=0.8277055>]\n",
      "Epoch 305/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1820\n",
      "Weights: [0.82738525, -0.15376848, 0.14476354, 0.40086865, -0.40392593, 0.8129551, <tf.Tensor: shape=(), dtype=float32, numpy=0.82793236>]\n",
      "Epoch 306/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1705\n",
      "Weights: [0.82723385, -0.15433596, 0.14576335, 0.39951727, -0.40308744, 0.8123238, <tf.Tensor: shape=(), dtype=float32, numpy=0.8280117>]\n",
      "Epoch 307/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1763\n",
      "Weights: [0.82736516, -0.15309536, 0.14491506, 0.39976552, -0.40348744, 0.8139779, <tf.Tensor: shape=(), dtype=float32, numpy=0.828057>]\n",
      "Epoch 308/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1736\n",
      "Weights: [0.82663757, -0.15156655, 0.14447872, 0.39957014, -0.4023266, 0.8124614, <tf.Tensor: shape=(), dtype=float32, numpy=0.8283715>]\n",
      "Epoch 309/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.1806\n",
      "Weights: [0.8263709, -0.15302353, 0.1455934, 0.39696726, -0.40169287, 0.81179816, <tf.Tensor: shape=(), dtype=float32, numpy=0.8280723>]\n",
      "Epoch 310/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1782\n",
      "Weights: [0.82800496, -0.15322585, 0.14512749, 0.40234318, -0.40508497, 0.81476367, <tf.Tensor: shape=(), dtype=float32, numpy=0.82807493>]\n",
      "Epoch 311/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1775\n",
      "Weights: [0.8285031, -0.15437977, 0.1463415, 0.40058705, -0.40420383, 0.8138977, <tf.Tensor: shape=(), dtype=float32, numpy=0.82806087>]\n",
      "Epoch 312/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1697\n",
      "Weights: [0.82655483, -0.15191364, 0.14444527, 0.399197, -0.40247908, 0.81194466, <tf.Tensor: shape=(), dtype=float32, numpy=0.82793236>]\n",
      "Epoch 313/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1713\n",
      "Weights: [0.8282031, -0.15320268, 0.14554651, 0.40277752, -0.40627956, 0.81480604, <tf.Tensor: shape=(), dtype=float32, numpy=0.8278811>]\n",
      "Epoch 314/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1882\n",
      "Weights: [0.8279367, -0.1534385, 0.14514709, 0.3985253, -0.40338308, 0.8126809, <tf.Tensor: shape=(), dtype=float32, numpy=0.82819915>]\n",
      "Epoch 315/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1877\n",
      "Weights: [0.82584906, -0.15237808, 0.14543693, 0.39800996, -0.402004, 0.8124777, <tf.Tensor: shape=(), dtype=float32, numpy=0.82801104>]\n",
      "Epoch 316/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1834\n",
      "Weights: [0.8278523, -0.15270853, 0.14477724, 0.3999948, -0.4040795, 0.81327546, <tf.Tensor: shape=(), dtype=float32, numpy=0.8276974>]\n",
      "Epoch 317/1000\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 611.1746\n",
      "Weights: [0.82760674, -0.15311365, 0.14503342, 0.3985012, -0.40303499, 0.8121142, <tf.Tensor: shape=(), dtype=float32, numpy=0.8280653>]\n",
      "Epoch 318/1000\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 611.1861\n",
      "Weights: [0.829518, -0.15463823, 0.14690778, 0.39891025, -0.40469977, 0.81300163, <tf.Tensor: shape=(), dtype=float32, numpy=0.82829255>]\n",
      "Epoch 319/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1663\n",
      "Weights: [0.8276896, -0.15371981, 0.14537905, 0.39890227, -0.40396342, 0.8122195, <tf.Tensor: shape=(), dtype=float32, numpy=0.82795584>]\n",
      "Epoch 320/1000\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 611.1699\n",
      "Weights: [0.82828486, -0.15342328, 0.14379807, 0.39889824, -0.40409127, 0.8128991, <tf.Tensor: shape=(), dtype=float32, numpy=0.82822686>]\n",
      "Epoch 321/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1763\n",
      "Weights: [0.82907957, -0.15323862, 0.14488722, 0.3997126, -0.4048976, 0.8129692, <tf.Tensor: shape=(), dtype=float32, numpy=0.8280998>]\n",
      "Epoch 322/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1682\n",
      "Weights: [0.8279133, -0.1528758, 0.1443651, 0.39620593, -0.40213072, 0.8110131, <tf.Tensor: shape=(), dtype=float32, numpy=0.8279054>]\n",
      "Epoch 323/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1737\n",
      "Weights: [0.82753, -0.15331134, 0.14535695, 0.39832124, -0.40422428, 0.8122529, <tf.Tensor: shape=(), dtype=float32, numpy=0.8285014>]\n",
      "Epoch 324/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1755\n",
      "Weights: [0.82963926, -0.15407288, 0.146073, 0.39991665, -0.40500534, 0.81425685, <tf.Tensor: shape=(), dtype=float32, numpy=0.8280481>]\n",
      "Epoch 325/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1684\n",
      "Weights: [0.8283418, -0.15345863, 0.14582986, 0.3987324, -0.4041613, 0.8131641, <tf.Tensor: shape=(), dtype=float32, numpy=0.82816947>]\n",
      "Epoch 326/1000\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 611.1765\n",
      "Weights: [0.8296633, -0.15419228, 0.1463553, 0.4007285, -0.40630943, 0.8151484, <tf.Tensor: shape=(), dtype=float32, numpy=0.8280605>]\n",
      "Epoch 327/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1868\n",
      "Weights: [0.8294495, -0.15321352, 0.14607406, 0.399018, -0.40488848, 0.8144413, <tf.Tensor: shape=(), dtype=float32, numpy=0.82808495>]\n",
      "Epoch 328/1000\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 611.1730\n",
      "Weights: [0.8294896, -0.15413006, 0.14591272, 0.40001267, -0.40485924, 0.81315064, <tf.Tensor: shape=(), dtype=float32, numpy=0.82810533>]\n",
      "Epoch 329/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1736\n",
      "Weights: [0.82684326, -0.15209556, 0.14373887, 0.39817443, -0.40393543, 0.811389, <tf.Tensor: shape=(), dtype=float32, numpy=0.8279327>]\n",
      "Epoch 330/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1881\n",
      "Weights: [0.82921267, -0.15325217, 0.1464185, 0.3983208, -0.4043651, 0.8136179, <tf.Tensor: shape=(), dtype=float32, numpy=0.82816744>]\n",
      "Epoch 331/1000\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 611.1838\n",
      "Weights: [0.8274089, -0.15102352, 0.14602236, 0.3985813, -0.40388188, 0.8126582, <tf.Tensor: shape=(), dtype=float32, numpy=0.8280492>]\n",
      "Epoch 332/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1657\n",
      "Weights: [0.82759166, -0.15245153, 0.14518622, 0.3987399, -0.40443337, 0.812155, <tf.Tensor: shape=(), dtype=float32, numpy=0.8281474>]\n",
      "Epoch 333/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1993\n",
      "Weights: [0.82902503, -0.15362574, 0.14639513, 0.39843303, -0.4050259, 0.81289953, <tf.Tensor: shape=(), dtype=float32, numpy=0.8279822>]\n",
      "Epoch 334/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1714\n",
      "Weights: [0.82843196, -0.15314189, 0.14600703, 0.39633036, -0.4037958, 0.81267786, <tf.Tensor: shape=(), dtype=float32, numpy=0.82803637>]\n",
      "Epoch 335/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1799\n",
      "Weights: [0.82712156, -0.15128203, 0.14398155, 0.39943662, -0.40507188, 0.8115138, <tf.Tensor: shape=(), dtype=float32, numpy=0.82789326>]\n",
      "Epoch 336/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1694\n",
      "Weights: [0.82953113, -0.15337297, 0.1462711, 0.39842898, -0.40588918, 0.81356984, <tf.Tensor: shape=(), dtype=float32, numpy=0.82859874>]\n",
      "Epoch 337/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1940\n",
      "Weights: [0.8254316, -0.14887244, 0.14313777, 0.39595848, -0.40075025, 0.8098056, <tf.Tensor: shape=(), dtype=float32, numpy=0.8278949>]\n",
      "Epoch 338/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1843\n",
      "Weights: [0.8305097, -0.15351835, 0.14577998, 0.3989474, -0.40622124, 0.81341726, <tf.Tensor: shape=(), dtype=float32, numpy=0.8288442>]\n",
      "Epoch 339/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1920\n",
      "Weights: [0.82938653, -0.15224005, 0.14540982, 0.3994458, -0.40559405, 0.8124107, <tf.Tensor: shape=(), dtype=float32, numpy=0.8282071>]\n",
      "Epoch 340/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1744\n",
      "Weights: [0.8291951, -0.15291843, 0.14583863, 0.39813778, -0.40439156, 0.81156135, <tf.Tensor: shape=(), dtype=float32, numpy=0.82849807>]\n",
      "Epoch 341/1000\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 611.1717\n",
      "Weights: [0.8285991, -0.15147409, 0.14504306, 0.39881808, -0.40478662, 0.8131232, <tf.Tensor: shape=(), dtype=float32, numpy=0.828292>]\n",
      "Epoch 342/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1666\n",
      "Weights: [0.8282914, -0.15071277, 0.1449789, 0.39803383, -0.40422168, 0.81263715, <tf.Tensor: shape=(), dtype=float32, numpy=0.82805014>]\n",
      "Epoch 343/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1951\n",
      "Weights: [0.82948685, -0.15176919, 0.14613922, 0.39986625, -0.4066281, 0.81480384, <tf.Tensor: shape=(), dtype=float32, numpy=0.82807565>]\n",
      "Epoch 344/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1957\n",
      "Weights: [0.8310679, -0.15456887, 0.14741465, 0.39919844, -0.40762824, 0.8141763, <tf.Tensor: shape=(), dtype=float32, numpy=0.8288096>]\n",
      "Epoch 345/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1600\n",
      "Weights: [0.82813734, -0.15155654, 0.1447304, 0.39578548, -0.4035337, 0.81117153, <tf.Tensor: shape=(), dtype=float32, numpy=0.82844746>]\n",
      "Epoch 346/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1771\n",
      "Weights: [0.8289244, -0.15189263, 0.14569125, 0.39877462, -0.40703943, 0.81389695, <tf.Tensor: shape=(), dtype=float32, numpy=0.8280966>]\n",
      "Epoch 347/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1857\n",
      "Weights: [0.8295383, -0.15179469, 0.14656453, 0.40025687, -0.40694743, 0.8145366, <tf.Tensor: shape=(), dtype=float32, numpy=0.828755>]\n",
      "Epoch 348/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1719\n",
      "Weights: [0.8290338, -0.15177082, 0.14677566, 0.3983721, -0.40537247, 0.8131144, <tf.Tensor: shape=(), dtype=float32, numpy=0.8279997>]\n",
      "Epoch 349/1000\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 611.1738\n",
      "Weights: [0.82811606, -0.15208638, 0.14640898, 0.3977058, -0.40613344, 0.8134422, <tf.Tensor: shape=(), dtype=float32, numpy=0.8286848>]\n",
      "Epoch 350/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1688\n",
      "Weights: [0.829014, -0.15127556, 0.14534423, 0.39742032, -0.40472147, 0.81204915, <tf.Tensor: shape=(), dtype=float32, numpy=0.8283675>]\n",
      "Epoch 351/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1697\n",
      "Weights: [0.8293474, -0.15295213, 0.14536013, 0.39944074, -0.40759295, 0.8137975, <tf.Tensor: shape=(), dtype=float32, numpy=0.82858455>]\n",
      "Epoch 352/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1746\n",
      "Weights: [0.8287139, -0.15001248, 0.14477268, 0.39696994, -0.40514168, 0.81275266, <tf.Tensor: shape=(), dtype=float32, numpy=0.82824314>]\n",
      "Epoch 353/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1735\n",
      "Weights: [0.83077633, -0.15346622, 0.1461718, 0.39658377, -0.40545082, 0.8115052, <tf.Tensor: shape=(), dtype=float32, numpy=0.8279834>]\n",
      "Epoch 354/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1769\n",
      "Weights: [0.83065486, -0.1532291, 0.14658114, 0.39768586, -0.40693808, 0.81407887, <tf.Tensor: shape=(), dtype=float32, numpy=0.8281837>]\n",
      "Epoch 355/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1989\n",
      "Weights: [0.8288573, -0.14958312, 0.14342137, 0.3984734, -0.40490383, 0.81189924, <tf.Tensor: shape=(), dtype=float32, numpy=0.8281404>]\n",
      "Epoch 356/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1779\n",
      "Weights: [0.82984406, -0.15200903, 0.14613035, 0.39848948, -0.40611556, 0.8132863, <tf.Tensor: shape=(), dtype=float32, numpy=0.82824224>]\n",
      "Epoch 357/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1826\n",
      "Weights: [0.8298874, -0.1531973, 0.14697582, 0.39755487, -0.4079027, 0.81363696, <tf.Tensor: shape=(), dtype=float32, numpy=0.8282454>]\n",
      "Epoch 358/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1741\n",
      "Weights: [0.8296019, -0.15045944, 0.14431612, 0.39851683, -0.40535477, 0.8113093, <tf.Tensor: shape=(), dtype=float32, numpy=0.82814574>]\n",
      "Epoch 359/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.2059\n",
      "Weights: [0.8318645, -0.15251897, 0.14660358, 0.39714333, -0.40632528, 0.8136342, <tf.Tensor: shape=(), dtype=float32, numpy=0.828122>]\n",
      "Epoch 360/1000\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 611.1738\n",
      "Weights: [0.82785463, -0.15076526, 0.14692006, 0.3973875, -0.40640342, 0.8137228, <tf.Tensor: shape=(), dtype=float32, numpy=0.8284816>]\n",
      "Epoch 361/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1783\n",
      "Weights: [0.82875884, -0.15010668, 0.14444607, 0.3966107, -0.40602398, 0.8118532, <tf.Tensor: shape=(), dtype=float32, numpy=0.8284812>]\n",
      "Epoch 362/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.1755\n",
      "Weights: [0.8300209, -0.15262558, 0.14656937, 0.39818686, -0.40749598, 0.8138818, <tf.Tensor: shape=(), dtype=float32, numpy=0.82816046>]\n",
      "Epoch 363/1000\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 611.1693\n",
      "Weights: [0.8300966, -0.15112329, 0.14452726, 0.39812547, -0.4066412, 0.81347156, <tf.Tensor: shape=(), dtype=float32, numpy=0.82825434>]\n",
      "Epoch 364/1000\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 611.1799\n",
      "Weights: [0.8310778, -0.15197411, 0.1457473, 0.39794728, -0.40646368, 0.81281793, <tf.Tensor: shape=(), dtype=float32, numpy=0.82817894>]\n",
      "Epoch 365/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1768\n",
      "Weights: [0.8278951, -0.15045033, 0.1456945, 0.3966889, -0.40586483, 0.8131226, <tf.Tensor: shape=(), dtype=float32, numpy=0.8281175>]\n",
      "Epoch 366/1000\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 611.1720\n",
      "Weights: [0.8297177, -0.15065606, 0.14435089, 0.3984503, -0.40721512, 0.812775, <tf.Tensor: shape=(), dtype=float32, numpy=0.8282464>]\n",
      "Epoch 367/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1657\n",
      "Weights: [0.8313577, -0.15155308, 0.14572255, 0.3991532, -0.40809464, 0.8136036, <tf.Tensor: shape=(), dtype=float32, numpy=0.828218>]\n",
      "Epoch 368/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1697\n",
      "Weights: [0.83067214, -0.15161958, 0.14646335, 0.39690375, -0.40642375, 0.8127059, <tf.Tensor: shape=(), dtype=float32, numpy=0.8284829>]\n",
      "Epoch 369/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1714\n",
      "Weights: [0.8313443, -0.15199308, 0.14676651, 0.39921084, -0.4070006, 0.8133528, <tf.Tensor: shape=(), dtype=float32, numpy=0.82805216>]\n",
      "Epoch 370/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1654\n",
      "Weights: [0.8305284, -0.15114193, 0.14517374, 0.3967722, -0.40739536, 0.813193, <tf.Tensor: shape=(), dtype=float32, numpy=0.82801807>]\n",
      "Epoch 371/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1873\n",
      "Weights: [0.82897687, -0.15035112, 0.14495231, 0.39614287, -0.4066053, 0.812723, <tf.Tensor: shape=(), dtype=float32, numpy=0.8277166>]\n",
      "Epoch 372/1000\n",
      "40/40 [==============================] - 13s 329ms/step - loss: 611.1760\n",
      "Weights: [0.82866436, -0.14838941, 0.14332432, 0.39619973, -0.4054939, 0.8111377, <tf.Tensor: shape=(), dtype=float32, numpy=0.8282183>]\n",
      "Epoch 373/1000\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 611.1782\n",
      "Weights: [0.8301167, -0.15098469, 0.14560851, 0.3987656, -0.40818614, 0.8134837, <tf.Tensor: shape=(), dtype=float32, numpy=0.82864606>]\n",
      "Epoch 374/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1840\n",
      "Weights: [0.8322514, -0.15286958, 0.14660528, 0.3997572, -0.40882477, 0.81349564, <tf.Tensor: shape=(), dtype=float32, numpy=0.8279451>]\n",
      "Epoch 375/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1890\n",
      "Weights: [0.8328323, -0.15288825, 0.14723903, 0.39866126, -0.40723014, 0.81239206, <tf.Tensor: shape=(), dtype=float32, numpy=0.8285967>]\n",
      "Epoch 376/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1752\n",
      "Weights: [0.8292851, -0.14986937, 0.144997, 0.3959555, -0.4062419, 0.8133948, <tf.Tensor: shape=(), dtype=float32, numpy=0.82836944>]\n",
      "Epoch 377/1000\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 611.2129\n",
      "Weights: [0.8296766, -0.15059015, 0.14653394, 0.39585876, -0.40579697, 0.8125897, <tf.Tensor: shape=(), dtype=float32, numpy=0.8281406>]\n",
      "Epoch 378/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1833\n",
      "Weights: [0.829274, -0.15026201, 0.144715, 0.3960838, -0.406956, 0.8122177, <tf.Tensor: shape=(), dtype=float32, numpy=0.828372>]\n",
      "Epoch 379/1000\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 611.1679\n",
      "Weights: [0.83240837, -0.15152456, 0.14586444, 0.39854905, -0.40803334, 0.81251776, <tf.Tensor: shape=(), dtype=float32, numpy=0.82816565>]\n",
      "Epoch 380/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1815\n",
      "Weights: [0.8289077, -0.14932474, 0.14372507, 0.39792666, -0.40629542, 0.8108773, <tf.Tensor: shape=(), dtype=float32, numpy=0.8279798>]\n",
      "Epoch 381/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1792\n",
      "Weights: [0.8298689, -0.15147685, 0.14725725, 0.39821056, -0.40914276, 0.81506455, <tf.Tensor: shape=(), dtype=float32, numpy=0.82818973>]\n",
      "Epoch 382/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1687\n",
      "Weights: [0.83035177, -0.15075065, 0.14559181, 0.39583087, -0.4068237, 0.8115903, <tf.Tensor: shape=(), dtype=float32, numpy=0.82814574>]\n",
      "Epoch 383/1000\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 611.1949\n",
      "Weights: [0.8289517, -0.14885534, 0.14398247, 0.3959016, -0.40554896, 0.8113724, <tf.Tensor: shape=(), dtype=float32, numpy=0.8284472>]\n",
      "Epoch 384/1000\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 611.1976\n",
      "Weights: [0.828285, -0.14786999, 0.14336024, 0.3937254, -0.40317297, 0.80964553, <tf.Tensor: shape=(), dtype=float32, numpy=0.8284161>]\n",
      "Epoch 385/1000\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 611.1840\n",
      "Weights: [0.82895744, -0.1501982, 0.14567493, 0.39495587, -0.4055769, 0.811848, <tf.Tensor: shape=(), dtype=float32, numpy=0.82820445>]\n",
      "Epoch 386/1000\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 611.1758\n",
      "Weights: [0.8298451, -0.148776, 0.14433102, 0.39806736, -0.4068648, 0.8120349, <tf.Tensor: shape=(), dtype=float32, numpy=0.828043>]\n",
      "Epoch 387/1000\n",
      "40/40 [==============================] - 181s 5s/step - loss: 611.1912\n",
      "Weights: [0.8294359, -0.14947684, 0.14507003, 0.3963448, -0.40698257, 0.81282747, <tf.Tensor: shape=(), dtype=float32, numpy=0.8281007>]\n",
      "Epoch 388/1000\n",
      "40/40 [==============================] - 14s 353ms/step - loss: 611.1678\n",
      "Weights: [0.82958144, -0.14982769, 0.14503767, 0.39562386, -0.40626603, 0.8121984, <tf.Tensor: shape=(), dtype=float32, numpy=0.8279915>]\n",
      "Epoch 389/1000\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 611.1693\n",
      "Weights: [0.8308894, -0.14989676, 0.14591226, 0.3978845, -0.40771073, 0.813225, <tf.Tensor: shape=(), dtype=float32, numpy=0.82824385>]\n",
      "Epoch 390/1000\n",
      "36/40 [==========================>...] - ETA: 1s - loss: 611.3973"
     ]
    }
   ],
   "source": [
    "# Now try to fit the model sampled from the real parameters.\n",
    "initializer = tf.keras.initializers.Constant([ 1., 0., 0., 0., 0., 1.]),\n",
    "model = build_hmm_to_fit_beta( states_observable=STATES_ARE_OBSERVABLE, mu=MU, initializer=initializer )\n",
    "\n",
    "compiler = CompilerInfoBeta(LR_EXPONENTIAL_DECAY)\n",
    "model.compile(\n",
    "    loss = compiler.loss,\n",
    "    optimizer = compiler.optimizer,\n",
    "    run_eagerly = True\n",
    ")\n",
    "BETA = [0.7, -0.3, 0.15, 0.4, -0.4, 0.8]\n",
    "history = fit_model(model, adstock, emission_real)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Supponiamo che il vettore fittato dal modello sia: #camp1: [ 0.8700186 ,  0.29879373, -0.35601237,  0.14475204,                                                     #camp2:   0.32184145,  0.06852179, -0.33719164,  0.80015075]\n",
    "Dal punto di vista teorico, i valori sono in linea con quelli sperati. Per interpetare i parametri, si tenga conto che: abbiamo 8 valori, di cui 4 per campagna: per ogni campagna il parametro 0 è l’influenza della campagna a passare da stato 0->1, il parametro 1 è l’influenza 0->2, il terzo è l’influenza 1->0 (quindi ci aspettiamo che sia negativo), e il quarto è l'influenza 1->2.\n",
    "Se compariamo i valori, partendo dalla tupla reale, l’obiettivo è osservare che la prima campagna influenzi il passaggio da stato 0 a stato 1 più della seconda campagna, mentre la seconda campagna dovrebbe avere l’effetto opposto, ovvero di influenzare più il passaggio da stato 1 a conversione.\n",
    "Dai valori dei parametri fittati partendo da inizializzazione di zeri, osserviamo alcune cose:\n",
    "1) Alcuni valori sembrano essere stati fittati correttamente. In particolare i valori di transizione da stato 1->2, ovvero 0.15 e 0.8.\n",
    "2) Gli altri pesi attribuiti sono coerenti con quelli reali. La campagna 1 mostra influenze piu alte nello stato zero (peso transizione 0->1 = 0.87, 0->2 = 0.3) rispetto alla campagna 2 (peso transizione 0->1 = 0.32, 0->2 = 0.07), mentre la campagna due mostra peso di transizione da stato intermedio a conversione più alto (1->2 = 0.8, vs campagna 1: 1->2 = 0.14). Pertanto, seppur i valori non siano esattamente quelli reali, viene catturata l'idea di avere campagne con influenze diverse in stati diversi, principale obiettivo del problema di attribution.\n",
    "3) Nel caso di sanity check partendo da parametri esatti, si è osservato che il vettore cambia nonostante la loss rimanga costante. Il vettore fittato da inizializzazione di zeri, sembrerebbe avvicinarsi a questa versione modificata del vettore reale. Comprendere per quale motivo ci si discosta dal vettore reale forse permetterebbe di risolvere questa discrepanza.\n",
    "\n",
    "Sotto, osserviamo anche la differenza media tra matrici di transizione ottenute con i parametri reali e quelli fittati dal modello."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "matrix_diff = lambda usr: tf.reduce_sum(make_transition_matrix(MU, model_test.weights[0], adstock[usr:usr+1]) - make_transition_matrix(MU,BETA, adstock[usr:usr+1]), axis=1)/30\n",
    "avg_diff = sum(matrix_diff(usr) for usr in range(N_users))/N_users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}